{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XFJMGF2GhMCM"
      },
      "source": [
        "# Assignment 2: Gradient Descent\n",
        "\n",
        "The goal of this exercise is to gain experience with a basic technique of Deep Learning, i.e., gradient descent.\n",
        "A two-dimensional loss surface is created manually and gradient descent is implemented.\n",
        "Several runs of gradient descent from different starting locations will be performed.\n",
        "The loss surface and the detected minima are plotted together in one 3D plot."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aKzqZe_UgdKf"
      },
      "source": [
        "## Compute the Gradient\n",
        "The loss function is manually defined as: $$\\mathcal J_{\\vec w}=w_1^2 + w_2^2 + 40 \\cos(w_1) \\cos(w_2) + w_1 \\sin(w_2)$$\n",
        "The weights $\\vec w = (w_1, w_2)^T$ shall be optimized such that the loss function has a minimum."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NX0qB7Emgf2b"
      },
      "source": [
        "### Task 1: Compute the Gradient\n",
        "\n",
        "The gradient $\\nabla \\mathcal J_{\\vec w}$ is defined as the partial derivatives of the loss function with respect to the two variables $w_1$ and $w_2$.\n",
        "We need to calculate it:\n",
        "\n",
        "* $\\frac{\\partial \\mathcal J}{\\partial w_1} = ...$\n",
        "* $\\frac{\\partial \\mathcal J}{\\partial w_2} = ...$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGZi-Gs5ghic"
      },
      "source": [
        "### Task 2: Implement the Loss Function\n",
        "\n",
        "Implement the loss function in Python, which takes a given $\\vec w$ and returns $\\mathcal J_{\\vec w}$ according to the given loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqRlmAfxhMCP"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "\n",
        "def loss(w):\n",
        "  return ..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_d2XYSHMhMCQ"
      },
      "source": [
        "### Task 3: Implement the Gradient\n",
        "\n",
        "Implement the gradient as a function in Python, which takes a given $\\vec w$ and returns $\\nabla\\mathcal J_{\\vec w}$ according to the analytical result in Task 1. \n",
        "Remember that the gradient needs to be computed and returned for both $w_1$ and $w_2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYvhZJnDhMCQ"
      },
      "outputs": [],
      "source": [
        "def gradient(w):\n",
        "  return ..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LcMfR2kX69Xe"
      },
      "source": [
        "### Test 1: Test Functions \n",
        "The codes below call the loss function from Task 2 and the gradient function from Task 3 with $\\vec w=(0,0)^T$, and then compare the return values with the given analytically computed values. \n",
        "Please check your implementation if the tests cannot be passed. \n",
        "\n",
        "Make sure your code can pass the test before moving to the next task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueZTSSTIhMCR",
        "outputId": "c93732f4-b578-4ec1-f40b-ecf66390cbe4"
      },
      "outputs": [],
      "source": [
        "w = numpy.zeros(2)\n",
        "\n",
        "# analytically computed expected values\n",
        "expected_loss = 40\n",
        "expected_gradient = numpy.array((0.,0.))\n",
        "\n",
        "# test loss function\n",
        "assert abs(loss(w) - expected_loss) < 1e-8\n",
        "assert numpy.all(numpy.abs(numpy.array(gradient(w)) - expected_gradient) < 1e-8)\n",
        "print(\"Tests passed\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxe3nIhnhMCR"
      },
      "source": [
        "## Implement Gradient Descent\n",
        "The procedure of gradient decent is the repeated application of two steps:\n",
        "\n",
        "* First, the gradient of the loss $\\nabla\\mathcal J_{\\vec w}$ is computed based on the current value of the parameters $\\vec w$.\n",
        "\n",
        "* Second, the weights are updated by moving a small step in the direction of the negative gradient: $\\vec w = \\vec w - \\eta\\nabla\\mathcal J_{\\vec w}$\n",
        "\n",
        "Optionally, the loss $\\mathcal J_{\\vec w}$ is computed to record the progress of the gradient descent.\n",
        "Finally, one or more appropriate criteria need to be defined to decide when to stop the procedure."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-NjDp5ldgxmZ"
      },
      "source": [
        "### Task 4: Termination Criterion\n",
        "\n",
        "(theoretical question) Define a proper termination criterion. Which error cases might occur and need to be considered?\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew3iZPupgymq"
      },
      "source": [
        "### Task 5: Implement Gradient Descent\n",
        "\n",
        "Implement a function that performs the gradient descent. This function should take as parameters an initial weight vector $\\vec w$ and a learning rate $\\eta$, and make use of the gradient function implemented in Task 3 and, possibly, the loss function from Task 2.\n",
        "It should return the optimized weight vector $\\vec w^*$. Incorporate the termination criterion designed in Task 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBGsNgPYhMCS"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(w, eta=0.01):\n",
        "  # copy the weights to not modify the original values\n",
        "  w_star = w.copy()\n",
        "\n",
        "  # perform iterative gradient descent\n",
        "  while ...:\n",
        "    # compute the gradient\n",
        "    ...\n",
        "\n",
        "    # update the weights\n",
        "    ...\n",
        "\n",
        "    # include additional termination criteria?\n",
        "\n",
        "  return w_star"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e83uiKEGhGi7"
      },
      "source": [
        "## Evaluate Gradient Descent"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ8BZ3B8hMCS"
      },
      "source": [
        "### Task 6: Run Gradient Descent\n",
        "Call the gradient descent function from Task 5 1000 times with different random weights $\\vec w\\in[-5,5]^2$ drawn from a uniform distribution and a learning rate of $\\eta=0.01$. \n",
        "Store the resulting optimized weight vectors in a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLCz_rizhMCT"
      },
      "outputs": [],
      "source": [
        "stored_weights = []\n",
        "\n",
        "for i in range(1000):\n",
        "  # create random weight vector\n",
        "  w = ...\n",
        "  # call gradient descent\n",
        "  w_star = ...\n",
        "  # store it in the list\n",
        "  ..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JOVpknxZ5cU_"
      },
      "source": [
        "### Test 2: Check Minima\n",
        "\n",
        "Counting the number of local minima in our loss function, we reach a total of 12. Please use this function to verify that your implementation could reach this number at maximum.\n",
        "\n",
        "Again, make sure you pass the test before moving to the next task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTth8eW3hMCT",
        "outputId": "ea0a7235-ed96-44a7-fc9e-370cfa32ff53"
      },
      "outputs": [],
      "source": [
        "maximum_number_of_minima = 12\n",
        "\n",
        "# compute the number of reached minima \n",
        "minima = []\n",
        "for w_star in stored_weights:\n",
        "  # check if this weight vector is far enough \n",
        "  # from all previously stored vectors\n",
        "  if all(numpy.linalg.norm(w_star-w) > 1e-3 for w in minima):\n",
        "    minima.append(w_star)\n",
        "number_of_minima = len(minima)\n",
        "\n",
        "assert number_of_minima <= maximum_number_of_minima\n",
        "\n",
        "print(\"Check passed. The number of minima\", number_of_minima, \"is lower than or equal to the maximum\", maximum_number_of_minima)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q8SUKqAghMCT"
      },
      "source": [
        "### Task 7: Find the Global Minimum\n",
        "\n",
        "Find the global minimum of our error function by evaluating the obtained optimized weight vectors from Task 6. \n",
        "Print the minimum and its loss value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF9ExVgwhMCU"
      },
      "outputs": [],
      "source": [
        "# find the lowest loss\n",
        "minimum_loss = ...\n",
        "minimum_weights = ...\n",
        "\n",
        "print(\"The minimum loss value of:\", minimum_loss, \"was found for minimum\", minimum_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxuktyA-hOCo"
      },
      "source": [
        "## Plot Error Surface and Points"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bj6cRLiRhMCU"
      },
      "source": [
        "### Task 8: Loss Surface Plot\n",
        "\n",
        "Plot the error surface of the given loss function. \n",
        "Limit range $\\vec w\\in[-10,10]^2$. \n",
        "For each of the optimized weights from Task 6, plot a marker into the 3D plot. \n",
        "An example can be found in the slides.\n",
        "\n",
        "When plotting the resulting optimized weights $\\vec w=(w_1, w_2)^T$, we need to define the third coordinate. \n",
        "What should this coordinate be?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmR3sdirhMCU"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot\n",
        "\n",
        "# create 3D axis\n",
        "figure = pyplot.figure()\n",
        "axis = figure.add_subplot(111, projection='3d', azim = -40, elev=50)\n",
        "\n",
        "# define range to plot\n",
        "w_range = ...\n",
        "w1, w2 = numpy.meshgrid(w_range, w_range)\n",
        "\n",
        "# compute loss for w1 and w2\n",
        "J = ...\n",
        "\n",
        "# plot surface with jet colormap\n",
        "axis.plot_surface(w1, w2, J, cmap=\"jet\", alpha=0.8)\n",
        "\n",
        "# plot resulting points in 3D\n",
        "for w_star in stored_weights:\n",
        "  # compute the z-position\n",
        "  z = ...\n",
        "  # plot as 3D point\n",
        "  axis.plot([w_star[0]], [w_star[1]], [z], \"kx\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "DL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a29cabff5744fce69e08a959ab87b9e77a9f67b498d08783caa8c3bb16f23a00"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
