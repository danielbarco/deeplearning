{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9DKZj8jK_Dry"
      },
      "source": [
        "# Assignment 4: Multi-Output Networks and Batch Processing\n",
        "\n",
        "\n",
        "The goal of this exercise is to get to know some regularization techniques when implementing deep learning methods.\n",
        "For this purpose, we select a dataset that contains data in different formats, some binary ($x_d \\in \\{-1,1\\}$) and some numerical ($x_d\\in \\mathbb N$); and some are categorical, which we ignore for now.\n",
        "As target values, this dataset contains three numerical outputs, so, $\\vec t \\in \\mathbb R^3$ for each sample.\n",
        "These target values should be approximated with a two-layer multi-output network that we will train with the $\\mathcal J^{L_2}$ loss.\n",
        "\n",
        "\n",
        "\n",
        "Remember to make use of `np` in the matrix calculation, e.g. `np.dot`, `np.exp`, `np.mean` \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5lrQLreklXtS"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The dataset of our choice is the Student Performance estimation dataset that was collected in Portugal in two different schools and with two different subjects, i.e., math and Portuguese (the mother tongue).\n",
        "The dataset contains many different inputs such as a binary representation of the school, gender, family sizes, and alike, as well as numerical representations of age, travel time, and alcohol consumption.\n",
        "The dataset also includes some categorical data, which we skip in this assignment.\n",
        "See https://archive.ics.uci.edu/ml/datasets/Student+Performance for more information on the dataset.\n",
        "As a start, we will rely on the Portuguese performance (`\"por\"`), but you can also try to use the Math samples (`\"mat\"`)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HSol7VoalR-7"
      },
      "source": [
        "### Task 1: Dataset Loading\n",
        "\n",
        "\n",
        "Load the dataset from files and provide the input matrix $\\mathbf X \\in \\mathbb R^{(D+1)\\times N}$ and the output matrix $\\mathbf T \\in \\mathbb R^{O\\times N}$.\n",
        "\n",
        "Due to the difficulty of the task, most of the implementation is provided.\n",
        "The implementation is very literal and, therefore, hopefully readable, while maybe not the most efficient.\n",
        "\n",
        "We skip categorical inputs (indexes 8-11) for now.\n",
        "All other entries are converted either into binary $(-1,1)$ or into an integer range $(0,1,\\ldots)$.\n",
        "The three outputs range between 0 and 20 each. The bias value for $x_0=1$ is also already included.\n",
        "You just need to make sure that the data $(X,T)$ is returned in the desired format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wE3DlW-e_Dr1"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Dataset origin: https://archive.ics.uci.edu/ml/datasets/Student+Performance\n",
        "\n",
        "def dataset(course=\"por\"):\n",
        "  # load dataset and provide input and target data\n",
        "  # possible data files are \"mat\" and \"por\"\n",
        "\n",
        "  # download data file from URL\n",
        "  dataset_zip_file = \"student.zip\"\n",
        "  if not os.path.exists(dataset_zip_file):\n",
        "    import urllib.request\n",
        "    urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00320/student.zip\", dataset_zip_file)\n",
        "    print (\"Downloaded datafile\", dataset_zip_file)\n",
        "\n",
        "  import zipfile\n",
        "  import csv\n",
        "  import io\n",
        "\n",
        "  # collect inputs\n",
        "  inputs = []\n",
        "  targets = []\n",
        "  # some default values: yes=1, no=-1\n",
        "  yn = {\"yes\":1.,\"no\":-1.}\n",
        "  # read through dataset (without actually unzippiung to a file):\n",
        "  # ... open zip file\n",
        "  zip = zipfile.ZipFile(dataset_zip_file)\n",
        "  # ... open data file inside of zip file and convert bytes to text\n",
        "  datafile = io.TextIOWrapper(zip.open(os.path.join(F\"student-{course}.csv\"), 'r'))\n",
        "  # ... read through the lines via CSV reader, using the correct delimited\n",
        "  reader = csv.reader(datafile, delimiter=\";\")\n",
        "  # ... skip header line\n",
        "  next(reader)\n",
        "  for splits in reader:\n",
        "    # read input values\n",
        "    inputs.append([\n",
        "      1.,                             #### BIAS ####\n",
        "      {\"GP\":1.,\"MS\":-1.}[splits[0]],  # school\n",
        "      {\"M\":1.,\"F\":-1.}[splits[1]],    # gender\n",
        "      float(splits[2]),               # age\n",
        "      {\"U\":1.,\"R\":-1.}[splits[3]],    # address\n",
        "      {\"LE3\":1.,\"GT3\":-1.}[splits[4]],# family size\n",
        "      {\"T\":1.,\"A\":-1.}[splits[5]],    # parents living together\n",
        "      float(splits[6]),               # mother education\n",
        "      float(splits[7]),               # father education\n",
        "      # skip categorical values\n",
        "      float(splits[12]),              # travel time\n",
        "      float(splits[13]),              # study time\n",
        "      float(splits[14]),              # failures\n",
        "      yn[splits[15]],                 # extra support\n",
        "      yn[splits[16]],                 # family support\n",
        "      yn[splits[17]],                 # paid support\n",
        "      yn[splits[18]],                 # activities\n",
        "      yn[splits[19]],                 # nursery school\n",
        "      yn[splits[20]],                 # higher education\n",
        "      yn[splits[21]],                 # internet\n",
        "      yn[splits[22]],                 # romantic\n",
        "      float(splits[23]),              # family relation\n",
        "      float(splits[24]),              # free time\n",
        "      float(splits[25]),              # going out\n",
        "      float(splits[26]),              # workday alcohol\n",
        "      float(splits[27]),              # weekend alcohol\n",
        "      float(splits[28]),              # health\n",
        "      float(splits[29]),              # absences\n",
        "    ])\n",
        "\n",
        "    # read targets values\n",
        "    targets.append([\n",
        "      float(splits[30]),              # grade for primary school\n",
        "      float(splits[31]),              # grade for secondary school\n",
        "      float(splits[32]),              # grade for tertiary school\n",
        "    ])\n",
        "\n",
        "  print(F\"Loaded dataset with {len(targets)} samples\")\n",
        "  # ?? why transpose the inputs?\n",
        "  return np.transpose(np.array(inputs)), np.transpose(np.array(targets))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWNwt2Rb_Dr3"
      },
      "source": [
        "### Test 1: Assert Valid Outputs\n",
        "\n",
        "This test will check the dimension of the loaded dataset, i.e. $\\mathbf X\\in \\mathbb R^{(D+1)\\times N}$ and $\\mathbf T \\in \\mathbb R^{O\\times N}$, and also assure that all target data is in the range $t\\in[0,20]$.\n",
        "\n",
        "Please make sure that your implementation can pass these tests before moving to the next task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaCpzc-l_Dr3",
        "outputId": "6fefb52a-492d-489a-aec3-156623dd939c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded dataset with 649 samples\n"
          ]
        }
      ],
      "source": [
        "X, T = dataset(\"por\")\n",
        "\n",
        "assert np.all(T >= 0) and np.all(T <= 20)\n",
        "\n",
        "assert X.shape[0] == 27\n",
        "assert T.shape[0] == 3\n",
        "assert T.shape[1] == X.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(27, 649)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dfk_P5eG_Dr3"
      },
      "source": [
        "### Task 2: Input Data Standardization\n",
        "\n",
        "Since the data is in different input regimes, we want to standardize the data.\n",
        "For this purpose, we need to compute the mean and the standard deviation of the data for each input dimension.\n",
        "Then, we implement a function to perform the standardization of the data using the previously computed mean and standard deviation. Make sure that you handle the bias neuron $x_0$ correctly.\n",
        "\n",
        "Please note that `np` has all the functionality that you need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0bUpUAHR_Dr3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x shape prior standardisation:  (27, 649)\n",
            "x mean prior standardisation:  1.800490783541631\n",
            "x shape standardised:  (27, 649)\n",
            "x mean standardised:  0.037037037037037014\n"
          ]
        }
      ],
      "source": [
        "# compute mean and standard deviation over dataset\n",
        "mean = np.mean(X, axis=1)\n",
        "std = np.std(X, axis=1)\n",
        "# assure to handle x_0 correctly\n",
        "mean[0]=0 #mean[0] is the mean of the first element of the X-vector, i.e. the bias neuron\n",
        "std[0]=1 #avoid division by 0\n",
        "\n",
        "def standardize(x, mean, std):\n",
        "  # standardize the given data with the given mean and standard deviation\n",
        "  # why standardize by subject (649) and not standardise coefficient (27?\n",
        "  # we standardise by parameter and for this,  \n",
        "  # we substract the mean from each subject & parameter\n",
        "  return (x - mean) / std\n",
        "\n",
        "def standardize(x, mean, std):\n",
        "  # standardize the given data with the given mean and standard deviation\n",
        "  return np.transpose((np.transpose(x) - mean)/std)\n",
        "\n",
        "print('x shape prior standardisation: ', X.shape)\n",
        "print('x mean prior standardisation: ', X.mean())\n",
        "\n",
        "# standardize our dataset\n",
        "X = standardize(X, mean, std)\n",
        "print('x shape standardised: ', X.shape)\n",
        "print('x mean standardised: ', X.mean())\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6MZ_MPc-_Dr4"
      },
      "source": [
        "### Task 3: Batch Processing\n",
        "\n",
        "In order to run stochastic gradient descent, we need to split our dataset into batches of a certain batch size $B$. Implement a function that turns the dataset $(X,T)$ into batches of a certain batch size $B$.\n",
        "Implement this function as a generator function, i.e., use ``yield`` instead of ``return``.\n",
        "Circulate the dataset afresh when all data is consumed, and shuffle the data in each epoch.\n",
        "Make sure that you yield both the input batch and the target batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([543, 532, 234, 288, 527, 335,   8, 128, 413, 419, 278,  38,  33,\n",
              "       292, 216, 323, 147, 190, 319, 146, 208,   6, 469, 125, 616,  30,\n",
              "       490,  58, 412, 536, 395,  57, 135, 361, 468,  11, 502,  44, 153,\n",
              "       585,   0, 593, 196, 528, 421, 266, 259, 316, 383, 439, 450, 563,\n",
              "       338, 206, 154, 459, 162, 420, 291, 348, 425, 415, 297, 484,   3,\n",
              "        98, 444, 551, 167, 503, 117, 357, 121, 345,  55, 173, 198, 287,\n",
              "       497, 165, 243, 426, 100, 189, 334, 488, 467, 379, 640,  66,   9,\n",
              "       106, 391, 309, 237, 623, 356, 567, 409, 460, 457, 512, 199, 462,\n",
              "       333, 633, 276, 140, 609, 201, 181, 647, 519, 410, 483, 516, 542,\n",
              "       145,  93, 236, 166, 549, 170, 293, 368, 161, 553, 625, 621, 554,\n",
              "       359, 326, 377, 385,  14, 581, 504, 517, 521, 254, 417, 485, 407,\n",
              "       508, 634, 169, 362, 384, 149, 441, 533, 393, 312, 556, 101,  56,\n",
              "       431,  61, 630, 263, 494, 576,  10, 423, 360, 477, 641,  79, 279,\n",
              "       289, 535, 306, 299, 130, 510, 428, 339, 505, 171, 308, 575,  39,\n",
              "       481, 219, 325, 455, 561,  34,  65, 163, 544, 320, 142,  99, 284,\n",
              "       456, 365, 282, 604, 321, 349,   4,   2, 134, 122, 474, 244, 203,\n",
              "       472, 367, 341, 486, 267, 475, 113, 564, 440, 496, 495, 277, 228,\n",
              "       102, 307, 313, 463, 471, 399, 133,  51, 305,  40, 466, 378, 539,\n",
              "        62, 353, 400, 464, 223, 253, 346,  37, 104, 552, 271, 318, 204,\n",
              "       637, 221, 610,  26, 434, 108,  25, 235, 241,  50, 600, 514, 210,\n",
              "       280, 518, 572, 286, 608, 432, 340, 602, 230, 381, 487,  45, 506,\n",
              "        12, 594, 183, 605, 218, 392, 638, 332, 599, 118, 622,  47, 507,\n",
              "       586, 256, 185,  86, 442, 296, 562, 436, 240, 352,  75, 648,  82,\n",
              "       523, 213, 366, 141, 315,  78, 304, 583, 369, 513, 557, 172,  87,\n",
              "        42,  17, 547, 619, 274,  76, 526, 498, 329,   7, 283, 200, 382,\n",
              "       620, 364, 401, 546, 270, 178, 639,  64, 603, 565, 405, 644,  35,\n",
              "       491, 174, 202, 116, 112, 500, 545, 264, 224, 540,  28, 328, 250,\n",
              "       355, 164, 548, 192,  63, 571, 387, 260, 373,  27, 197, 406,  24,\n",
              "        31, 584, 322, 110, 209,  67, 492, 160, 245, 248,  70,  83, 157,\n",
              "       123, 612, 233,  73, 193, 194, 303,  74, 427,  71, 499, 628,  92,\n",
              "       179, 285,  97,  90, 295, 336, 107, 238, 363, 151, 124, 489, 150,\n",
              "       258, 559,  72, 143, 511, 470, 249, 645, 214, 327,  80, 155, 408,\n",
              "       558, 184, 433, 394, 159,  53, 290, 195, 482, 225, 534,  15, 298,\n",
              "       587, 480, 239, 158, 607, 569, 131, 246,   5,  16, 598, 411, 138,\n",
              "        77, 626, 212, 402, 376, 404, 579, 380, 642, 111, 281, 574, 588,\n",
              "       211, 501, 389, 388, 515, 342, 215,  48, 451,   1, 520, 350,  81,\n",
              "       317,  59, 537, 343, 589, 347, 252, 386, 636, 120, 629, 418, 375,\n",
              "       390,  52, 269, 220,  43,  95, 580, 631, 103,  91, 422,  29, 105,\n",
              "       560, 358, 541, 272, 371, 119, 611, 590,  84,  20, 624, 222, 137,\n",
              "       374, 615, 207, 452, 453, 152, 447, 205, 429, 524, 493, 227,  49,\n",
              "       465, 337, 156, 646, 109, 397,  18, 446,  46, 438, 136, 275, 331,\n",
              "       430,  60, 370, 606, 127, 596, 311, 330, 618, 424, 126, 344, 595,\n",
              "        94, 229, 473, 176,  19, 257, 531, 226, 461, 114, 448, 570, 458,\n",
              "        88, 568, 175,  22, 591, 454, 632, 398, 168, 301,  96, 435, 354,\n",
              "       414, 577, 351, 265,  13, 255, 251, 529, 416, 300, 324, 449, 635,\n",
              "       372,  69, 148, 614, 509, 302, 232,  54, 597, 522, 555, 525, 573,\n",
              "        89, 132, 566, 177, 476,  23, 268, 273, 294, 445, 613, 180,  21,\n",
              "       115, 182,  32, 188, 262, 403, 231, 443, 129, 582, 191, 578, 478,\n",
              "       601, 310,  41, 242, 314, 592, 247, 187, 479, 437, 186,  68, 396,\n",
              "       617, 261, 627, 643, 144,  36, 530, 538, 550, 217,  85, 139])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "indices = np.random.permutation(X.shape[1])\n",
        "indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GoDHo80J_Dr4"
      },
      "outputs": [],
      "source": [
        "def batch(X, T, batch_size=16):\n",
        "  counter = 0\n",
        "  while counter < X.shape[1]:\n",
        "    counter += 1\n",
        "    # shuffle dataset in each epoch\n",
        "    # we shuffle the patients not the coefficients \n",
        "    indices = np.random.permutation(X.shape[1])\n",
        "    # yield the batch\n",
        "    # first select the patients, then the first # patients according to batchsize\n",
        "    yield X[:,indices][:,:batch_size], T[:,indices][:,:batch_size]\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def batch(X, T, batch_size=16):\n",
        "  permutation = np.random.permutation(X.shape[1])\n",
        "  i = 0\n",
        "  # indicator if new epoch started\n",
        "  new_epoch = True\n",
        "  while True:\n",
        "    # shuffle dataset in each epoch\n",
        "    if (i + batch_size) >= X.shape[1]:\n",
        "      permutation = np.random.permutation(X.shape[1])\n",
        "      i = 0\n",
        "      new_epoch = True\n",
        "    # yield the batch\n",
        "    yield X[:, permutation[i:i + batch_size]], T[:, permutation[i:i + batch_size]], new_epoch\n",
        "    new_epoch = False\n",
        "    i += batch_size"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NgBsBOsa_Dr4"
      },
      "source": [
        "### Test 2: Test your Batches\n",
        "\n",
        "This test is to assure that your batch generation function works as expected. \n",
        "We define some test data for this purpose.\n",
        "The code below checks whether your batch function returns batches with correct content, i.e., $(\\vec x, \\vec t)$-alignment. \n",
        "It also checks that the batches are in the correct dimensions, i.e., that $\\mathbf X \\in \\mathbb R^{(D+1)\\times B}$ and $\\mathbf T \\in \\mathbb R^{O\\times B}$.\n",
        "\n",
        "Make sure you can pass this test before moving forward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fFndxbf1_CmR"
      },
      "outputs": [],
      "source": [
        "XX = np.array([[i] * 5 for i in range(50)]).T\n",
        "TT = np.array([[i] for i in range(10,60)]).T\n",
        "for counter, (x,t,e) in enumerate(batch(XX, TT, 16)):\n",
        "  assert x.shape[0] == 5\n",
        "  assert x.shape[1] == 16\n",
        "  assert t.shape[0] == 1\n",
        "  assert t.shape[1] == 16\n",
        "  assert np.all(x == t-10)\n",
        "  if counter == 20: break"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0B8o_mE1l6mp"
      },
      "source": [
        "## Gradient Descent\n",
        "\n",
        "To train a two-layer multi-output regression network, we need to implement some functions.\n",
        "The network output is computed in three steps:\n",
        "\n",
        "  * Compute network activation for a batch of inputs $\\mathbf X$: $\\mathbf A = \\mathbf W^{(1)}\\mathbf X$\n",
        "  * Call the activation function element-wise: $\\mathbf H = g(\\mathbf A)$. Here, we rely on the logistic activation function $\\sigma$. Assure that the hidden neuron bias $\\mathbf H_{0,:}$ is set appropriately.\n",
        "  * Compute the output $\\mathbf Y$ of the batch: $\\mathbf Y = \\mathbf W^{(2)}\\mathbf H$."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0pGCwC0L_Dr4"
      },
      "source": [
        "### Task 4: Multi-Output Network\n",
        "\n",
        "Implement a multi-target network that computes the output matrix $\\mathbf Y$ for a given input dataset/batch $\\mathbf X$ and given parameters $\\Theta=(\\mathbf W^{(1)}, \\mathbf W^{(2)})$ using `np` operations. \n",
        "The function should return both the output $\\mathbf Y$ and the output of the hidden units $\\mathbf H$ since we will need these in gradient descent. Select the logistic function $\\sigma$ as the activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "j1gErN4z_Dr4"
      },
      "outputs": [],
      "source": [
        "def network(X, Theta):\n",
        "  W1, W2 = Theta\n",
        "\n",
        "  # compute activation\n",
        "  A = np.dot(W1, X)\n",
        "\n",
        "  # compute hidden unit output\n",
        "  # using omega function from slides p. 4\n",
        "  H = 1 / (1 + np.exp(-A))\n",
        "  H[0] = 1 # bias neuron\n",
        "\n",
        "  # compute network output\n",
        "  Y = np.dot(W2, A)\n",
        "\n",
        "  return Y, H"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt0CMab4_Dr5"
      },
      "source": [
        "### Task 5: Loss Implementation\n",
        "\n",
        "Implement a loss function that returns the squared loss $\\mathcal J^{L_2} = \\frac1B \\|\\mathbf Y - \\mathbf T\\|_F^2$ for given network outputs $\\mathbf Y$ and target values $\\mathbf T$.\n",
        "Use `np` or `scipy` functionality for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RSwKiqIc_Dr5"
      },
      "outputs": [],
      "source": [
        "def loss(Y, T):\n",
        "  # Do not understand \n",
        "  B = T.shape[1]\n",
        "  return (1/B) * np.linalg.norm(Y-T, \"fro\") ** 2 #Frobenius norm"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fD2srCKN_Dr5"
      },
      "source": [
        "### Task 6: Gradient Implementation\n",
        "\n",
        "Implement a function that computes and returns the gradient for a given batch $(\\mathbf X, \\mathbf T)$, the given network outputs $\\mathbf Y$ and $\\mathbf H$ as well as current parameters $\\Theta=(\\mathbf W^{(1)}, \\mathbf W^{(2)})$.\n",
        "Make sure to compute the gradient with respect to both weight matrices. Remember that we have used $\\sigma$ as the activation function.\n",
        "Implement the function using the fast version provided in the lecture and make use of `np` operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Qe8QYVnP_Dr5"
      },
      "outputs": [],
      "source": [
        "def gradient(X, T, Y, H, Theta):\n",
        "  W1, W2 = Theta\n",
        "\n",
        "  # first layer gradient\n",
        "  g1 = (2 / Y.shape[1]) * np.dot((np.dot(W2.T, (Y - T))) * H * (1 - H), X.T)\n",
        "  # second layer gradient\n",
        "  g2 = (2 / Y.shape[1]) * np.dot((Y - T), H.T)\n",
        "  \n",
        "  return g1, g2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CpeCOHbE_Dr5"
      },
      "source": [
        "### Task 7: Iterative Gradient Descent\n",
        "\n",
        "\n",
        "Implement gradient descent for a given number of 10'000 epochs (**not batches!**) using given initial parameters $\\Theta$ and a given batch size $B$, as well as a learning rate of $\\eta=0.001$.\n",
        "\n",
        "Make use of the standardized dataset from Task 2, split into batches with the function from Task 3, the network from Task 4, the loss from Task 5, and the gradient from Task 6.\n",
        "\n",
        "Make sure that the network output $\\mathbf Y$ and the hidden unit output $\\mathbf H$ are computed only once for each batch. After applying gradient descent, add an option to use momentum learning with the given parameter `mu`.\n",
        "At the end of each epoch, compute and store the loss values for each batch in a list, and this list will be returned at the end.\n",
        "\n",
        "How many iterations do we need when $B < N$? How can you know whether your current batch is the last one of the current epoch?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QnuFqkg-82l2"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X, T, Theta, B , eta=0.001, mu=None):\n",
        "  loss_values = []\n",
        "\n",
        "  max_epochs = 10000\n",
        "  # gradient descent without batches\n",
        "    # iterate over batches\n",
        "  W1, W2 = Theta\n",
        "  W1_r_minus_1 = np.zeros(W1.shape)\n",
        "  W2_r_minus_1 = np.zeros(W2.shape)\n",
        "  W1_r = W1\n",
        "  W2_r = W2\n",
        "  \n",
        "  for epoch in range(max_epochs):\n",
        "    # iterate over batches\n",
        "    for n, (x,t) in enumerate(batch(X, T, batch_size=B)):\n",
        "        # compute network output\n",
        "        Y, H = network(x, Theta)\n",
        "        # compute and append loss\n",
        "        loss_values.append(loss(Y, t))\n",
        "\n",
        "        # compute gradient\n",
        "        g1, g2 = gradient(x, t, Y, H, Theta)\n",
        "        # store current and previous Theta for momentum learning\n",
        "        # There is no Theta^(r-1) on the first iteration, so we use a matrix of 0s instead\n",
        "        W1_r_minus_1 = W1_r\n",
        "        W2_r_minus_1 = W2_r\n",
        "        \n",
        "        W1_r = W1\n",
        "        W2_r = W2\n",
        "        \n",
        "        # and apply gradient descent\n",
        "        W1 -= eta * g1\n",
        "        W2 -= eta * g2\n",
        "        \n",
        "        # apply momentum learning if asked\n",
        "        if mu:\n",
        "          W1 += mu*(W1_r - W1_r_minus_1)\n",
        "          W2 += mu*(W2_r - W2_r_minus_1)\n",
        "\n",
        "\n",
        "          # return the obtained loss values at the end\n",
        "        return loss_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_descent(X, T, Theta, B, eta=0.001, mu=None):\n",
        "  loss_values = []\n",
        "  W1, W2 = Theta\n",
        "\n",
        "  max_epochs = 10000\n",
        "  max_batches = (T.shape[1] // B) * max_epochs\n",
        "\n",
        "  epoch_counter = 0\n",
        "  generator = batch(X, T, batch_size=B)\n",
        "  # iterate over batches\n",
        "  for i, (x,t,e) in enumerate(generator):\n",
        "    if e:\n",
        "      epoch_counter += 1\n",
        "      if i != 0:\n",
        "        loss_values.append(l)\n",
        "        if (epoch_counter > max_epochs):\n",
        "          break\n",
        "\n",
        "    # compute network output\n",
        "    y, h = network(x, (W1, W2))\n",
        "    # compute and append loss\n",
        "    l = loss(y, t)\n",
        "\n",
        "    # compute gradient\n",
        "    g1, g2 = gradient(x, t, y, h, (W1, W2))\n",
        "\n",
        "    # store current and previous Theta for momentum learning\n",
        "    if i == 0: # There is no Theta^(r-1) on the first iteration, so we use a matrix of 0s instead\n",
        "      W1_r_min_1 = np.zeros(W1.shape)\n",
        "      W2_r_min_1 = np.zeros(W2.shape)\n",
        "    else:\n",
        "      W1_r_min_1 = W1_r\n",
        "      W2_r_min_1 = W2_r\n",
        "    W1_r = W1\n",
        "    W2_r = W2\n",
        "    \n",
        "    # and apply gradient descent\n",
        "    W1 -= eta * g1\n",
        "    W2 -= eta * g2\n",
        "    \n",
        "    # apply momentum learning if asked\n",
        "    if mu:\n",
        "      W1 += mu*(W1_r - W1_r_min_1)\n",
        "      W2 += mu*(W2_r - W2_r_min_1)\n",
        "\n",
        "\n",
        "  # return the obtained loss values at the end\n",
        "  return loss_values"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NIvoljpW_Dr5"
      },
      "source": [
        "### Task 8: Run Gradient Descent\n",
        "\n",
        "Select an appropriate number of hidden neurons $K$.\n",
        "Instantiate the weight matrices $\\Theta=(\\mathbf W^{(1)}, \\mathbf W^{(2)})$ using the Xavier method as introduced in the lecture.\n",
        "\n",
        "Run the gradient descent three times, first as normal gradient descent, second as stochastic gradient descent with batch size $B=16$, and third with the same setup as the second but with momentum learning involved, select $\\mu =0.9$.\n",
        "\n",
        "How can you achieve this without requiring separate implementations of the ``gradient_descent`` function from Task 7?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "r4BnX1m8_Dr6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/hw/y61sswm55ss5_mz_3jfglnh03lrr77/T/ipykernel_68065/1306157415.py:9: RuntimeWarning: overflow encountered in exp\n",
            "  H = 1 / (1 + np.exp(-A))\n",
            "/var/folders/hw/y61sswm55ss5_mz_3jfglnh03lrr77/T/ipykernel_68065/405747607.py:5: RuntimeWarning: invalid value encountered in multiply\n",
            "  g1 = (2 / Y.shape[1]) * np.dot((np.dot(W2.T, (Y - T))) * H * (1 - H), X.T)\n"
          ]
        }
      ],
      "source": [
        "K = 20\n",
        "D = X.shape[0]\n",
        "O = 3\n",
        "# Xavier initialization\n",
        "W1 = np.random.uniform(-1/np.sqrt(D), 1/np.sqrt(D), (K+1,D))\n",
        "W2 = np.random.uniform(-1/np.sqrt(D), 1/np.sqrt(D), (O,K+1))\n",
        "Theta = [W1, W2]\n",
        "\n",
        "import copy\n",
        "\n",
        "# run gradient descent with full dataset\n",
        "Theta1 = copy.deepcopy(Theta)\n",
        "GD = gradient_descent(X, T, Theta1, X.shape[1])\n",
        "\n",
        "# run stochastic gradient descent with batches of size 16\n",
        "Theta2 = copy.deepcopy(Theta)\n",
        "SGD = gradient_descent(X, T, Theta2, 16)\n",
        "\n",
        "# run stochastic gradient descent with batches of size 16 and momentum mu=0.9\n",
        "Theta3 = copy.deepcopy(Theta)\n",
        "SGD_Mo = gradient_descent(X, T, Theta2, 16, mu=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " nan,\n",
              " ...]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SGD_Mo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi5Wt88CnsNB"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Finally, we want to evaluate how the learning process went and what the network has actually learned.\n",
        "For the former, we will plot the loss values obtained during training.\n",
        "For the latter, we define one specific sample of our own, and we evaluate the impact of several factors on the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1CGUw7k_Dr6"
      },
      "source": [
        "### Task 9: Plotting Loss Progression\n",
        "\n",
        "To show the learning process of the networks, plot the loss values of the three gradient descent steps from Task 8 together into one plot.\n",
        "Do we need to take care of something when plotting both together?\n",
        "\n",
        "Use logarithmic axes wherever you see fit.\n",
        "An exemplary loss progression plot can be found in the slides.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "wMFMvuRg_Dr6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x114ea7730>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot\n",
        "\n",
        "pyplot.plot(SGD, \"b-\", label=\"Stochastic Gradient Descent\")\n",
        "pyplot.plot(SGD_Mo, \"r-\", label=\"Stochastic Gradient Descent with Momentum\")\n",
        "pyplot.plot(GD, \"g-\", label=\"Gradient Descent\")\n",
        "pyplot.legend()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sDJp99mK_Dr6"
      },
      "source": [
        "### Task 10: Example Evaluation (optional)\n",
        "\n",
        "We want to see what the network has learned.\n",
        "Therefore, we evaluate some data points that would represent a typical Swiss student (except for the school entry, where we select one of them randomly).\n",
        "You can select a specific example, but you can also imagine a student.\n",
        "Please refer to https://archive.ics.uci.edu/ml/datasets/Student+Performance on possible values and the implementation in Tasks 1 and 2 on how to generate an input sample $\\vec x$ for our network. Also, remember that input data need to be standardized before feeding it to the network. \n",
        "\n",
        "Compute the scores that your student would likely get by asking the network, using the parameters $\\Theta$ optimized with stochastic gradient descent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9RPCifeO_Dr6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction :\t Ellipsis\n"
          ]
        }
      ],
      "source": [
        "# select a specific example\n",
        "example = np.array([[\n",
        "  1.,    # BIAS\n",
        "  ...,   # school (select -1 or 1)\n",
        "  ...,   # gender\n",
        "  ...,   # age\n",
        "  ...,   # adress\n",
        "  ...,   # family size\n",
        "  ...,   # parents living together\n",
        "  ...,   # mother education\n",
        "  ...,   # father education\n",
        "  ...,   # travel time\n",
        "  ...,   # study time\n",
        "  ...,   # class failure before\n",
        "\n",
        "  ...,   # support from school\n",
        "  ...,   # support from the family\n",
        "  ...,   # paid extra support \n",
        "  ...,   # out-of-school activities\n",
        "  ...,   # nursery school\n",
        "  ...,   # want to do higher ed\n",
        "  ...,   # internet access\n",
        "  ...,   # romantic relation\n",
        "\n",
        "  ...,   # relation to family\n",
        "  ...,   # amount of free time\n",
        "  ...,   # go out with peers\n",
        "  ...,   # alcoholic drinks during the week\n",
        "  ...,   # alcoholic drinks in the weekend\n",
        "  ...,   # health status\n",
        "  ...    # days of absence\n",
        "]]).T\n",
        "\n",
        "# compute network output\n",
        "prediction = ...\n",
        "print(\"Prediction :\\t\", prediction)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9lB1b-8V_Dr6"
      },
      "source": [
        "### Task 11: Influence of Data Dimensions (optional)\n",
        "\n",
        "\n",
        "For some dimensions in the input feature $\\vec x$, we want to test how different input values for this dimension would influence the outcome.\n",
        "Particularly, we test:\n",
        "\n",
        "  * Gender at index $d=2$: change between male ($1$) and female ($-1$)\n",
        "  * Weekly study time at index $d=10$: vary in the range $[1,4]$ \n",
        "  * Past Failures at index $d=11$: vary in range $[0,3]$ \n",
        "  * Additional classes at index $d=14$: change between yes ($1$) and no ($-1$)\n",
        "  * Romantic relations at index $d=19$: change between yes ($1$) and no ($-1$)\n",
        "  * Weekday alcohol consumption at index $d=23$: varies in the range $[1,6]$.\n",
        "\n",
        "Note that the indexes include the fact that we are omitting some input dimensions, so they might differ from what is listed on the webpage.\n",
        "\n",
        "Did you expect this output?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "buTufJpd_Dr6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# implement a way to modify the input at a given index with certain values\n",
        "# and to predict and print the network output for this modification\n",
        "...\n",
        "# run this with the 4 modifications and their according to values as seen above\n",
        "..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "2dd53f8ad749bca69f7250ce75eb4f0def59db5cf79075a9716322ffc58e8a2e"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('DL')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
