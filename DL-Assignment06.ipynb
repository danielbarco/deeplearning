{"cells":[{"cell_type":"markdown","metadata":{"id":"eH27MSc7aQLq"},"source":["# Assignment 6: Convolutional Networks\n","\n","Before we start, we should assure that we have activated CUDA -- otherwise training might take very long.\n","In Google Colaboratory:\n","\n","1. Check the options Runtime -> Change Runtime Type on top of the page.\n","2. In the popup window, select hardware accelerator GPU.\n","\n","Afterward, the following command should run successfully:"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4448,"status":"ok","timestamp":1677241993135,"user":{"displayName":"Xinyi Zhang","userId":"00376105640142165248"},"user_tz":-60},"id":"jPstyY7AaQLv","outputId":"94c07ab8-d6ab-4ef1-8c6c-b3a54de1ba58"},"outputs":[{"name":"stdout","output_type":"stream","text":["Successfully enabled CUDA processing\n"]}],"source":["import torch\n","if torch.cuda.is_available():\n","  print(\"Successfully enabled CUDA processing\")\n","else:\n","  print(\"CUDA processing not available. Things will be slow :-(\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"W08ImtCeaQLw"},"source":["## Dataset\n","\n","In PyTorch, a dataset stores a list of input and target tensors $(X^n, T^n)$.\n","In case of **MNIST** dataset, the inputs are $X^n \\in \\mathbb R^{28\\times28}$ and $T^n \\in \\{0,\\ldots,9\\}$.\n","More precisely, the data in the dataset is provided in form of `PIL.Image.Image`, which represents an image class with some more functionality, and pixel values in range $[0, 255]$.\n","In order to convert these images into `torch.Tensor`'s in range $[0,1]$, we can use the [ToTensor](https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html) transform.\n","Furthermore, in `PyTorch` batches are created from datasets using the [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"q9s7rhj4iDKu"},"source":["\n","### Task 1: Dataset Loading\n","\n","\n","Here, we use the [MNIST dataset](https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html) of gray images for categorical classification.\n","\n","Write a function that returns the training and the testing set of MNIST, using the given transform."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n","  warn(f\"Failed to load image Python extension: {e}\")\n"]}],"source":["import torch\n","import torchvision"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"UdAXKwqUaQLx"},"outputs":[],"source":["\n","\n","def datasets(transform):\n","  trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","  testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","\n","  return trainset, testset"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2Abp751vaQLx"},"source":["### Test 1: Data Types\n","\n","When we create the dataset with `transform=None`, all inputs shall be of type `PIL.Image.Image`, and all targets are integral."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"hAMpr6hhaQLx"},"outputs":[{"ename":"AttributeError","evalue":"module 'torch' has no attribute '_six'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mPIL\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainset, testset \u001b[39m=\u001b[39m datasets(transform\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m x,t \u001b[39min\u001b[39;00m trainset:\n\u001b[1;32m      5\u001b[0m   \u001b[39m# check datatype of input x\u001b[39;00m\n\u001b[1;32m      6\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(x, PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mImage)\n","Cell \u001b[0;32mIn[3], line 2\u001b[0m, in \u001b[0;36mdatasets\u001b[0;34m(transform)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdatasets\u001b[39m(transform):\n\u001b[0;32m----> 2\u001b[0m   trainset \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39;49mdatasets\u001b[39m.\u001b[39;49mMNIST(root\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./data\u001b[39;49m\u001b[39m'\u001b[39;49m, train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, download\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, transform\u001b[39m=\u001b[39;49mtransform)\n\u001b[1;32m      3\u001b[0m   testset \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mMNIST(root\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./data\u001b[39m\u001b[39m'\u001b[39m, train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, transform\u001b[39m=\u001b[39mtransform)\n\u001b[1;32m      5\u001b[0m   \u001b[39mreturn\u001b[39;00m trainset, testset\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/mnist.py:91\u001b[0m, in \u001b[0;36mMNIST.__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     84\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     85\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m     download: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     90\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(root, transform\u001b[39m=\u001b[39;49mtransform, target_transform\u001b[39m=\u001b[39;49mtarget_transform)\n\u001b[1;32m     92\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain \u001b[39m=\u001b[39m train  \u001b[39m# training set or test set\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_legacy_exist():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/vision.py:39\u001b[0m, in \u001b[0;36mVisionDataset.__init__\u001b[0;34m(self, root, transforms, transform, target_transform)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     32\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     33\u001b[0m     root: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     target_transform: Optional[Callable] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     37\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     _log_api_usage_once(\u001b[39mself\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(root, torch\u001b[39m.\u001b[39;49m_six\u001b[39m.\u001b[39mstring_classes):\n\u001b[1;32m     40\u001b[0m         root \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexpanduser(root)\n\u001b[1;32m     41\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot \u001b[39m=\u001b[39m root\n","\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute '_six'"]}],"source":["import PIL\n","trainset, testset = datasets(transform=None)\n","\n","for x,t in trainset:\n","  # check datatype of input x\n","  assert isinstance(x, PIL.Image.Image)\n","  # check datatype of target t\n","  assert isinstance(t, int)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"rEhGkNEdaQLy"},"source":["### Task 2: Data Loaders\n","\n","\n","Create the dataset with transform `ToTensor`. Create two data loaders, one for the training set and one for the testing set. The training batch size should be $B=64$, for the testing set, you can select any batch size of your choice.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"72TgSAmDaQLy"},"outputs":[],"source":["transform = torchvision.transforms.ToTensor\n","trainset, testset = datasets(transform=transform)\n","\n","B = 64\n","trainloader = torch.utils.data.DataLoader(...)\n","testloader = torch.utils.data.DataLoader(...)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_VmIeKXQaQLz"},"source":["### Test 2: Batches\n","\n","Check that all batches generated by the training set data loader have the batch size of $B$ -- except for the last batch, which you need to compute by yourself (you might want to make use of the modulo operator `%`).\n","\n","Afterward, we check that all inputs and targets are of type `torch.Tensor`, that all input values are in range $[0,1]$ and that all target values are in range $[0,9]$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XQ-wtDlPaQL0"},"outputs":[],"source":["# compute the size of last batch\n","last_batch = ...\n","\n","for x,t in trainloader:\n","  # check datatype, size and content of x\n","  assert isinstance(x, torch.Tensor)\n","  assert(torch.all(x >= 0)) and torch.all(x <= 1)  \n","  assert len(x)==B or len(x) == last_batch\n","\n","  # check datatype, size and content of t\n","  assert isinstance(t, torch.Tensor)\n","  assert(torch.all(t >= 0)) and torch.all(t <= 9)\n","  assert len(t)==B or len(t) == last_batch"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HCcVzXbiaQL0"},"source":["## Networks\n","As last week, we will rely on [Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) to create networks with particular lists of consecutive layers.\n","Particularly, we will investigate two different versions of networks, one fully-connected network and one convolutional network, with the same number of learnable layers.\n","\n","### Task 3: Fully-Connected Network\n","\n","\n","Implement a function that returns a three-layer fully-connected network in `pytorch`.\n","Use $\\tanh$ as activation function between the two fully-connected layers, and provide the possibility to change the number of inputs $D$, the number of hidden neurons $K$ and the number of outputs $O$.\n","Use the following layers:\n","\n","1. A [Flatten layer](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) to turn the $28\\times28$ pixel image (2D) into a $28\\cdot28$ pixel vector (1D).\n","2. A [fully-connected layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) with D input neurons and K outputs.\n","3. A [$\\tanh$ activation function](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html).\n","4. A fully-connected layer with K input neurons and K outputs.\n","5. A $\\tanh$ activation function.\n","6. A fully-connected layer with K input neurons and O outputs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kUllttuBaQL0"},"outputs":[],"source":["def fully_connected(D, K, O):\n","  return torch.nn.Sequential(\n","    ...\n","  )"]},{"cell_type":"markdown","metadata":{"id":"_nL0shNEaQL1"},"source":["### Task 4: Convolutions Output (theoretical question)\n","\n","Consider the network as defined in Task 5.\n","Assume that the input is a $28\\times28$ grayscale image.\n","How many hidden neurons do we need in the final fully-connected layer for a given number $Q_2$ of output channels of the second convolution?\n","\n","..."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"OkWbJWGbaQL1"},"source":["### Task 5: Convolutional Network\n","\n","Implement a function that generates a convolutional network with the following layers:\n","\n","1. A [2D convolutional layer](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) with $Q_1$ channels, kernel size $5\\times5$, stride 1 and padding 2.\n","2. A [2D maximum pooling](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) with pooling size $2\\times2$ and stride 2.\n","3. A $\\tanh$ activation function.\n","4. A 2D convolutional layer with $Q_2$ channels, kernel size $5\\times5$, stride 1 and padding 2.\n","5. A 2D maximum pooling with pooling size $2\\times2$ and stride 2.\n","6. A $\\tanh$ activation function.\n","7. A flattening layer to turn the 3D feature map into a 1D vector.\n","8. A fully-connected layer with the appropriate number of inputs and $O$ outputs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mS9cuYsSaQL1"},"outputs":[],"source":["def convolutional(Q1, Q2, O):\n","  return torch.nn.Sequential(\n","    ...\n","  )"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6IL7ji_qaQL2"},"source":["## Network Training\n","For training and evaluating the network, we rely on standard functionality in PyTorch.\n","We use the standard [categorical cross-entropy loss](https://pytorch.org/docs/stable/nn.html#loss-functions) together with a [stochastic gradient descent](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) optimizer.\n","For training, we use the batched implementation of the dataset, for which we perform one update step for each training batch.\n","After having gone through the full training dataset, we compute accuracy and loss values for the testing set (we simply make use of the testing set for validation).\n","\n","\n","### Task 6: Training and Validation Loop\n","\n","Implement a function that takes the network, the number of epochs, and the learning rate.\n","Select the correct loss function for categorical classification and SGD optimizer.\n","Iterate the following steps for the given number of epochs:\n","\n","1. Train the network with all batches of the training data.\n","2. Compute the testing set loss and testing set accuracy.\n","3. Store both in a vector.\n","\n","What do we need to take care of?\n","\n","Finally, return the lists of validation losses and accuracies."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I8q12fyYaQL2"},"outputs":[],"source":["def train(network, epochs, eta, momentum):\n","  # select loss function and optimizer\n","  loss = ...\n","  optimizer = ...\n","\n","  # instantiate the correct device\n","  device = torch.device(\"cuda\")\n","  network = network.to(device)\n","\n","  # collect loss values and accuracies over the training epochs\n","  val_loss, val_acc = [], []\n","\n","  for epoch in range(epochs):\n","    # train network on training data\n","    for x,t in trainloader:\n","      # put data to device\n","      ...\n","      # train\n","      ...\n","\n","    # test network on test data\n","    with torch.no_grad():\n","      for x,t in testloader:\n","        # put data to device\n","        ...\n","        # compute validation loss\n","        ...\n","        # compute validation accuracy\n","        ...\n","\n","\n","  # return loss and accuracy values\n","  return val_loss, val_acc"]},{"cell_type":"markdown","metadata":{"id":"tEssOE_PaQL2"},"source":["### Task 7: Fully-Connected Training\n","\n","Create a fully-connected network with $K=100$ hidden and $O=10$ output neurons.\n","Train the network for 10 epochs with $\\eta=0.01$, $momentum=0.9$ and store the obtained test losses and accuracies.\n","Brave people can also train for 100 epochs (which will take up to 30 minutes)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EjW7zpuvaQL2"},"outputs":[],"source":["fc = fully_connected(...)\n","fc_loss, fc_acc = train(...)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"eGW5qI6naQL3"},"source":["### Task 8: Convolutional Training\n","\n","\n","Create a convolutional network with $Q_1=16$ and $Q_2=32$ convolutional channels and $O=10$ output neurons.\n","Train the network for 10 epochs with $\\eta=0.01$, momentum $=0.9$ and store the obtained test losses and accuracies.\n","Again, 100 epochs will take up to 30 minutes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4hQSxjIEaQL3"},"outputs":[],"source":["cv = convolutional(...)\n","cv_loss, cv_acc = train(...)"]},{"cell_type":"markdown","metadata":{"id":"R2lhQiuyaQL3"},"source":["### Task 9: Plotting\n","\n","Plot the two lists of loss values in one plot. Plot the two lists of accuracy values into another plot."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8SMsfktXaQL3"},"outputs":[],"source":["from matplotlib import pyplot\n","pyplot.figure(figsize=(10,3))\n","ax = pyplot.subplot(121)\n","# plot loss values of FC and CV network over epochs\n","...\n","\n","ax = pyplot.subplot(122)\n","# plot accuracy values of FC and CV network over epochs\n","..."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vua5OAWnaQL3"},"source":["### Task 10: Learnable Parameters \n","\n","Estimate roughly how many learnable parameters the two networks have by analytically computing and adding the number of parameters in each layer.\n","\n","Fully-connected Network:\n","- first fully-connected layer: ...\n","- second fully-connected layer: ...\n","- third fully-connected layer: ...\n","- total: ...\n","\n","Convolutional Network:\n","- first convolutional layer: ...\n","- second convolutional layer: ...\n","- fully-connected layer: ...\n","- total: ...\n","\n","\n","\n","Now, compute the number of parameters in the networks by summing the number of parameters in each layer using `pytorch` functionality.\n","You can use the `numel()` function from a `torch.Tensor` to provide the number of (learnable) parameters stored in a tensor.\n","How do you obtain this list of all learnable parameters from a `pytorch` [Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aC_ZOtKUaQL4"},"outputs":[],"source":["def parameter_count(network):\n","  return ...\n","\n","print(\"Fully-connected Network:\", parameter_count(fc))\n","print(\"Convolutional Network:\", parameter_count(cv))"]}],"metadata":{"colab":{"provenance":[]},"interpreter":{"hash":"2dd53f8ad749bca69f7250ce75eb4f0def59db5cf79075a9716322ffc58e8a2e"},"kernelspec":{"display_name":"Python 3.8.10 ('DL')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
