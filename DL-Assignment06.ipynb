{"cells":[{"cell_type":"markdown","metadata":{"id":"eH27MSc7aQLq"},"source":["# Assignment 6: Convolutional Networks\n","\n","Before we start, we should assure that we have activated CUDA -- otherwise training might take very long.\n","In Google Colaboratory:\n","\n","1. Check the options Runtime -> Change Runtime Type on top of the page.\n","2. In the popup window, select hardware accelerator GPU.\n","\n","Afterward, the following command should run successfully:"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4448,"status":"ok","timestamp":1677241993135,"user":{"displayName":"Xinyi Zhang","userId":"00376105640142165248"},"user_tz":-60},"id":"jPstyY7AaQLv","outputId":"94c07ab8-d6ab-4ef1-8c6c-b3a54de1ba58"},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["Successfully enabled CUDA processing\n"]}],"source":["import torch\n","if torch.cuda.is_available():\n","  print(\"Successfully enabled CUDA processing\")\n","else:\n","  print(\"CUDA processing not available. Things will be slow :-(\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"W08ImtCeaQLw"},"source":["## Dataset\n","\n","In PyTorch, a dataset stores a list of input and target tensors $(X^n, T^n)$.\n","In case of **MNIST** dataset, the inputs are $X^n \\in \\mathbb R^{28\\times28}$ and $T^n \\in \\{0,\\ldots,9\\}$.\n","More precisely, the data in the dataset is provided in form of `PIL.Image.Image`, which represents an image class with some more functionality, and pixel values in range $[0, 255]$.\n","In order to convert these images into `torch.Tensor`'s in range $[0,1]$, we can use the [ToTensor](https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html) transform.\n","Furthermore, in `PyTorch` batches are created from datasets using the [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"q9s7rhj4iDKu"},"source":["\n","### Task 1: Dataset Loading\n","\n","\n","Here, we use the [MNIST dataset](https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html) of gray images for categorical classification.\n","\n","Write a function that returns the training and the testing set of MNIST, using the given transform."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import torch\n","import torchvision"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"UdAXKwqUaQLx"},"outputs":[],"source":["\n","\n","def datasets(transform):\n","  trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","  testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","\n","  return trainset, testset"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2Abp751vaQLx"},"source":["### Test 1: Data Types\n","\n","When we create the dataset with `transform=None`, all inputs shall be of type `PIL.Image.Image`, and all targets are integral."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"hAMpr6hhaQLx"},"outputs":[],"source":["import PIL\n","trainset, testset = datasets(transform=None)\n","\n","for x,t in trainset:\n","  # check datatype of input x\n","  assert isinstance(x, PIL.Image.Image)\n","  # check datatype of target t\n","  assert isinstance(t, int)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"rEhGkNEdaQLy"},"source":["### Task 2: Data Loaders\n","\n","\n","Create the dataset with transform `ToTensor`. Create two data loaders, one for the training set and one for the testing set. The training batch size should be $B=64$, for the testing set, you can select any batch size of your choice.\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"72TgSAmDaQLy"},"outputs":[],"source":["transform = torchvision.transforms.ToTensor()\n","trainset, testset = datasets(transform=transform)\n","\n","B = 64\n","trainloader = torch.utils.data.DataLoader(dataset = trainset, batch_size=B)\n","testloader = torch.utils.data.DataLoader(dataset = testset, batch_size=B)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_VmIeKXQaQLz"},"source":["### Test 2: Batches\n","\n","Check that all batches generated by the training set data loader have the batch size of $B$ -- except for the last batch, which you need to compute by yourself (you might want to make use of the modulo operator `%`).\n","\n","Afterward, we check that all inputs and targets are of type `torch.Tensor`, that all input values are in range $[0,1]$ and that all target values are in range $[0,9]$."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["32"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["len(trainset) % B"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"XQ-wtDlPaQL0"},"outputs":[],"source":["# compute the size of last batch\n","last_batch = len(trainset) % B\n","\n","for x,t in trainloader:\n","  # check datatype, size and content of x\n","  assert isinstance(x, torch.Tensor)\n","  assert(torch.all(x >= 0)) and torch.all(x <= 1)  \n","  assert len(x)==B or len(x) == last_batch\n","\n","  # check datatype, size and content of t\n","  assert isinstance(t, torch.Tensor)\n","  assert(torch.all(t >= 0)) and torch.all(t <= 9)\n","  assert len(t)==B or len(t) == last_batch"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HCcVzXbiaQL0"},"source":["## Networks\n","As last week, we will rely on [Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) to create networks with particular lists of consecutive layers.\n","Particularly, we will investigate two different versions of networks, one fully-connected network and one convolutional network, with the same number of learnable layers.\n","\n","### Task 3: Fully-Connected Network\n","\n","\n","Implement a function that returns a three-layer fully-connected network in `pytorch`.\n","Use $\\tanh$ as activation function between the two fully-connected layers, and provide the possibility to change the number of inputs $D$, the number of hidden neurons $K$ and the number of outputs $O$.\n","Use the following layers:\n","\n","1. A [Flatten layer](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) to turn the $28\\times28$ pixel image (2D) into a $28\\cdot28$ pixel vector (1D).\n","2. A [fully-connected layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) with D input neurons and K outputs.\n","3. A [$\\tanh$ activation function](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html).\n","4. A fully-connected layer with K input neurons and K outputs.\n","5. A $\\tanh$ activation function.\n","6. A fully-connected layer with K input neurons and O outputs."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"kUllttuBaQL0"},"outputs":[],"source":["def fully_connected(D, K, O):\n","  return torch.nn.Sequential(\n","    # no parameters are required in flatten\n","    torch.nn.Flatten(),\n","    torch.nn.Linear(in_features= D, out_features=K),\n","    torch.nn.Tanh(),\n","    torch.nn.Linear(in_features= K, out_features=K),\n","    torch.nn.Tanh(),\n","    torch.nn.Linear(in_features= K, out_features=O),\n","\n","  )"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","           Flatten-1                  [-1, 784]               0\n","            Linear-2                  [-1, 100]          78,500\n","              Tanh-3                  [-1, 100]               0\n","            Linear-4                  [-1, 100]          10,100\n","              Tanh-5                  [-1, 100]               0\n","            Linear-6                   [-1, 10]           1,010\n","================================================================\n","Total params: 89,610\n","Trainable params: 89,610\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.01\n","Params size (MB): 0.34\n","Estimated Total Size (MB): 0.35\n","----------------------------------------------------------------\n"]}],"source":["from torchsummary import summary\n","\n","fc = fully_connected(D=28*28, K=100, O=10 )\n","network = fc\n","device = torch.device(\"cuda\")\n","network = network.to(device)\n","summary(network, (1,28,28))"]},{"cell_type":"markdown","metadata":{"id":"_nL0shNEaQL1"},"source":["### Task 4: Convolutions Output (theoretical question)\n","\n","Consider the network as defined in Task 5.\n","Assume that the input is a $28\\times28$ grayscale image.\n","How many hidden neurons do we need in the final fully-connected layer for a given number $Q_2$ of output channels of the second convolution?\n","\n","..."]},{"cell_type":"markdown","metadata":{},"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"OkWbJWGbaQL1"},"source":["### Task 5: Convolutional Network\n","\n","Implement a function that generates a convolutional network with the following layers:\n","\n","1. A [2D convolutional layer](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) with $Q_1$ channels, kernel size $5\\times5$, stride 1 and padding 2.\n","2. A [2D maximum pooling](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) with pooling size $2\\times2$ and stride 2.\n","3. A $\\tanh$ activation function.\n","4. A 2D convolutional layer with $Q_2$ channels, kernel size $5\\times5$, stride 1 and padding 2.\n","5. A 2D maximum pooling with pooling size $2\\times2$ and stride 2.\n","6. A $\\tanh$ activation function.\n","7. A flattening layer to turn the 3D feature map into a 1D vector.\n","8. A fully-connected layer with the appropriate number of inputs and $O$ outputs."]},{"cell_type":"code","execution_count":42,"metadata":{"id":"mS9cuYsSaQL1"},"outputs":[],"source":["def convolutional(Q1, Q2, O):\n","  return torch.nn.Sequential(\n","      # careful in_channels = 1 not Q1\n","      torch.nn.Conv2d(in_channels=1, out_channels=Q1, kernel_size= (5,5), stride=1, padding=2),\n","      torch.nn.MaxPool2d(kernel_size=(2,2), stride=2),\n","      torch.nn.Tanh(),\n","      torch.nn.Conv2d(in_channels=Q1, out_channels=Q2, kernel_size= (5,5), stride=1, padding=2),\n","      torch.nn.MaxPool2d(kernel_size=(2,2), stride=2),\n","      torch.nn.Tanh(),\n","      # flatten without parameters\n","      torch.nn.Flatten(),\n","      torch.nn.Linear(in_features=Q2 * 7 * 7, out_features=O)\n","  )"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 16, 28, 28]             416\n","         MaxPool2d-2           [-1, 16, 14, 14]               0\n","              Tanh-3           [-1, 16, 14, 14]               0\n","            Conv2d-4           [-1, 32, 14, 14]          12,832\n","         MaxPool2d-5             [-1, 32, 7, 7]               0\n","              Tanh-6             [-1, 32, 7, 7]               0\n","           Flatten-7                 [-1, 1568]               0\n","            Linear-8                   [-1, 10]          15,690\n","================================================================\n","Total params: 28,938\n","Trainable params: 28,938\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.23\n","Params size (MB): 0.11\n","Estimated Total Size (MB): 0.34\n","----------------------------------------------------------------\n"]}],"source":["from torchsummary import summary\n","\n","cv = convolutional(Q1 = 16, Q2 =32, O=10 )\n","network = cv \n","device = torch.device(\"cuda\")\n","network = network.to(device)\n","summary(network, (1,28,28))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6IL7ji_qaQL2"},"source":["## Network Training\n","For training and evaluating the network, we rely on standard functionality in PyTorch.\n","We use the standard [categorical cross-entropy loss](https://pytorch.org/docs/stable/nn.html#loss-functions) together with a [stochastic gradient descent](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) optimizer.\n","For training, we use the batched implementation of the dataset, for which we perform one update step for each training batch.\n","After having gone through the full training dataset, we compute accuracy and loss values for the testing set (we simply make use of the testing set for validation).\n","\n","\n","### Task 6: Training and Validation Loop\n","\n","Implement a function that takes the network, the number of epochs, and the learning rate.\n","Select the correct loss function for categorical classification and SGD optimizer.\n","Iterate the following steps for the given number of epochs:\n","\n","1. Train the network with all batches of the training data.\n","2. Compute the testing set loss and testing set accuracy.\n","3. Store both in a vector.\n","\n","What do we need to take care of?\n","\n","Finally, return the lists of validation losses and accuracies."]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["def accuracy(Z, T):\n","  # check if we have binary or categorical classification\n","  # for binary classification, we will have a two-dimensional target tensor\n","  if len(T.shape) == 2:\n","    # binary classification\n","    # If z is equal or larger than the threshold 0.5, then we predict 1, otherwise 0 \n","    # we use the .float() function to convert the boolean to a float\n","    # then we compare the prediction with the target and compute the mean\n","    \n","    # ??? our data is binary between 0 and 1, so why do you use 0 as threshold ???\n","    # So only if we use sigmoid activation function for binary or softmax for multi-class\n","    # after the last layer, we need to use 0.5 as threshold\n","    return torch.mean(((Z>=0).float() == T).float())\n","\n","  else:\n","    # categorical classification\n","    # the argmax function returns the index of the maximum value\n","    # we use the .float() function to convert the boolean to a float\n","    # then we compare the prediction with the target and compute the mean\n","    # return torch.mean((torch.argmax(Z, dim=1).float() == T).float())\n","    \n","    # Y is the index of the maximum value in Z\n","    Y = torch.argmax(Z, dim=1)\n","    return torch.mean((Y == T).float())"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"I8q12fyYaQL2"},"outputs":[],"source":["def train(network, epochs, eta, momentum):\n","  # select loss function and optimizer\n","  loss = torch.nn.BCELoss()\n","  optimizer = torch.optim.SGD(\n","    network.parameters(), \n","    lr=eta,\n","    momentum= momentum,\n","  )\n","\n","  # instantiate the correct device\n","  device = torch.device(\"cuda\")\n","  network = network.to(device)\n","\n","  # collect loss values and accuracies over the training epochs\n","  val_loss, val_acc = [], []\n","\n","  for epoch in range(epochs):\n","    # train network on training data\n","    for x,t in trainloader:\n","      # put data to device\n","      x, t = x.to(device), t.to(device)\n","      # train\n","      # train on training set\n","      # ... compute network output on training data\n","      Z = network(x)\n","      # ... compute loss from network output and target data\n","      loss = loss(Z, t)\n","      loss.backward()\n","      # ... perform parameter update\n","      optimizer.step()\n","      # ... remember loss\n","      loss.append(loss.item())\n","\n","\n","    # test network on test data\n","    with torch.no_grad():\n","      for x,t in testloader:\n","        # put data to device\n","        x, t = x.to(device), t.to(device)\n","        # compute validation loss\n","        Z = network(x)\n","        # ... compute loss from network output and target data\n","        loss = loss(Z, t)\n","        # ... remember loss\n","        val_loss.append(loss.item())\n","        # ... compute validation set accuracy\n","        val_acc.append(accuracy(Z, t).item())\n","\n","\n","\n","  # return loss and accuracy values\n","  return val_loss, val_acc"]},{"cell_type":"markdown","metadata":{"id":"tEssOE_PaQL2"},"source":["### Task 7: Fully-Connected Training\n","\n","Create a fully-connected network with $K=100$ hidden and $O=10$ output neurons.\n","Train the network for 10 epochs with $\\eta=0.01$, $momentum=0.9$ and store the obtained test losses and accuracies.\n","Brave people can also train for 100 epochs (which will take up to 30 minutes)."]},{"cell_type":"code","execution_count":28,"metadata":{"id":"EjW7zpuvaQL2"},"outputs":[{"ename":"ValueError","evalue":"Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 10])) is deprecated. Please ensure they have the same size.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m fc \u001b[39m=\u001b[39m fully_connected(D\u001b[39m=\u001b[39m\u001b[39m28\u001b[39m\u001b[39m*\u001b[39m\u001b[39m28\u001b[39m, K\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, O\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m )\n\u001b[0;32m----> 2\u001b[0m fc_loss, fc_acc \u001b[39m=\u001b[39m train(network \u001b[39m=\u001b[39;49m fc, epochs \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m, eta \u001b[39m=\u001b[39;49m \u001b[39m0.01\u001b[39;49m, momentum \u001b[39m=\u001b[39;49m \u001b[39m0.9\u001b[39;49m)\n","Cell \u001b[0;32mIn[26], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(network, epochs, eta, momentum)\u001b[0m\n\u001b[1;32m     25\u001b[0m Z \u001b[39m=\u001b[39m network(x)\n\u001b[1;32m     26\u001b[0m \u001b[39m# ... compute loss from network output and target data\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m loss \u001b[39m=\u001b[39m loss(Z, t)\n\u001b[1;32m     28\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     29\u001b[0m \u001b[39m# ... perform parameter update\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 619\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3084\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3085\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize():\n\u001b[0;32m-> 3086\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3087\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3088\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m   3089\u001b[0m     )\n\u001b[1;32m   3091\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3092\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n","\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 10])) is deprecated. Please ensure they have the same size."]}],"source":["fc = fully_connected(D=28*28, K=100, O=10 )\n","fc_loss, fc_acc = train(network = fc, epochs = 100, eta = 0.01, momentum = 0.9)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"eGW5qI6naQL3"},"source":["### Task 8: Convolutional Training\n","\n","\n","Create a convolutional network with $Q_1=16$ and $Q_2=32$ convolutional channels and $O=10$ output neurons.\n","Train the network for 10 epochs with $\\eta=0.01$, momentum $=0.9$ and store the obtained test losses and accuracies.\n","Again, 100 epochs will take up to 30 minutes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4hQSxjIEaQL3"},"outputs":[],"source":["cv = convolutional(...)\n","cv_loss, cv_acc = train(...)"]},{"cell_type":"markdown","metadata":{"id":"R2lhQiuyaQL3"},"source":["### Task 9: Plotting\n","\n","Plot the two lists of loss values in one plot. Plot the two lists of accuracy values into another plot."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8SMsfktXaQL3"},"outputs":[],"source":["from matplotlib import pyplot\n","pyplot.figure(figsize=(10,3))\n","ax = pyplot.subplot(121)\n","# plot loss values of FC and CV network over epochs\n","...\n","\n","ax = pyplot.subplot(122)\n","# plot accuracy values of FC and CV network over epochs\n","..."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vua5OAWnaQL3"},"source":["### Task 10: Learnable Parameters \n","\n","Estimate roughly how many learnable parameters the two networks have by analytically computing and adding the number of parameters in each layer.\n","\n","Fully-connected Network:\n","- first fully-connected layer: ...\n","- second fully-connected layer: ...\n","- third fully-connected layer: ...\n","- total: ...\n","\n","Convolutional Network:\n","- first convolutional layer: ...\n","- second convolutional layer: ...\n","- fully-connected layer: ...\n","- total: ...\n","\n","\n","\n","Now, compute the number of parameters in the networks by summing the number of parameters in each layer using `pytorch` functionality.\n","You can use the `numel()` function from a `torch.Tensor` to provide the number of (learnable) parameters stored in a tensor.\n","How do you obtain this list of all learnable parameters from a `pytorch` [Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aC_ZOtKUaQL4"},"outputs":[],"source":["def parameter_count(network):\n","  return ...\n","\n","print(\"Fully-connected Network:\", parameter_count(fc))\n","print(\"Convolutional Network:\", parameter_count(cv))"]}],"metadata":{"colab":{"provenance":[]},"interpreter":{"hash":"2dd53f8ad749bca69f7250ce75eb4f0def59db5cf79075a9716322ffc58e8a2e"},"kernelspec":{"display_name":"Python 3.8.10 ('DL')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
