{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jDR8f9l2LrNu"
      },
      "source": [
        "# Assignment 9: Convolutional Auto-Encoder\n",
        "\n",
        "In this assignment, we show that it is possible to learn from unlabeled data using a convolutional auto-encoder network. \n",
        "The task is to reduce (a noisy version of) an image of the handwritten digits of MNIST into a deep feature representation, without making use of their labels, and reconstruct the sample from that representation.\n",
        "\n",
        "For this purpose, we implement a convolutional auto-encoder and a denoising auto-encoder that learn a $K=10$-dimensional deep feature representation of each image, and uses this representation to reconstruct images to the original size of $28\\times28$ pixels.\n",
        "We show that such a network can be used to denoise images."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EYyIQ7dufhHT"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "We will make use of the default implementation of the `torchvision.datasets.MNIST` dataset. \n",
        "MNIST has 10 labels of digit images.\n",
        "However, besides the last task, we do not make use of the labels of the dataset, but we only utilize the images.\n",
        "We instantiate the training and test sets of MNIST -- again we will use the test set for validation purposes."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3_VRuI9Bfd1s"
      },
      "source": [
        "### Task 1: Datasets\n",
        "\n",
        "Instantiate the training and validation datasets of MNIST, i.e., split the training and validation sets, and make use of data loaders.\n",
        "Select a simple `ToTensor` transform.\n",
        "Instantiate a training data loader using a batch size of $B=32$, and a validation data loader with 100 samples in a batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "EW5O0-dvLrNw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "B = 32 # batch size\n",
        "\n",
        "# training set and data loader\n",
        "train_set = trainset = torchvision.datasets.MNIST(\n",
        "  root = \"./data\",\n",
        "  train=True, download=True, transform=torchvision.transforms.ToTensor()\n",
        ")\n",
        "train_loader = torch.utils.data.DataLoader(dataset = trainset, batch_size=B)\n",
        "\n",
        "# validation set and data loader\n",
        "validation_set = testset = torchvision.datasets.MNIST(\n",
        "  root = \"./data\",\n",
        "  train=False, download=True, transform=torchvision.transforms.ToTensor()\n",
        ")\n",
        "validation_loader = torch.utils.data.DataLoader(dataset = validation_set, batch_size=B)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oJzhaUraSpPG"
      },
      "source": [
        "### Task 2: Random Noise\n",
        "\n",
        "Implement a function to generate noisy images by adding a uniform noise to the clean images:\n",
        "\n",
        "$$\\mathbf X_{\\mathrm{noise}} = \\mathbf X + \\alpha \\mathbf U^{D\\times E}$$ \n",
        "\n",
        "where $\\mathbf U$ is a tensor with entries generated from a uniform distribution between $-1$ and $1$. \n",
        "$D=E=28$ are the width and the height of the original image. \n",
        "To represent actual images, we will restrict the pixel values of $\\mathbf X_{\\mathrm{noise}}$ to be in the range $[0,1]$ by clipping any value that is outside that range.\n",
        "\n",
        "Note that this function will also be used with batches of samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "N_DefcwdSpPH"
      },
      "outputs": [],
      "source": [
        "def noise(x, alpha=0.5):\n",
        "  # generate noise \n",
        "  N = np.random.uniform(-1, 1, x.shape)\n",
        "  # Add noise and clamp\n",
        "  noisy_sample = torch.clamp(x + alpha * N, 0, 1)\n",
        "\n",
        "  return noisy_sample"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_6QRshMXgHoc"
      },
      "source": [
        "### Noisy Image Visualization\n",
        "\n",
        "Here we just assure that the images are correct, by displaying them to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "KW3QT2jVgLFM"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# get first validation image\n",
        "input = validation_set[0][0]\n",
        "\n",
        "# generate noise image\n",
        "noisy = noise(input)\n",
        "\n",
        "# plot images\n",
        "from matplotlib import pyplot\n",
        "pyplot.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "fig, ax = pyplot.subplots(1, 2)\n",
        "ax[0].imshow(input.squeeze())\n",
        "ax[0].axis(\"off\")\n",
        "ax[1].imshow(noisy.squeeze())\n",
        "ax[1].axis(\"off\")\n",
        "pyplot.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "392dfD3Xht-h"
      },
      "source": [
        "## Auto-Encoder Network\n",
        "\n",
        "The auto-encoder network is composed of two parts: the encoder that transforms the input image to a deep feature representation; and the decoder that produces an image from such a deep feature.\n",
        "\n",
        "For the encoder $\\mathcal E$, we will basically use the same convolutional network topology as in the last assignment.\n",
        "An exception is that we perform our down-sampling via striding and not via pooling.\n",
        "After each convolution, we apply the ReLU activation.\n",
        "The output of the encoder is a $K=10$ dimensional deep feature representation.\n",
        "The complete encoder network topology can be found below in Topology 1(a).\n",
        "\n",
        "The decoder $\\mathcal D$ performs the inverse operations of the encoder.\n",
        "A fully-connected layer is used to increase the number of samples to the same size as the output of the flattening of the encoder.\n",
        "Then, a ReLU activation is applied. \n",
        "The flattening needs to be undone next by reshaping the vector into the correct dimensionality.\n",
        "A fractionally-strided convolutional layer increases the intermediate representation by a factor of 2.\n",
        "Note that the fractionally-strided convolution is implemented in `torch.nn.ConvTranspose2d`, and the `stride` parameter should have the same value as for the encoder.\n",
        "Additionally, the `torch.nn.ConvTranspose2d` has a parameter `output_padding` which needs to be adapted to reach the correct output shape (see Test 1).\n",
        "After this layer, we perform another ReLU activation and another fractionally-strided convolution to arrive at the original input dimension.\n",
        "The complete decoder network topology can be found below in Topology 1(b).\n",
        "\n",
        "Finally, we combine the two sub-networks into one auto-encoder network.\n",
        "While there exist several possibilities for doing this, we will implement a third `torch.nn.Module` that contains an instance of the encoder and an instance of the decoder."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iDSQU7nvluAM"
      },
      "source": [
        "Topology 1: Network configurations of the (a) encoder and (b) decoder networks\n",
        "\n",
        "(a) Encoder Network\n",
        "\n",
        "*   2D convolutional layer with $Q_1$ channels, kernel size $5\\times5$, **stride 2** and padding 2\n",
        "*   activation function ReLU\n",
        "*   2D convolutional layer with $Q_2$ channels, kernel size $5\\times5$, **stride 2** and padding 2\n",
        "*   flatten layer to convert the convolution output into a vector\n",
        "*   activation function ReLU\n",
        "*   fully-connected layer with the correct number of inputs and $K$ outputs\n",
        "\n",
        "(b) Encoder Network\n",
        "\n",
        "*   fully-connected layer with $K$ inputs and the correct number of outputs\n",
        "*   activation function ReLU\n",
        "*   reshaping to convert the vector into a convolution input\n",
        "*   2D **fractionally-strided convolutional** layer with $Q_2$ channels, kernel size $5\\times5$, stride 2 and padding 2\n",
        "*   activation function ReLU\n",
        "*   2D **fractionally-strided convolutional** layer with $Q_1$ channels, kernel size $5\\times5$, stride 2 and padding 2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sulFbFbPLrNx"
      },
      "source": [
        "### Task 3: Encoder Network\n",
        "\n",
        "Implement the encoder network for given parameters $Q_1$, $Q_2$, and $K$ as given in Topology 1(a).\n",
        "Implement a network class that derives from `torch.nn.Module` and implement the `__init__` and the `forward` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "IASbngJNLrNy"
      },
      "outputs": [],
      "source": [
        "class Encoder (torch.nn.Module):\n",
        "  def __init__(self, Q1, Q2, K):\n",
        "    # call base class constrcutor\n",
        "    super(Encoder,self).__init__()\n",
        "      # define convolutional layers\n",
        "    self.conv1 = torch.nn.Conv2d(1, Q1, kernel_size= [5,5], stride=2, padding=2)\n",
        "    self.conv2 = torch.nn.Conv2d(Q1, Q2, kernel_size= [5,5], stride=2, padding=2)\n",
        "    # pooling and activation functions will be re-used for the different stages\n",
        "    self.act = torch.nn.ReLU()\n",
        "    # define fully-connected layers\n",
        "    self.flatten = torch.nn.Flatten()\n",
        "    # ?? how do I know the size of the input to the first fully-connected layer?\n",
        "    \n",
        "    self.fc = torch.nn.Linear(Q2 * 7 * 7, K)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # get the deep feature representation\n",
        "        # compute first layer of convolution, pooling and activation\n",
        "    a = self.act(self.conv1(x))\n",
        "    # compute second layer of convolution, pooling and activation\n",
        "    a = self.act(self.conv2(a))\n",
        "    # get the deep features as the output of the first fully-connected layer\n",
        "    # ?? why do I flatten before input to the first fully-connected layer?\n",
        "    deep_feature = self.act(self.fc(self.flatten(a)))\n",
        "    return deep_feature"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6OH24In8LrNy"
      },
      "source": [
        "### Task 4: Decoder Network\n",
        "\n",
        "Implement the decoder network for given parameters $Q_1$, $Q_2$, and $K$ as given in Topology 1(b).\n",
        "Implement a network class that derives from `torch.nn.Module` and implement the `__init__` and the `forward` methods.\n",
        "The output of the decoder network is supposed to have values in the range $[0,1]$, similar to the input values.\n",
        "We need to make sure that only these values can be achieved.\n",
        "Think of possible ways of doing that, and apply the way that seems most reasonable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "YTiDRV5tLrNz"
      },
      "outputs": [],
      "source": [
        "class Decoder (torch.nn.Module):\n",
        "  def __init__(self, Q1, Q2, K):\n",
        "    # call base class constrcutor\n",
        "    super(Decoder,self).__init__()\n",
        "    # fully-connected layer\n",
        "    self.fc = torch.nn.Linear(K, Q2 * 7 * 7)\n",
        "    # convolutional layers\n",
        "    self.deconv1 = torch.nn.ConvTranspose2d(1, Q1, kernel_size= [5,5], stride=2, padding=2, output_padding=1)\n",
        "    self.deconv2 = torch.nn.ConvTranspose2d(Q1, 1, kernel_size= [5,5], stride=2, padding=2, output_padding=1)\n",
        "    # activation function\n",
        "    self.act = torch.nn.ReLU()\n",
        "    # unflatten\n",
        "    self.unflatten = torch.nn.Unflatten(1, (Q2, 7, 7))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # reconstruct the output image\n",
        "    a = self.unflatten(self.act(self.fc(x)))\n",
        "    a = self.act(self.deconv1(a))\n",
        "    output = self.deconv2(a)\n",
        "    return output"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph6VHeFJLrNz"
      },
      "source": [
        "### Task 5: Joint Auto-Encoder Network\n",
        "\n",
        "Implement the auto-encoder network by combining the encoder and the decoder.\n",
        "In the `__init__` function, instantiate an encoder from Task 3 and a decoder from Task 4.\n",
        "In `forward`, pass the input through the encoder and the decoder: $\\mathbf Y = \\mathcal D(\\mathcal E(\\mathbf X))$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "DYdl42m3LrN0"
      },
      "outputs": [],
      "source": [
        "class AutoEncoder(torch.nn.Module):\n",
        "  def __init__(self, Q1, Q2, K):\n",
        "    super(AutoEncoder,self).__init__()\n",
        "    self.encoder = Encoder(Q1, Q2, K)\n",
        "    self.decoder = Decoder(Q1, Q2, K)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # encode input\n",
        "    deep_feature = self.encoder(x)\n",
        "    # decode to output\n",
        "    reconstructed = self.decoder(deep_feature)\n",
        "    return reconstructed"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RmZrCO0hLrN0"
      },
      "source": [
        "### Test 1: Output Sizes\n",
        "\n",
        "The code below instantiates the auto-encoder network with $Q_1 = Q_2 = 32$ and $K=10$. \n",
        "Then the given input $\\mathbf X$ is provided to the (untrained) auto-encoder network.\n",
        "Use this code to verify that the deep feature extracted by the encoder and the output from the decoder part both have the desired size. \n",
        "Also, we verify that the output values are between 0 and 1.\n",
        "\n",
        "If the tests cannot be passed, please check the implementations in Task 3, 4, and 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "yLttmKotLrN1"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Given transposed=1, weight of size [1, 32, 5, 5], expected input[1, 32, 7, 7] to have 1 channels, but got 32 channels instead",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[77], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39massert\u001b[39;00m deep_features\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[39m# use decoder to generate an image and check its size and value range\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m output \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39;49mdecoder(deep_features)\n\u001b[1;32m     15\u001b[0m \u001b[39massert\u001b[39;00m output\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m:] \u001b[39m==\u001b[39m (\u001b[39m28\u001b[39m,\u001b[39m28\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[39massert\u001b[39;00m torch\u001b[39m.\u001b[39mall(output \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mall(output \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[75], line 18\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     16\u001b[0m   \u001b[39m# reconstruct the output image\u001b[39;00m\n\u001b[1;32m     17\u001b[0m   a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munflatten(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(x)))\n\u001b[0;32m---> 18\u001b[0m   a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeconv1(a))\n\u001b[1;32m     19\u001b[0m   output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeconv2(a)\n\u001b[1;32m     20\u001b[0m   \u001b[39mreturn\u001b[39;00m output\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:956\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    951\u001b[0m num_spatial_dims \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    952\u001b[0m output_padding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_padding(\n\u001b[1;32m    953\u001b[0m     \u001b[39minput\u001b[39m, output_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    954\u001b[0m     num_spatial_dims, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 956\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv_transpose2d(\n\u001b[1;32m    957\u001b[0m     \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[1;32m    958\u001b[0m     output_padding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [1, 32, 5, 5], expected input[1, 32, 7, 7] to have 1 channels, but got 32 channels instead"
          ]
        }
      ],
      "source": [
        "# run on cuda device?\n",
        "device = torch.device(\"cuda\")\n",
        "# create network\n",
        "network = AutoEncoder(32, 32, 10).to(device)\n",
        "\n",
        "# create or select a sample\n",
        "x = torch.randn((1,1,28,28))\n",
        "\n",
        "# use encoder to encode image and check its size\n",
        "deep_features = network.encoder(x.to(device))\n",
        "assert deep_features.shape[1] == 10\n",
        "\n",
        "# use decoder to generate an image and check its size and value range\n",
        "output = network.decoder(deep_features)\n",
        "assert output.shape[2:] == (28,28)\n",
        "assert torch.all(output >= 0) and torch.all(output <= 1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ped3OmgBi5lD"
      },
      "source": [
        "## Training and Evaluation\n",
        "We will implement two training procedures, a simple auto-encoder network and a denoising auto-encoder network, which should be combined into one function.\n",
        "To train the network, we will use the $L_2$ distance between the output and the input of the network as a loss function, which is implemented in `torch.nn.MSELoss`:\n",
        "\n",
        "  $$\\mathcal J^{L_2} (\\mathbf X, \\mathbf Y) = \\|\\mathbf X - \\mathbf Y\\|^2$$\n",
        "\n",
        "For optimization, we will make use of the `Adam` optimizer with a learning rate of $\\eta=0.001$.\n",
        "We will run the training for 10 epochs and compute training and validation set loss after each epoch.\n",
        "\n",
        "Denoising training requires generating noisy images before forwarding them into the network and taking the loss between output and the clean image.\n",
        "\n",
        "For evaluation, we will check whether some of the validation set samples are correctly reconstructed from the auto-encoder network and whether the noise is removed from the denoising network.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dhJJQpLVSpPH"
      },
      "source": [
        "### Task 6: Training Loop\n",
        "\n",
        "For a given network and learning rate, implement a function that initiates loss function (`torch.nn.MSELoss`), optimizer, and trains the network for 10 epochs on the training data.\n",
        "If parameter `denoise` is `True`, generate noisy batch with factor `alpha`.\n",
        "Compute the running average of the training loss for each epoch.\n",
        "At the end of each epoch, also compute the validation set loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx-zo0jXLrN1"
      },
      "outputs": [],
      "source": [
        "def training_loop(network, lr=0.001, denoise=False, alpha=0.5):\n",
        "  # define optimizer\n",
        "  optimizer = torch.optim.Adam(network.parameters(), lr=lr)\n",
        "  # define loss function\n",
        "  loss = torch.nn.MSELoss()\n",
        "\n",
        "  for epoch in range(10):\n",
        "    # evaluate average loss for training and validation set\n",
        "    train_loss = validation_loss = 0.\n",
        "\n",
        "    for x,_ in train_loader:\n",
        "\n",
        "      # generate noise images by factor alpha as input if denoise is True\n",
        "      \n",
        "      # compute network output\n",
        "      y = network(x)\n",
        "\n",
        "      # compute loss between output and input\n",
        "      J = loss(y, x)\n",
        "      # perform update\n",
        "      optimizer.zero_grad()\n",
        "      # accumulate loss\n",
        "      train_loss += J.item()\n",
        "\n",
        "\n",
        "    # compute validation loss\n",
        "    with torch.no_grad():\n",
        "      for x,t in validation_loader:\n",
        "        # generate noise images by factor alpha as input if denoise is True\n",
        "\n",
        "        # compute network output\n",
        "        y = network(x)\n",
        "        # compute loss\n",
        "        J = loss(y, x)\n",
        "        # accumulate loss\n",
        "        validation_loss += J.item()\n",
        "\n",
        "\n",
        "    # print average loss for training and validation\n",
        "    print(f\"\\rEpoch {epoch+1}; train: {train_loss/len(train_set):1.5f}, val: {validation_loss/len(validation_set):1.5f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aOTDzpzugjGs"
      },
      "source": [
        "### Task 7: Training of two Networks\n",
        "\n",
        "Instantiate two `AutoEncoder` networks, one to train with only clean samples, and one to train with noisy images. Call the training loop for each network.\n",
        "\n",
        "Note: If the training loss and validation loss do not decrease during training, try to reduce the learning rate (to $\\eta=0.0005$ or even lower) and restart the training.\n",
        "You will need to re-initialize the network, too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiN0hu4HgtgZ"
      },
      "outputs": [],
      "source": [
        "# define network\n",
        "network1 = AutoEncoder(32, 32, 10).to(device)\n",
        "\n",
        "# perform auto-encoder training \n",
        "training_loop(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYFsBnXR-iBi"
      },
      "outputs": [],
      "source": [
        "# define network\n",
        "network2 = ...\n",
        "\n",
        "# perform denoising auto-encoder training \n",
        "training_loop(...)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H2wfOMLWLrN2"
      },
      "source": [
        "### Task 8: Reconstruction Result\n",
        "\n",
        "This task is to visualize the reconstructed images from their originals.\n",
        "For this purpose, select one image for each label from the first batch of the validation set. \n",
        "Generate the noisy input by Task 2.\n",
        "\n",
        "Forward the clean images through the trained auto-encoder network to extract their reconstructions. \n",
        "Forward the noisy inputs through the trained denoising network to remove noise.\n",
        "\n",
        "To show the difference, plot the original sample, reconstructed sample, noisy sample, and denoised sample for one label in one column. \n",
        "Make a single plot with 4 rows and 10 columns. \n",
        "See the reference plot in the slides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiliTlgkyDUu"
      },
      "outputs": [],
      "source": [
        "# get first validation set batch\n",
        "x,t = next(iter(validation_loader))\n",
        "\n",
        "# select one image for each label\n",
        "...\n",
        "\n",
        "# If required, convert the list of select images into tensor through torch.stack\n",
        "original = ...\n",
        "\n",
        "# compute reconstructed samples\n",
        "reconstructed = ...\n",
        "# generate noisy samples\n",
        "noisy = ...\n",
        "# compute denoised samples\n",
        "denoised = ...\n",
        "\n",
        "samples = [original, reconstructed, noisy, denoised]\n",
        "\n",
        "# plot images\n",
        "from matplotlib import pyplot\n",
        "pyplot.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "fig, axes = pyplot.subplots(nrows=..., ncols=..., figsize=(15, 6))\n",
        "\n",
        "for i in range(...):\n",
        "  for j in range(...):\n",
        "    axes[i][j].imshow(samples[i][j].squeeze())\n",
        "    axes[i][j].axis(\"off\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "2dd53f8ad749bca69f7250ce75eb4f0def59db5cf79075a9716322ffc58e8a2e"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('DL')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
