{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jDR8f9l2LrNu"
      },
      "source": [
        "# Assignment 9: Convolutional Auto-Encoder\n",
        "\n",
        "In this assignment, we show that it is possible to learn from unlabeled data using a convolutional auto-encoder network. \n",
        "The task is to reduce (a noisy version of) an image of the handwritten digits of MNIST into a deep feature representation, without making use of their labels, and reconstruct the sample from that representation.\n",
        "\n",
        "For this purpose, we implement a convolutional auto-encoder and a denoising auto-encoder that learn a $K=10$-dimensional deep feature representation of each image, and uses this representation to reconstruct images to the original size of $28\\times28$ pixels.\n",
        "We show that such a network can be used to denoise images."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EYyIQ7dufhHT"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "We will make use of the default implementation of the `torchvision.datasets.MNIST` dataset. \n",
        "MNIST has 10 labels of digit images.\n",
        "However, besides the last task, we do not make use of the labels of the dataset, but we only utilize the images.\n",
        "We instantiate the training and test sets of MNIST -- again we will use the test set for validation purposes."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3_VRuI9Bfd1s"
      },
      "source": [
        "### Task 1: Datasets\n",
        "\n",
        "Instantiate the training and validation datasets of MNIST, i.e., split the training and validation sets, and make use of data loaders.\n",
        "Select a simple `ToTensor` transform.\n",
        "Instantiate a training data loader using a batch size of $B=32$, and a validation data loader with 100 samples in a batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EW5O0-dvLrNw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "B = 32 # batch size\n",
        "\n",
        "# training set and data loader\n",
        "train_set = trainset = torchvision.datasets.MNIST(\n",
        "  root = \"./data\",\n",
        "  train=True, download=True, transform=torchvision.transforms.ToTensor()\n",
        ")\n",
        "train_loader = torch.utils.data.DataLoader(dataset = trainset, batch_size=B)\n",
        "\n",
        "# validation set and data loader\n",
        "validation_set = testset = torchvision.datasets.MNIST(\n",
        "  root = \"./data\",\n",
        "  train=False, download=True, transform=torchvision.transforms.ToTensor()\n",
        ")\n",
        "validation_loader = torch.utils.data.DataLoader(dataset = validation_set, batch_size=B)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oJzhaUraSpPG"
      },
      "source": [
        "### Task 2: Random Noise\n",
        "\n",
        "Implement a function to generate noisy images by adding a uniform noise to the clean images:\n",
        "\n",
        "$$\\mathbf X_{\\mathrm{noise}} = \\mathbf X + \\alpha \\mathbf U^{D\\times E}$$ \n",
        "\n",
        "where $\\mathbf U$ is a tensor with entries generated from a uniform distribution between $-1$ and $1$. \n",
        "$D=E=28$ are the width and the height of the original image. \n",
        "To represent actual images, we will restrict the pixel values of $\\mathbf X_{\\mathrm{noise}}$ to be in the range $[0,1]$ by clipping any value that is outside that range.\n",
        "\n",
        "Note that this function will also be used with batches of samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N_DefcwdSpPH"
      },
      "outputs": [],
      "source": [
        "def noise(x, alpha=0.5):\n",
        "  # generate noise \n",
        "  N = np.random.uniform(-1, 1, x.shape)\n",
        "  # Add noise and clamp\n",
        "  noisy_sample = torch.clamp(x + alpha * N, 0, 1)\n",
        "\n",
        "  return noisy_sample"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_6QRshMXgHoc"
      },
      "source": [
        "### Noisy Image Visualization\n",
        "\n",
        "Here we just assure that the images are correct, by displaying them to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# get first validation image\n",
        "input = validation_set[0][0]\n",
        "\n",
        "# generate noise image\n",
        "noisy = noise(input)\n",
        "\n",
        "# plot images\n",
        "from matplotlib import pyplot\n",
        "pyplot.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "fig, ax = pyplot.subplots(1, 2)\n",
        "ax[0].imshow(input.squeeze())\n",
        "ax[0].axis(\"off\")\n",
        "ax[1].imshow(noisy.squeeze())\n",
        "ax[1].axis(\"off\")\n",
        "pyplot.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "392dfD3Xht-h"
      },
      "source": [
        "## Auto-Encoder Network\n",
        "\n",
        "The auto-encoder network is composed of two parts: the encoder that transforms the input image to a deep feature representation; and the decoder that produces an image from such a deep feature.\n",
        "\n",
        "For the encoder $\\mathcal E$, we will basically use the same convolutional network topology as in the last assignment.\n",
        "An exception is that we perform our down-sampling via striding and not via pooling.\n",
        "After each convolution, we apply the ReLU activation.\n",
        "The output of the encoder is a $K=10$ dimensional deep feature representation.\n",
        "The complete encoder network topology can be found below in Topology 1(a).\n",
        "\n",
        "The decoder $\\mathcal D$ performs the inverse operations of the encoder.\n",
        "A fully-connected layer is used to increase the number of samples to the same size as the output of the flattening of the encoder.\n",
        "Then, a ReLU activation is applied. \n",
        "The flattening needs to be undone next by reshaping the vector into the correct dimensionality.\n",
        "A fractionally-strided convolutional layer increases the intermediate representation by a factor of 2.\n",
        "Note that the fractionally-strided convolution is implemented in `torch.nn.ConvTranspose2d`, and the `stride` parameter should have the same value as for the encoder.\n",
        "Additionally, the `torch.nn.ConvTranspose2d` has a parameter `output_padding` which needs to be adapted to reach the correct output shape (see Test 1).\n",
        "After this layer, we perform another ReLU activation and another fractionally-strided convolution to arrive at the original input dimension.\n",
        "The complete decoder network topology can be found below in Topology 1(b).\n",
        "\n",
        "Finally, we combine the two sub-networks into one auto-encoder network.\n",
        "While there exist several possibilities for doing this, we will implement a third `torch.nn.Module` that contains an instance of the encoder and an instance of the decoder."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iDSQU7nvluAM"
      },
      "source": [
        "Topology 1: Network configurations of the (a) encoder and (b) decoder networks\n",
        "\n",
        "(a) Encoder Network\n",
        "\n",
        "*   2D convolutional layer with $Q_1$ channels, kernel size $5\\times5$, **stride 2** and padding 2\n",
        "*   activation function ReLU\n",
        "*   2D convolutional layer with $Q_2$ channels, kernel size $5\\times5$, **stride 2** and padding 2\n",
        "*   flatten layer to convert the convolution output into a vector\n",
        "*   activation function ReLU\n",
        "*   fully-connected layer with the correct number of inputs and $K$ outputs\n",
        "\n",
        "(b) Encoder Network\n",
        "\n",
        "*   fully-connected layer with $K$ inputs and the correct number of outputs\n",
        "*   activation function ReLU\n",
        "*   reshaping to convert the vector into a convolution input\n",
        "*   2D **fractionally-strided convolutional** layer with $Q_2$ channels, kernel size $5\\times5$, stride 2 and padding 2\n",
        "*   activation function ReLU\n",
        "*   2D **fractionally-strided convolutional** layer with $Q_1$ channels, kernel size $5\\times5$, stride 2 and padding 2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sulFbFbPLrNx"
      },
      "source": [
        "### Task 3: Encoder Network\n",
        "\n",
        "Implement the encoder network for given parameters $Q_1$, $Q_2$, and $K$ as given in Topology 1(a).\n",
        "Implement a network class that derives from `torch.nn.Module` and implement the `__init__` and the `forward` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IASbngJNLrNy"
      },
      "outputs": [],
      "source": [
        "class Encoder (torch.nn.Module):\n",
        "  def __init__(self, Q1, Q2, K):\n",
        "    # call base class constrcutor\n",
        "    super(Encoder,self).__init__()\n",
        "      # define convolutional layers\n",
        "    self.conv1 = torch.nn.Conv2d(1, Q1, kernel_size= [5,5], stride=2, padding=2)\n",
        "    self.conv2 = torch.nn.Conv2d(Q1, Q2, kernel_size= [5,5], stride=2, padding=2)\n",
        "    # pooling and activation functions will be re-used for the different stages\n",
        "    self.act = torch.nn.ReLU()\n",
        "    # define fully-connected layers\n",
        "    self.flatten = torch.nn.Flatten()\n",
        "    # ?? how do I know the size of the input to the first fully-connected layer?\n",
        "    \n",
        "    self.fc = torch.nn.Linear(Q2 * 7 * 7, K)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # get the deep feature representation\n",
        "        # compute first layer of convolution, pooling and activation\n",
        "    a = self.act(self.conv1(x))\n",
        "    # compute second layer of convolution, pooling and activation\n",
        "    a = self.act(self.conv2(a))\n",
        "    # get the deep features as the output of the first fully-connected layer\n",
        "    # ?? why do I flatten before input to the first fully-connected layer?\n",
        "    deep_feature = self.act(self.fc(self.flatten(a)))\n",
        "    return deep_feature"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6OH24In8LrNy"
      },
      "source": [
        "### Task 4: Decoder Network\n",
        "\n",
        "Implement the decoder network for given parameters $Q_1$, $Q_2$, and $K$ as given in Topology 1(b).\n",
        "Implement a network class that derives from `torch.nn.Module` and implement the `__init__` and the `forward` methods.\n",
        "The output of the decoder network is supposed to have values in the range $[0,1]$, similar to the input values.\n",
        "We need to make sure that only these values can be achieved.\n",
        "Think of possible ways of doing that, and apply the way that seems most reasonable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YTiDRV5tLrNz"
      },
      "outputs": [],
      "source": [
        "class Decoder (torch.nn.Module):\n",
        "  def __init__(self, Q1, Q2, K):\n",
        "    # call base class constrcutor\n",
        "    super(Decoder,self).__init__()\n",
        "    # fully-connected layer\n",
        "    self.fc = torch.nn.Linear(K, Q2 * 7 * 7)\n",
        "    # convolutional layers\n",
        "    self.deconv1 = torch.nn.ConvTranspose2d(Q2, Q1, kernel_size= [5,5], stride=2, padding=2, output_padding=1)\n",
        "    self.deconv2 = torch.nn.ConvTranspose2d(Q1, 1, kernel_size= [5,5], stride=2, padding=2, output_padding=1)\n",
        "    # activation function\n",
        "    self.act = torch.nn.ReLU()\n",
        "    # unflatten\n",
        "    self.unflatten = torch.nn.Unflatten(dim=1, unflattened_size=(Q2,7,7))\n",
        "      \n",
        "  def forward(self, x):\n",
        "    # reconstruct the output image\n",
        "    a = self.unflatten(self.act(self.fc(x)))\n",
        "    a = self.act(self.deconv1(a))\n",
        "    a = self.deconv2(a)\n",
        "    output = torch.sigmoid(a)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "def output_dim(features_in, kernel_size, padding, stride, dilation = 1):\n",
        "    \"\"\"floor just returns rounded down to nearest int\n",
        "\n",
        "    :param features_in: _description_\n",
        "    :type features_in: _type_\n",
        "    :param kernel_size: _description_\n",
        "    :type kernel_size: _type_\n",
        "    :param padding: _description_\n",
        "    :type padding: _type_\n",
        "    :param stride: _description_\n",
        "    :type stride: _type_\n",
        "    \"\"\"\n",
        "    return math.floor((features_in + 2 * padding - dilation * (kernel_size - 1) -1) / stride ) +1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph6VHeFJLrNz"
      },
      "source": [
        "### Task 5: Joint Auto-Encoder Network\n",
        "\n",
        "Implement the auto-encoder network by combining the encoder and the decoder.\n",
        "In the `__init__` function, instantiate an encoder from Task 3 and a decoder from Task 4.\n",
        "In `forward`, pass the input through the encoder and the decoder: $\\mathbf Y = \\mathcal D(\\mathcal E(\\mathbf X))$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DYdl42m3LrN0"
      },
      "outputs": [],
      "source": [
        "class AutoEncoder(torch.nn.Module):\n",
        "  def __init__(self, Q1, Q2, K):\n",
        "    super(AutoEncoder,self).__init__()\n",
        "    self.encoder = Encoder(Q1, Q2, K)\n",
        "    self.decoder = Decoder(Q1, Q2, K)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # encode input\n",
        "    deep_feature = self.encoder(x)\n",
        "    # decode to output\n",
        "    reconstructed = self.decoder(deep_feature)\n",
        "    return reconstructed"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RmZrCO0hLrN0"
      },
      "source": [
        "### Test 1: Output Sizes\n",
        "\n",
        "The code below instantiates the auto-encoder network with $Q_1 = Q_2 = 32$ and $K=10$. \n",
        "Then the given input $\\mathbf X$ is provided to the (untrained) auto-encoder network.\n",
        "Use this code to verify that the deep feature extracted by the encoder and the output from the decoder part both have the desired size. \n",
        "Also, we verify that the output values are between 0 and 1.\n",
        "\n",
        "If the tests cannot be passed, please check the implementations in Task 3, 4, and 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yLttmKotLrN1"
      },
      "outputs": [],
      "source": [
        "# run on cuda device?\n",
        "device = torch.device(\"cuda\")\n",
        "# create network\n",
        "network = AutoEncoder(32, 32, 10).to(device)\n",
        "\n",
        "# create or select a sample\n",
        "x = torch.randn((1,1,28,28))\n",
        "\n",
        "# use encoder to encode image and check its size\n",
        "deep_features = network.encoder(x.to(device))\n",
        "assert deep_features.shape[1] == 10\n",
        "\n",
        "# use decoder to generate an image and check its size and value range\n",
        "output = network.decoder(deep_features)\n",
        "assert output.shape[2:] == (28,28)\n",
        "assert torch.all(output >= 0) and torch.all(output <= 1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ped3OmgBi5lD"
      },
      "source": [
        "## Training and Evaluation\n",
        "We will implement two training procedures, a simple auto-encoder network and a denoising auto-encoder network, which should be combined into one function.\n",
        "To train the network, we will use the $L_2$ distance between the output and the input of the network as a loss function, which is implemented in `torch.nn.MSELoss`:\n",
        "\n",
        "  $$\\mathcal J^{L_2} (\\mathbf X, \\mathbf Y) = \\|\\mathbf X - \\mathbf Y\\|^2$$\n",
        "\n",
        "For optimization, we will make use of the `Adam` optimizer with a learning rate of $\\eta=0.001$.\n",
        "We will run the training for 10 epochs and compute training and validation set loss after each epoch.\n",
        "\n",
        "Denoising training requires generating noisy images before forwarding them into the network and taking the loss between output and the clean image.\n",
        "\n",
        "For evaluation, we will check whether some of the validation set samples are correctly reconstructed from the auto-encoder network and whether the noise is removed from the denoising network.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dhJJQpLVSpPH"
      },
      "source": [
        "### Task 6: Training Loop\n",
        "\n",
        "For a given network and learning rate, implement a function that initiates loss function (`torch.nn.MSELoss`), optimizer, and trains the network for 10 epochs on the training data.\n",
        "If parameter `denoise` is `True`, generate noisy batch with factor `alpha`.\n",
        "Compute the running average of the training loss for each epoch.\n",
        "At the end of each epoch, also compute the validation set loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "kx-zo0jXLrN1"
      },
      "outputs": [],
      "source": [
        "def training_loop(network, lr=0.001, denoise=False, alpha=0.5):\n",
        "  # define optimizer\n",
        "  optimizer = torch.optim.Adam(network.parameters(), lr=lr)\n",
        "  # define loss function\n",
        "  loss = torch.nn.MSELoss()\n",
        "\n",
        "  for epoch in range(10):\n",
        "    # evaluate average loss for training and validation set\n",
        "    train_loss = validation_loss = 0.\n",
        "\n",
        "    for x,_ in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # generate noise images by factor alpha as input if denoise is True\n",
        "      if denoise:\n",
        "        x = torch.tensor(noise(x, alpha), dtype=torch.float32)\n",
        "      \n",
        "      # compute network output\n",
        "      y = network(x.to(device))\n",
        "\n",
        "      # compute loss between output and input\n",
        "      J = loss(y, x.to(device))\n",
        "      # perform update\n",
        "      J.backward()\n",
        "      optimizer.step()\n",
        "      # accumulate loss\n",
        "      train_loss += J.item()\n",
        "\n",
        "\n",
        "    # compute validation loss\n",
        "    with torch.no_grad():\n",
        "      for x,t in validation_loader:\n",
        "        # generate noise images by factor alpha as input if denoise is True\n",
        "\n",
        "        # compute network output\n",
        "        y = network(x.to(device))\n",
        "        # compute loss\n",
        "        J = loss(y, x.to(device))\n",
        "        # accumulate loss\n",
        "        validation_loss += J.item()\n",
        "\n",
        "\n",
        "    # print average loss for training and validation\n",
        "    print(f\"\\rEpoch {epoch+1}; train: {train_loss/len(train_set):1.5f}, val: {validation_loss/len(validation_set):1.5f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aOTDzpzugjGs"
      },
      "source": [
        "### Task 7: Training of two Networks\n",
        "\n",
        "Instantiate two `AutoEncoder` networks, one to train with only clean samples, and one to train with noisy images. Call the training loop for each network.\n",
        "\n",
        "Note: If the training loss and validation loss do not decrease during training, try to reduce the learning rate (to $\\eta=0.0005$ or even lower) and restart the training.\n",
        "You will need to re-initialize the network, too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HiN0hu4HgtgZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1; train: 0.00144, val: 0.00112\n",
            "Epoch 2; train: 0.00092, val: 0.00089\n",
            "Epoch 3; train: 0.00084, val: 0.00084\n",
            "Epoch 4; train: 0.00080, val: 0.00081\n",
            "Epoch 5; train: 0.00077, val: 0.00078\n",
            "Epoch 6; train: 0.00075, val: 0.00077\n",
            "Epoch 7; train: 0.00074, val: 0.00075\n",
            "Epoch 8; train: 0.00073, val: 0.00074\n",
            "Epoch 9; train: 0.00072, val: 0.00073\n",
            "Epoch 10; train: 0.00071, val: 0.00073\n"
          ]
        }
      ],
      "source": [
        "# define network\n",
        "network1 = AutoEncoder(32, 32, 10).to(device)\n",
        "\n",
        "# perform auto-encoder training \n",
        "training_loop(network1, lr=0.0005, denoise=False, alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "CYFsBnXR-iBi"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_52293/1992853878.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(noise(x, alpha), dtype=torch.float32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1; train: 0.00199, val: 0.00167\n",
            "Epoch 2; train: 0.00179, val: 0.00176\n",
            "Epoch 3; train: 0.00171, val: 0.00170\n",
            "Epoch 4; train: 0.00168, val: 0.00165\n",
            "Epoch 5; train: 0.00165, val: 0.00161\n",
            "Epoch 6; train: 0.00164, val: 0.00157\n",
            "Epoch 7; train: 0.00163, val: 0.00155\n",
            "Epoch 8; train: 0.00162, val: 0.00151\n",
            "Epoch 9; train: 0.00161, val: 0.00150\n",
            "Epoch 10; train: 0.00160, val: 0.00148\n"
          ]
        }
      ],
      "source": [
        "# define network\n",
        "network1 = AutoEncoder(32, 32, 10).to(device)\n",
        "\n",
        "# perform auto-encoder training \n",
        "training_loop(network1, lr=0.0005, denoise=True, alpha=0.5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H2wfOMLWLrN2"
      },
      "source": [
        "### Task 8: Reconstruction Result\n",
        "\n",
        "This task is to visualize the reconstructed images from their originals.\n",
        "For this purpose, select one image for each label from the first batch of the validation set. \n",
        "Generate the noisy input by Task 2.\n",
        "\n",
        "Forward the clean images through the trained auto-encoder network to extract their reconstructions. \n",
        "Forward the noisy inputs through the trained denoising network to remove noise.\n",
        "\n",
        "To show the difference, plot the original sample, reconstructed sample, noisy sample, and denoised sample for one label in one column. \n",
        "Make a single plot with 4 rows and 10 columns. \n",
        "See the reference plot in the slides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "YiliTlgkyDUu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_52293/976784709.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  noisy = torch.tensor(noise(original), dtype=torch.float32).to(device)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x600 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "validation_loader = torch.utils.data.DataLoader(dataset = validation_set, batch_size=128)\n",
        "\n",
        "# get first validation set batch\n",
        "x,t = next(iter(validation_loader))\n",
        "\n",
        "# select first image of each class  \n",
        "select = [ x[t==i][0] for i in range(10) ]\n",
        "\n",
        "\n",
        "# If required, convert the list of select images into tensor through torch.stack\n",
        "original = torch.stack(select)\n",
        "\n",
        "# compute reconstructed samples\n",
        "reconstructed = network1(original.to(device)).cpu().detach()  \n",
        "# generate noisy samples\n",
        "noisy = torch.tensor(noise(original), dtype=torch.float32).to(device)\n",
        "# compute denoised samples\n",
        "denoised = network1(noisy).cpu().detach()\n",
        "\n",
        "samples = [original, reconstructed, noisy, denoised]\n",
        "\n",
        "# plot images\n",
        "from matplotlib import pyplot\n",
        "pyplot.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "fig, axes = pyplot.subplots(nrows=2, ncols = 5, figsize=(15, 6))\n",
        "\n",
        "for i in range(2):\n",
        "  for j in range(5):\n",
        "    axes[i][j].imshow(samples[i][j].squeeze())\n",
        "    axes[i][j].axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "2dd53f8ad749bca69f7250ce75eb4f0def59db5cf79075a9716322ffc58e8a2e"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('DL')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
