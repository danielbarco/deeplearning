{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jDR8f9l2LrNu"
      },
      "source": [
        "# Assignment 9: Convolutional Auto-Encoder\n",
        "\n",
        "In this assignment, we show that it is possible to learn from unlabeled data using a convolutional auto-encoder network. \n",
        "The task is to reduce (a noisy version of) an image of the handwritten digits of MNIST into a deep feature representation, without making use of their labels, and reconstruct the sample from that representation.\n",
        "\n",
        "For this purpose, we implement a convolutional auto-encoder and a denoising auto-encoder that learn a $K=10$-dimensional deep feature representation of each image, and uses this representation to reconstruct images to the original size of $28\\times28$ pixels.\n",
        "We show that such a network can be used to denoise images."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EYyIQ7dufhHT"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "We will make use of the default implementation of the `torchvision.datasets.MNIST` dataset. \n",
        "MNIST has 10 labels of digit images.\n",
        "However, besides the last task, we do not make use of the labels of the dataset, but we only utilize the images.\n",
        "We instantiate the training and test sets of MNIST -- again we will use the test set for validation purposes."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3_VRuI9Bfd1s"
      },
      "source": [
        "### Task 1: Datasets\n",
        "\n",
        "Instantiate the training and validation datasets of MNIST, i.e., split the training and validation sets, and make use of data loaders.\n",
        "Select a simple `ToTensor` transform.\n",
        "Instantiate a training data loader using a batch size of $B=32$, and a validation data loader with 100 samples in a batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "EW5O0-dvLrNw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "B = 32 # batch size\n",
        "\n",
        "# training set and data loader\n",
        "train_set = trainset = torchvision.datasets.MNIST(\n",
        "  root = \"./data\",\n",
        "  train=True, download=True, transform=torchvision.transforms.ToTensor()\n",
        ")\n",
        "train_loader = torch.utils.data.DataLoader(dataset = trainset, batch_size=B)\n",
        "\n",
        "# validation set and data loader\n",
        "validation_set = testset = torchvision.datasets.MNIST(\n",
        "  root = \"./data\",\n",
        "  train=False, download=True, transform=torchvision.transforms.ToTensor()\n",
        ")\n",
        "validation_loader = torch.utils.data.DataLoader(dataset = validation_set, batch_size=B)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oJzhaUraSpPG"
      },
      "source": [
        "### Task 2: Random Noise\n",
        "\n",
        "Implement a function to generate noisy images by adding a uniform noise to the clean images:\n",
        "\n",
        "$$\\mathbf X_{\\mathrm{noise}} = \\mathbf X + \\alpha \\mathbf U^{D\\times E}$$ \n",
        "\n",
        "where $\\mathbf U$ is a tensor with entries generated from a uniform distribution between $-1$ and $1$. \n",
        "$D=E=28$ are the width and the height of the original image. \n",
        "To represent actual images, we will restrict the pixel values of $\\mathbf X_{\\mathrm{noise}}$ to be in the range $[0,1]$ by clipping any value that is outside that range.\n",
        "\n",
        "Note that this function will also be used with batches of samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "N_DefcwdSpPH"
      },
      "outputs": [],
      "source": [
        "def noise(x, alpha=0.5):\n",
        "  # generate noise \n",
        "  N = np.random.uniform(-1, 1, x.shape)\n",
        "  # Add noise and clamp\n",
        "  noisy_sample = torch.clamp(x + alpha * N, 0, 1)\n",
        "\n",
        "  return noisy_sample"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_6QRshMXgHoc"
      },
      "source": [
        "### Noisy Image Visualization\n",
        "\n",
        "Here we just assure that the images are correct, by displaying them to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "KW3QT2jVgLFM"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# get first validation image\n",
        "input = validation_set[0][0]\n",
        "\n",
        "# generate noise image\n",
        "noisy = noise(input)\n",
        "\n",
        "# plot images\n",
        "from matplotlib import pyplot\n",
        "pyplot.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "fig, ax = pyplot.subplots(1, 2)\n",
        "ax[0].imshow(input.squeeze())\n",
        "ax[0].axis(\"off\")\n",
        "ax[1].imshow(noisy.squeeze())\n",
        "ax[1].axis(\"off\")\n",
        "pyplot.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "392dfD3Xht-h"
      },
      "source": [
        "## Auto-Encoder Network\n",
        "\n",
        "The auto-encoder network is composed of two parts: the encoder that transforms the input image to a deep feature representation; and the decoder that produces an image from such a deep feature.\n",
        "\n",
        "For the encoder $\\mathcal E$, we will basically use the same convolutional network topology as in the last assignment.\n",
        "An exception is that we perform our down-sampling via striding and not via pooling.\n",
        "After each convolution, we apply the ReLU activation.\n",
        "The output of the encoder is a $K=10$ dimensional deep feature representation.\n",
        "The complete encoder network topology can be found below in Topology 1(a).\n",
        "\n",
        "The decoder $\\mathcal D$ performs the inverse operations of the encoder.\n",
        "A fully-connected layer is used to increase the number of samples to the same size as the output of the flattening of the encoder.\n",
        "Then, a ReLU activation is applied. \n",
        "The flattening needs to be undone next by reshaping the vector into the correct dimensionality.\n",
        "A fractionally-strided convolutional layer increases the intermediate representation by a factor of 2.\n",
        "Note that the fractionally-strided convolution is implemented in `torch.nn.ConvTranspose2d`, and the `stride` parameter should have the same value as for the encoder.\n",
        "Additionally, the `torch.nn.ConvTranspose2d` has a parameter `output_padding` which needs to be adapted to reach the correct output shape (see Test 1).\n",
        "After this layer, we perform another ReLU activation and another fractionally-strided convolution to arrive at the original input dimension.\n",
        "The complete decoder network topology can be found below in Topology 1(b).\n",
        "\n",
        "Finally, we combine the two sub-networks into one auto-encoder network.\n",
        "While there exist several possibilities for doing this, we will implement a third `torch.nn.Module` that contains an instance of the encoder and an instance of the decoder."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iDSQU7nvluAM"
      },
      "source": [
        "Topology 1: Network configurations of the (a) encoder and (b) decoder networks\n",
        "\n",
        "(a) Encoder Network\n",
        "\n",
        "*   2D convolutional layer with $Q_1$ channels, kernel size $5\\times5$, **stride 2** and padding 2\n",
        "*   activation function ReLU\n",
        "*   2D convolutional layer with $Q_2$ channels, kernel size $5\\times5$, **stride 2** and padding 2\n",
        "*   flatten layer to convert the convolution output into a vector\n",
        "*   activation function ReLU\n",
        "*   fully-connected layer with the correct number of inputs and $K$ outputs\n",
        "\n",
        "(b) Encoder Network\n",
        "\n",
        "*   fully-connected layer with $K$ inputs and the correct number of outputs\n",
        "*   activation function ReLU\n",
        "*   reshaping to convert the vector into a convolution input\n",
        "*   2D **fractionally-strided convolutional** layer with $Q_2$ channels, kernel size $5\\times5$, stride 2 and padding 2\n",
        "*   activation function ReLU\n",
        "*   2D **fractionally-strided convolutional** layer with $Q_1$ channels, kernel size $5\\times5$, stride 2 and padding 2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sulFbFbPLrNx"
      },
      "source": [
        "### Task 3: Encoder Network\n",
        "\n",
        "Implement the encoder network for given parameters $Q_1$, $Q_2$, and $K$ as given in Topology 1(a).\n",
        "Implement a network class that derives from `torch.nn.Module` and implement the `__init__` and the `forward` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "IASbngJNLrNy"
      },
      "outputs": [],
      "source": [
        "class Encoder (torch.nn.Module):\n",
        "  def __init__(self, Q1, Q2, K):\n",
        "    # call base class constrcutor\n",
        "    super(Encoder,self).__init__()\n",
        "      # define convolutional layers\n",
        "    self.conv1 = torch.nn.Conv2d(1, Q1, kernel_size= [5,5], stride=2, padding=2)\n",
        "    self.conv2 = torch.nn.Conv2d(Q1, Q2, kernel_size= [5,5], stride=2, padding=2)\n",
        "    # pooling and activation functions will be re-used for the different stages\n",
        "    self.act = torch.nn.ReLU()\n",
        "    # define fully-connected layers\n",
        "    self.flatten = torch.nn.Flatten()\n",
        "    # ?? how do I know the size of the input to the first fully-connected layer?\n",
        "    \n",
        "    self.fc = torch.nn.Linear(Q2 * 7 * 7, K)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # get the deep feature representation\n",
        "        # compute first layer of convolution, pooling and activation\n",
        "    a = self.act(self.conv1(x))\n",
        "    # compute second layer of convolution, pooling and activation\n",
        "    a = self.act(self.conv2(a))\n",
        "    # get the deep features as the output of the first fully-connected layer\n",
        "    # ?? why do I flatten before input to the first fully-connected layer?\n",
        "    deep_feature = self.act(self.fc(self.flatten(a)))\n",
        "    return deep_feature"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6OH24In8LrNy"
      },
      "source": [
        "### Task 4: Decoder Network\n",
        "\n",
        "Implement the decoder network for given parameters $Q_1$, $Q_2$, and $K$ as given in Topology 1(b).\n",
        "Implement a network class that derives from `torch.nn.Module` and implement the `__init__` and the `forward` methods.\n",
        "The output of the decoder network is supposed to have values in the range $[0,1]$, similar to the input values.\n",
        "We need to make sure that only these values can be achieved.\n",
        "Think of possible ways of doing that, and apply the way that seems most reasonable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "YTiDRV5tLrNz"
      },
      "outputs": [],
      "source": [
        "class Decoder (torch.nn.Module):\n",
        "  def __init__(self, Q1, Q2, K):\n",
        "    # call base class constrcutor\n",
        "    super(Decoder,self).__init__()\n",
        "    # fully-connected layer\n",
        "    self.fc = torch.nn.Linear(K, Q2 * 7 * 7)\n",
        "    # convolutional layers\n",
        "    self.deconv1 = torch.nn.ConvTranspose2d(Q2, Q1, kernel_size= [5,5], stride=2, padding=2, output_padding=1)\n",
        "    self.deconv2 = torch.nn.ConvTranspose2d(Q1, 1, kernel_size= [5,5], stride=2, padding=2, output_padding=1)\n",
        "    # activation function\n",
        "    self.act = torch.nn.ReLU()\n",
        "    # unflatten\n",
        "    self.unflatten = torch.nn.Unflatten(dim=1, unflattened_size=(Q2,7,7))\n",
        "      \n",
        "  def forward(self, x):\n",
        "    # reconstruct the output image\n",
        "    a = self.unflatten(self.act(self.fc(x)))\n",
        "    a = self.act(self.deconv1(a))\n",
        "    a = self.deconv2(a)\n",
        "    output = torch.sigmoid(a)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "def output_dim(features_in, kernel_size, padding, stride, dilation = 1):\n",
        "    \"\"\"floor just returns rounded down to nearest int\n",
        "\n",
        "    :param features_in: _description_\n",
        "    :type features_in: _type_\n",
        "    :param kernel_size: _description_\n",
        "    :type kernel_size: _type_\n",
        "    :param padding: _description_\n",
        "    :type padding: _type_\n",
        "    :param stride: _description_\n",
        "    :type stride: _type_\n",
        "    \"\"\"\n",
        "    return math.floor((features_in + 2 * padding - dilation * (kernel_size - 1) -1) / stride ) +1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph6VHeFJLrNz"
      },
      "source": [
        "### Task 5: Joint Auto-Encoder Network\n",
        "\n",
        "Implement the auto-encoder network by combining the encoder and the decoder.\n",
        "In the `__init__` function, instantiate an encoder from Task 3 and a decoder from Task 4.\n",
        "In `forward`, pass the input through the encoder and the decoder: $\\mathbf Y = \\mathcal D(\\mathcal E(\\mathbf X))$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "DYdl42m3LrN0"
      },
      "outputs": [],
      "source": [
        "class AutoEncoder(torch.nn.Module):\n",
        "  def __init__(self, Q1, Q2, K):\n",
        "    super(AutoEncoder,self).__init__()\n",
        "    self.encoder = Encoder(Q1, Q2, K)\n",
        "    self.decoder = Decoder(Q1, Q2, K)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # encode input\n",
        "    deep_feature = self.encoder(x)\n",
        "    # decode to output\n",
        "    reconstructed = self.decoder(deep_feature)\n",
        "    return reconstructed"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RmZrCO0hLrN0"
      },
      "source": [
        "### Test 1: Output Sizes\n",
        "\n",
        "The code below instantiates the auto-encoder network with $Q_1 = Q_2 = 32$ and $K=10$. \n",
        "Then the given input $\\mathbf X$ is provided to the (untrained) auto-encoder network.\n",
        "Use this code to verify that the deep feature extracted by the encoder and the output from the decoder part both have the desired size. \n",
        "Also, we verify that the output values are between 0 and 1.\n",
        "\n",
        "If the tests cannot be passed, please check the implementations in Task 3, 4, and 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "yLttmKotLrN1"
      },
      "outputs": [],
      "source": [
        "# run on cuda device?\n",
        "device = torch.device(\"cuda\")\n",
        "# create network\n",
        "network = AutoEncoder(32, 32, 10).to(device)\n",
        "\n",
        "# create or select a sample\n",
        "x = torch.randn((1,1,28,28))\n",
        "\n",
        "# use encoder to encode image and check its size\n",
        "deep_features = network.encoder(x.to(device))\n",
        "assert deep_features.shape[1] == 10\n",
        "\n",
        "# use decoder to generate an image and check its size and value range\n",
        "output = network.decoder(deep_features)\n",
        "assert output.shape[2:] == (28,28)\n",
        "assert torch.all(output >= 0) and torch.all(output <= 1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ped3OmgBi5lD"
      },
      "source": [
        "## Training and Evaluation\n",
        "We will implement two training procedures, a simple auto-encoder network and a denoising auto-encoder network, which should be combined into one function.\n",
        "To train the network, we will use the $L_2$ distance between the output and the input of the network as a loss function, which is implemented in `torch.nn.MSELoss`:\n",
        "\n",
        "  $$\\mathcal J^{L_2} (\\mathbf X, \\mathbf Y) = \\|\\mathbf X - \\mathbf Y\\|^2$$\n",
        "\n",
        "For optimization, we will make use of the `Adam` optimizer with a learning rate of $\\eta=0.001$.\n",
        "We will run the training for 10 epochs and compute training and validation set loss after each epoch.\n",
        "\n",
        "Denoising training requires generating noisy images before forwarding them into the network and taking the loss between output and the clean image.\n",
        "\n",
        "For evaluation, we will check whether some of the validation set samples are correctly reconstructed from the auto-encoder network and whether the noise is removed from the denoising network.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dhJJQpLVSpPH"
      },
      "source": [
        "### Task 6: Training Loop\n",
        "\n",
        "For a given network and learning rate, implement a function that initiates loss function (`torch.nn.MSELoss`), optimizer, and trains the network for 10 epochs on the training data.\n",
        "If parameter `denoise` is `True`, generate noisy batch with factor `alpha`.\n",
        "Compute the running average of the training loss for each epoch.\n",
        "At the end of each epoch, also compute the validation set loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "kx-zo0jXLrN1"
      },
      "outputs": [],
      "source": [
        "def training_loop(network, lr=0.001, denoise=False, alpha=0.5):\n",
        "  # define optimizer\n",
        "  optimizer = torch.optim.Adam(network.parameters(), lr=lr)\n",
        "  # define loss function\n",
        "  loss = torch.nn.MSELoss()\n",
        "\n",
        "  for epoch in range(10):\n",
        "    # evaluate average loss for training and validation set\n",
        "    train_loss = validation_loss = 0.\n",
        "\n",
        "    for x,_ in train_loader:\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # generate noise images by factor alpha as input if denoise is True\n",
        "      if denoise:\n",
        "        x = noise(x, alpha)\n",
        "      \n",
        "      # compute network output\n",
        "      y = network(x.to(device))\n",
        "\n",
        "      # compute loss between output and input\n",
        "      J = loss(y, x.to(device))\n",
        "      # perform update\n",
        "      J.backward()\n",
        "      optimizer.step()\n",
        "      # accumulate loss\n",
        "      train_loss += J.item()\n",
        "\n",
        "\n",
        "    # compute validation loss\n",
        "    with torch.no_grad():\n",
        "      for x,t in validation_loader:\n",
        "        # generate noise images by factor alpha as input if denoise is True\n",
        "\n",
        "        # compute network output\n",
        "        y = network(x.to(device))\n",
        "        # compute loss\n",
        "        J = loss(y, x.to(device))\n",
        "        # accumulate loss\n",
        "        validation_loss += J.item()\n",
        "\n",
        "\n",
        "    # print average loss for training and validation\n",
        "    print(f\"\\rEpoch {epoch+1}; train: {train_loss/len(train_set):1.5f}, val: {validation_loss/len(validation_set):1.5f}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aOTDzpzugjGs"
      },
      "source": [
        "### Task 7: Training of two Networks\n",
        "\n",
        "Instantiate two `AutoEncoder` networks, one to train with only clean samples, and one to train with noisy images. Call the training loop for each network.\n",
        "\n",
        "Note: If the training loss and validation loss do not decrease during training, try to reduce the learning rate (to $\\eta=0.0005$ or even lower) and restart the training.\n",
        "You will need to re-initialize the network, too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "HiN0hu4HgtgZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1; train: 0.00352, val: 0.00090\n",
            "Epoch 2; train: 0.00350, val: 0.00090\n",
            "Epoch 3; train: 0.00350, val: 0.00090\n",
            "Epoch 4; train: 0.00350, val: 0.00090\n",
            "Epoch 5; train: 0.00350, val: 0.00090\n"
          ]
        }
      ],
      "source": [
        "# define network\n",
        "network1 = AutoEncoder(32, 32, 10).to(device)\n",
        "\n",
        "# perform auto-encoder training \n",
        "training_loop(network1, lr=0.001, denoise=False, alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYFsBnXR-iBi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1; train: 0.00643, val: 0.00645\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[113], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m network1 \u001b[39m=\u001b[39m AutoEncoder(\u001b[39m32\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m10\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m \u001b[39m# perform auto-encoder training \u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m training_loop(network1, lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m, denoise\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, alpha\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m)\n",
            "Cell \u001b[0;32mIn[111], line 11\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(network, lr, denoise, alpha)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m      8\u001b[0m   \u001b[39m# evaluate average loss for training and validation set\u001b[39;00m\n\u001b[1;32m      9\u001b[0m   train_loss \u001b[39m=\u001b[39m validation_loss \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[0;32m---> 11\u001b[0m   \u001b[39mfor\u001b[39;00m x,_ \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m     \u001b[39m# generate noise images by factor alpha as input if denoise is True\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \n\u001b[1;32m     15\u001b[0m     \u001b[39m# compute network output\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     y \u001b[39m=\u001b[39m network(x\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m     18\u001b[0m     \u001b[39m# compute loss between output and input\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    128\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:167\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    166\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n\u001b[0;32m--> 167\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mview(pic\u001b[39m.\u001b[39msize[\u001b[39m1\u001b[39m], pic\u001b[39m.\u001b[39msize[\u001b[39m0\u001b[39m], \u001b[39mlen\u001b[39m(pic\u001b[39m.\u001b[39;49mgetbands()))\n\u001b[1;32m    168\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m    169\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:1290\u001b[0m, in \u001b[0;36mImage.getbands\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         ims\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(\u001b[39mfilter\u001b[39m\u001b[39m.\u001b[39mfilter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mim\u001b[39m.\u001b[39mgetband(c))))\n\u001b[1;32m   1288\u001b[0m     \u001b[39mreturn\u001b[39;00m merge(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode, ims)\n\u001b[0;32m-> 1290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetbands\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1291\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[39m    Returns a tuple containing the name of each band in this image.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[39m    For example, ``getbands`` on an RGB image returns (\"R\", \"G\", \"B\").\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1296\u001b[0m \u001b[39m    :rtype: tuple\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m     \u001b[39mreturn\u001b[39;00m ImageMode\u001b[39m.\u001b[39mgetmode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode)\u001b[39m.\u001b[39mbands\n",
            "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1758\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_is_thread_alive.py:9\u001b[0m, in \u001b[0;36mis_thread_alive\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m      6\u001b[0m _temp \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mThread()\n\u001b[1;32m      7\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(_temp, \u001b[39m'\u001b[39m\u001b[39m_is_stopped\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# Python 3.x has this\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mis_thread_alive\u001b[39m(t):\n\u001b[1;32m     10\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mnot\u001b[39;00m t\u001b[39m.\u001b[39m_is_stopped\n\u001b[1;32m     12\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(_temp, \u001b[39m'\u001b[39m\u001b[39m_Thread__stopped\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# Python 2.x has this\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# define network\n",
        "network1 = AutoEncoder(32, 32, 10).to(device)\n",
        "\n",
        "# perform auto-encoder training \n",
        "training_loop(network1, lr=0.001, denoise=True, alpha=0.5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H2wfOMLWLrN2"
      },
      "source": [
        "### Task 8: Reconstruction Result\n",
        "\n",
        "This task is to visualize the reconstructed images from their originals.\n",
        "For this purpose, select one image for each label from the first batch of the validation set. \n",
        "Generate the noisy input by Task 2.\n",
        "\n",
        "Forward the clean images through the trained auto-encoder network to extract their reconstructions. \n",
        "Forward the noisy inputs through the trained denoising network to remove noise.\n",
        "\n",
        "To show the difference, plot the original sample, reconstructed sample, noisy sample, and denoised sample for one label in one column. \n",
        "Make a single plot with 4 rows and 10 columns. \n",
        "See the reference plot in the slides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "YiliTlgkyDUu"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Input type (double) and bias type (float) should be the same",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[139], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m noisy \u001b[39m=\u001b[39m noise(original)\n\u001b[1;32m     17\u001b[0m \u001b[39m# compute denoised samples\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m denoised \u001b[39m=\u001b[39m network1(noisy\u001b[39m.\u001b[39;49mto(device))\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     20\u001b[0m samples \u001b[39m=\u001b[39m [original, reconstructed, noisy, denoised]\n\u001b[1;32m     22\u001b[0m \u001b[39m# plot images\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[102], line 9\u001b[0m, in \u001b[0;36mAutoEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[1;32m      8\u001b[0m   \u001b[39m# encode input\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m   deep_feature \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m     10\u001b[0m   \u001b[39m# decode to output\u001b[39;00m\n\u001b[1;32m     11\u001b[0m   reconstructed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(deep_feature)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[68], line 20\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     18\u001b[0m   \u001b[39m# get the deep feature representation\u001b[39;00m\n\u001b[1;32m     19\u001b[0m       \u001b[39m# compute first layer of convolution, pooling and activation\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m   a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x))\n\u001b[1;32m     21\u001b[0m   \u001b[39m# compute second layer of convolution, pooling and activation\u001b[39;00m\n\u001b[1;32m     22\u001b[0m   a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(a))\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (double) and bias type (float) should be the same"
          ]
        }
      ],
      "source": [
        "\n",
        "validation_loader = torch.utils.data.DataLoader(dataset = validation_set, batch_size=128)\n",
        "\n",
        "# get first validation set batch\n",
        "x,t = next(iter(validation_loader))\n",
        "\n",
        "# select first image of each class  \n",
        "select = [ x[t==i][0] for i in range(10) ]\n",
        "\n",
        "\n",
        "# If required, convert the list of select images into tensor through torch.stack\n",
        "original = torch.stack(select)\n",
        "\n",
        "# compute reconstructed samples\n",
        "reconstructed = network1(original.to(device)).cpu().detach()  \n",
        "# generate noisy samples\n",
        "noisy = noise(original)\n",
        "# compute denoised samples\n",
        "denoised = network1(noisy.to(device)).cpu().detach()\n",
        "\n",
        "samples = [original, reconstructed, noisy, denoised]\n",
        "\n",
        "# plot images\n",
        "from matplotlib import pyplot\n",
        "pyplot.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "fig, axes = pyplot.subplots(nrows=2, ncols = 5, figsize=(15, 6))\n",
        "\n",
        "for i in range(2):\n",
        "  for j in range(5):\n",
        "    axes[i][j].imshow(samples[i][j].squeeze())\n",
        "    axes[i][j].axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "2dd53f8ad749bca69f7250ce75eb4f0def59db5cf79075a9716322ffc58e8a2e"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('DL')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
