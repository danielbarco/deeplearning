{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HanD0NjcXVH_"
      },
      "source": [
        "# Assignment 12: Radial Basis Function Network\n",
        "\n",
        "In this assignment, we show an alternative learning strategy that has been forgotten in the last few years but might get more popularity in the near future.\n",
        "A radial basis function layer has some similarities with a fully-connected layer, but the outputs of the layer are not determined by a dot product between the weights and the input but rather by computing a distance.\n",
        "Unfortunately, `PyTorch` does not provide us with an implementation of an RBF layer, so we need to implement our own.\n",
        "Additionally, the activation function is replaced by a function that creates higher outputs for small absolute inputs, such as a Gaussian distribution:\n",
        "\n",
        "$$a_r = ||\\vec w_r - \\vec x\\|^2 $$ \n",
        "$$h_r = e^{-\\frac{a_r^2}{2 \\sigma_r^2}}$$\n",
        "\n",
        "\n",
        "Finally, we want to combine the convolutional network with an RBF layer and a final fully-connected layer to compute the 10 outputs.\n",
        "For simplicity, let us define $K$ as the input dimension of our RBF layer, $R$ as the number of basis functions (the number of outputs) of our RBF layer, and $O=10$ as the number of outputs (logits) of our network.\n",
        "The complete network topology is given below.\n",
        "We are both interested in the deep feature representation that is the output of the first fully-connected layer and in the logits that are output from the second fully-connected layer.\n",
        "\n",
        "1. 2D convolutional layer with $Q_1$ channels, kernel size $5\\times5$, stride 1 and padding 2\n",
        "2. 2D maximum pooling layer with kernel size $2\\times2$ and stride 2\n",
        "3. Activation function ReLU\n",
        "4. 2D convolutional layer with $Q_2$ channels, kernel size $5\\times5$, stride 1 and padding 2\n",
        "5. 2D maximum pooling layer with kernel size $2\\times2$ and stride 2\n",
        "6. Activation function ReLU\n",
        "7. Flatten layer to convert the convolution output into a vector\n",
        "8. Fully-connected layer with the correct number of inputs and $K$ outputs\n",
        "9. RBF layer with $K$ inputs and $R$ outputs (implemented in Task 2)\n",
        "10. RBF activation function (implemented in Task 3)\n",
        "11. Fully-connected layer with $R$ inputs and $O$ outputs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6Eu6i0ZpH7-X"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "We will train and test our methods on the MNIST dataset.\n",
        "\n",
        "### Task 1: Dataset\n",
        "\n",
        "We will make use of the default implementations of the MNIST dataset.\n",
        "As usual, we will need the training and validation set splits of MNIST, including data loaders.\n",
        "Select appropriate batch sizes for training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8K08_O3-XVIE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "# define transformation\n",
        "transform = torchvision.transforms.ToTensor()\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "validationset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "\n",
        "B = 128 # batch size\n",
        "# training set and data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset = trainset, batch_size=B)\n",
        "\n",
        "# validation set and data loader\n",
        "validation_loader = torch.utils.data.DataLoader(dataset = validationset, batch_size=B)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ziu2RG2-XVIF"
      },
      "source": [
        "## Radial Basis Function\n",
        "\n",
        "We will split our implementation into three different parts.\n",
        "First, we implement the output $a_r$ as provided above.\n",
        "Second, we implement the activation function $h_r$.\n",
        "Finally, we build our network by inserting an RBF layer in between the fully-connected layers of our default network.\n",
        "\n",
        "In order to implement the output $a_k$ of the RBF layer, we will use a weight matrix $\\mathbf W\\in \\mathbb R^{R\\times K} = \\left[\\vec w_1, \\vec w_2,\\ldots, \\vec w_R \\right]$ where each vector is of dimension $K$.\n",
        "When handling batches $\\mathbf X \\in \\mathbb R^{B\\times K}$, we require to compute our activations as:\n",
        "\n",
        "  $$\\mathbf A \\in \\mathbb R^{B\\times R} = (a_{n,r}) \\qquad \\text{with} \\qquad a_{n,r} =  \\bigl\\|\\vec w_r - \\vec x^{[n]}\\bigr\\|^2 = \\sum\\limits_{k=1}^K \\bigl\\|w_{r,k} - \\vec x_r^{[n]}\\bigr\\|^2$$\n",
        "\n",
        "In order to speed up processing and enable the use of `tensor` operations, we need to bring $\\mathbf W$ and $\\mathbf X$ to the same size $\\mathbb R^{B\\times R\\times K}$ by logically (**not physically!**) copying the data and the weights to $\\mathcal W$ and $\\mathcal X$.\n",
        "Then, the resulting $ \\mathcal A = || \\mathcal W - \\mathcal X||$ needs to be summed up over dimension $K$ to arrive at $\\mathbf A$ as given above.\n",
        "\n",
        "\n",
        "### Task 2: Radial Basis Function Layer\n",
        "\n",
        "Implement the RBF layer in PyTorch that computes the activation $\\mathbf A$ of a radial basis function. Derive your layer from `torch.nn.Module`.\n",
        "In the `__init__` function, instantiate the weight matrix $\\mathbf W$ as a `torch.nn.Parameter`, and initialize the weight values randomly from values in range $[-2,2]$.\n",
        "In `forward`, compute and return the activation from the stored weight matrix and the given input batch as indicated above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y-8GCoUkXVIG"
      },
      "outputs": [],
      "source": [
        "class RBFLayer(torch.nn.Module):\n",
        "  def __init__(self, K, R):\n",
        "    # call base class constructor\n",
        "    super(RBFLayer, self).__init__()\n",
        "    self.K = K\n",
        "    self.R = R\n",
        "    # store a parameter for the basis functions\n",
        "    self.W = torch.nn.Parameter()\n",
        "    # initialize the matrix between -2 and 2\n",
        "    self.W.data.uniform_(-2, 2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # collect the required shape parameters, B, R, K\n",
        "    # B, R, K = Batch, x.shape[1], class\n",
        "    B, R, K = x.shape[0], self.R, self.K\n",
        "    print(B,R,K)\n",
        "    # Bring the weight matrix of shape R,K to size B,R,K by adding batch dimension (B, dim 0)\n",
        "    W = self.W.expand(B)\n",
        "    print(W.shape)\n",
        "    # Bring the input matrix of shape B,K to size B,R,K by adding R dimension (dim=1)\n",
        "    X = x.expand(B, R, K)\n",
        "    # compute the activation euclidean distance\n",
        "    A = torch.sqrt(torch.sum((W - X)**2, dim=2))\n",
        "    return A"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0fTgxXDnXVIH"
      },
      "source": [
        "### Task 3: Radial Basis Function Activation\n",
        "\n",
        "The activation function also requires a `Parameter`, i.e., the standard deviations of the Gaussian.\n",
        "Hence, we need to implement the activation function also as a `torch.nn.Module`.\n",
        "Here, we are treating the denominator of the Gaussian as a separate variable: `sigma2 = 2*sigma*sigma`.\n",
        "\n",
        "Implement the activation function with learnable `sigma2` parameters.\n",
        "Initialize all `sigma` parameters with the value of 1.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oBTIu_0bXVII"
      },
      "outputs": [],
      "source": [
        "class RBFActivation(torch.nn.Module):\n",
        "  def __init__(self, R):\n",
        "    # call base class constructor\n",
        "    super(RBFActivation, self).__init__()\n",
        "    self.R = R\n",
        "    # store a parameter for the basis functions\n",
        "    sigma = torch.nn.Parameter()\n",
        "    # initialize sigma 1\n",
        "    sigma.data.fill_(1)\n",
        "    self.sigma2 = 2 * sigma**2\n",
        "  def forward(self, x):\n",
        "    # implement the RBF activation function\n",
        "    output = torch.exp(- x**2 / self.sigma2)\n",
        "    return output"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gBh5O3BHQ-yl"
      },
      "source": [
        "### Test 1: RBF Layer and Activation\n",
        "\n",
        "We instantiate an RBF layer and an RBF activation function for $K=4$ and $R=12$ and generate a random batch of size $B=16$.\n",
        "We call both the RBF layer and the activation on the batch, and make sure that the resulting output is of shape $B\\times R$.\n",
        "This test also assures that the layers above are functioning and do not raise exceptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sVX3oJ6QQ-yz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16 12 4\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "The expanded size of the tensor (4) must match the existing size (0) at non-singleton dimension 2.  Target sizes: [16, 12, 4].  Tensor sizes: [0]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m test_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mempty((\u001b[39m16\u001b[39m,\u001b[39m4\u001b[39m))\n\u001b[1;32m      8\u001b[0m \u001b[39m# forward test data through the layer and the activation\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m a \u001b[39m=\u001b[39m test_RBF_layer(test_data)\n\u001b[1;32m     10\u001b[0m h \u001b[39m=\u001b[39m test_RBF_activation(a)\n\u001b[1;32m     12\u001b[0m \u001b[39m# check that the shape is correct\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[2], line 18\u001b[0m, in \u001b[0;36mRBFLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(B,R,K)\n\u001b[1;32m     17\u001b[0m \u001b[39m# Bring the weight matrix of shape R,K to size B,R,K by adding batch dimension (B, dim 0)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m W \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW\u001b[39m.\u001b[39;49mexpand(B, R, K)\n\u001b[1;32m     19\u001b[0m \u001b[39mprint\u001b[39m(W\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     20\u001b[0m \u001b[39m# Bring the input matrix of shape B,K to size B,R,K by adding R dimension (dim=1)\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (4) must match the existing size (0) at non-singleton dimension 2.  Target sizes: [16, 12, 4].  Tensor sizes: [0]"
          ]
        }
      ],
      "source": [
        "# instantiate layer and activation\n",
        "test_RBF_layer = RBFLayer(K=4, R=12)\n",
        "test_RBF_activation = RBFActivation(R=12)\n",
        "\n",
        "# create test data batch\n",
        "test_data = torch.empty((16,4))\n",
        "\n",
        "# forward test data through the layer and the activation\n",
        "a = test_RBF_layer(test_data)\n",
        "h = test_RBF_activation(a)\n",
        "\n",
        "# check that the shape is correct\n",
        "assert h.shape == (16,12)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "najtqWa9XVIJ"
      },
      "source": [
        "### Task 4: Radial Basis Function Network\n",
        "\n",
        "As the network, we rely on our convolutional network from Assignment 8.\n",
        "However, this time we add an RBF layer and its activation between the first and the second fully-connected layer.\n",
        "We will return both the deep features of dimension $K$ and the logits of dimension $O$ in `forward`, which we will use later for visualization purposes.\n",
        "Note that the processing will happen on batch level.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHtAdGwtXVIK"
      },
      "outputs": [],
      "source": [
        "class RBFNetwork(torch.nn.Module):\n",
        "  def __init__(self, Q1, Q2, K, R, O):\n",
        "    # call base class constrcutor\n",
        "    super(RBFNetwork,self).__init__()\n",
        "    # convolutional define layers\n",
        "    self.conv1 = ...\n",
        "    self.conv2 = ...\n",
        "    # pooling and activation functions will be re-used for the different stages\n",
        "    self.pool = ...\n",
        "    self.act = ...\n",
        "    # define first fully-connected layer\n",
        "    self.flatten = ...\n",
        "    self.fc1 = ...\n",
        "    # define RBF layer and its activation\n",
        "    self.rbf_layer = ...\n",
        "    self.rbf_activation = ...\n",
        "    # define second fully-connected layer\n",
        "    self.fc2 = ...\n",
        "  \n",
        "  def forward(self,x):\n",
        "    ...\n",
        "    # get the deep feature layer as the output of the first fully-connected layer\n",
        "    deep_feature = ...\n",
        "    # apply the RBF layer and activation\n",
        "    ...\n",
        "    # apply the last fully-connected layer to obtain the logits\n",
        "    logits = ...\n",
        "    # return both the logits and the deep features\n",
        "    return logits, deep_feature\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PW3L6HWqXVIK"
      },
      "source": [
        "### Task 5: Training and Validation Loop\n",
        "\n",
        "The training and validation loops are as usual.\n",
        "Instantiate the network with $Q_1=64$, $Q_2=32$, $K=2$, $R=100$ and $O=10$.\n",
        "Instantiate loss function and optimizer.\n",
        "Train the network on the training set.\n",
        "Compute the validation set accuracy after each epoch of training.\n",
        "\n",
        "Hints: \n",
        "\n",
        "* The validation set accuracy after the first epoch should be more than 80%. If it is much lower, increase the learning rate and/or change the optimizer.\n",
        "* On the other hand, if the accuracy gets stuck around 10% and does not change over the epochs, reduce the learning rate.\n",
        "* The training on the GPU might take several minutes. On the CPU, training times might be increased."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-UbzcihRWb8"
      },
      "outputs": [],
      "source": [
        "def training_loop(network):\n",
        "  optimizer = ...\n",
        "  loss = ...\n",
        "\n",
        "  for epoch in range(20):\n",
        "    accuracy_train = 0\n",
        "    for x,t in train_loader:\n",
        "      \n",
        "      # train network with the current batch\n",
        "        ...\n",
        "\n",
        "    # compute validation set accuracy\n",
        "    accuracy_valid = 0\n",
        "    with torch.no_grad():\n",
        "      for x,t in validation_loader:\n",
        "        ...\n",
        "\n",
        "    print(F\"Epoch {epoch+1}: train accuracy: {accuracy_train/len(train_set):1.4f} test accuracy: {accuracy_valid/len(validation_set):1.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbiKj8MdJXQ6"
      },
      "outputs": [],
      "source": [
        "# instantiate RBFnetwork and train model\n",
        "network = RBFNetwork(...)\n",
        "training_loop(...)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VVTlgxaq0JY0"
      },
      "source": [
        "### (Optional) Task 5a: \n",
        "\n",
        "Consider the minimum number for $R$, and use this number to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abneIe54XVIL"
      },
      "outputs": [],
      "source": [
        "# instantiate RBFnetwork and train model\n",
        "network_minimal = RBFNetwork(...)\n",
        "training_loop(...)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xizKtFn6XVIL"
      },
      "source": [
        "## Visualization\n",
        "We have selected the dimensionality of the deep features to be $K=2$ in order to be able to visualize them.\n",
        "The goal is to extract the deep feature representations of all validation samples.\n",
        "For each sample, we regard the 2D representation as a point in a 2D space, which we can mark with a color that is dependent on the label of that sample.\n",
        "After plotting a dot for each of the validation samples, we can observe if we find some structure in the feature space.\n",
        "\n",
        "Finally, also the learned basis functions $\\vec w_r$ can be plotted, including their according to size $\\sigma_r$.\n",
        "This allows us to see whether the RBF network has adapted itself to the data."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oSpq6R6LJiWI"
      },
      "source": [
        "### Task 6: Deep Feature Extraction\n",
        "\n",
        "Iterate through the validation set samples and extract the 2D deep feature representations for the images.\n",
        "Store the results in 10 different lists, one for each target class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDpNrbSeXVIM"
      },
      "outputs": [],
      "source": [
        "def get_features(network):\n",
        "  # extract all deep features for all validation set samples\n",
        "  features = [[] for _ in range(10)]\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x,t in validation_loader:\n",
        "      # extract deep features\n",
        "      ...\n",
        "      # separate the 10 different targets into separate lists\n",
        "      ...\n",
        "\n",
        "  # convert features to 2D for later processing/plotting\n",
        "  features = [torch.stack(f) for f in features]\n",
        "  return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gF7Y6yOrOsUP"
      },
      "outputs": [],
      "source": [
        "# call the function to get the deep features\n",
        "features = ..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PFGcHVas1BeI"
      },
      "source": [
        "### (Optional) Task 6a: Deep Feature Extraction for `network_minimal`\n",
        "Follow the exact same setup as Task 6. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tswhlg-4SpNN"
      },
      "outputs": [],
      "source": [
        "# get the deep features from the network_minimal\n",
        "features_minimal = ..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VWyNBj68XVIM"
      },
      "source": [
        "### Task 7: Deep Feature Visualization & Task 8: Basis Function Visualization\n",
        "\n",
        "We provide a list of 10 different colors, one for each digit class.\n",
        "For each of the 10 digits, plot all the samples in a 2D plot in the according color as a dot, maybe using `pyplot.scatter`. \n",
        "\n",
        "Obtain the basis functions that were learned during network training from the layer implemented in Task 2.\n",
        "Obtain the scaling factor $\\sigma_r$ for each of the basis functions that were learned by the activation function layer from Task 3.\n",
        "For each of the basis functions $\\vec w_r$, draw a black circle with a radius corresponding to `sigmas`, which is computed from `sigma2` as follows:\n",
        "$$\\mathrm{sigmas} = \\sqrt\\frac{\\mathrm{sigma2}}{2}$$\n",
        "Overlay the plot from Task 7 with these circles.\n",
        "\n",
        "Notes:\n",
        "\n",
        "* The `s=` parameter to the `scatter` function is given in pts, so you might need to scale those values to be visible.\n",
        "* Since each notebook cell uses its own drawing process, we need to combine Tasks 7 and 8 here.\n",
        "* An example for $R=100$ can be found from exercise -- obtaining the shown legend is more difficult and does not need to be replicated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLgFDfkLXVIM"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "from matplotlib import pyplot\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "def plot(features, network):\n",
        "  # define 10 visually distinct colors\n",
        "  colors = numpy.array([\n",
        "      [230, 25, 75],\n",
        "      [60, 180, 75],\n",
        "      [255, 225, 25],\n",
        "      [67, 99, 216],\n",
        "      [245, 130, 49],\n",
        "      [145, 30, 180],\n",
        "      [70, 240, 240],\n",
        "      [240, 50, 230],\n",
        "      [188, 246, 12],\n",
        "      [250, 190, 190],\n",
        "  ]) / 255.\n",
        "\n",
        "\n",
        "  # generate 10 scatter plots inside the same figure, one for each label\n",
        "  ...\n",
        "\n",
        "  # get the basis functions from the rbf layer\n",
        "  basis_functions = ...\n",
        "  # get the sizes from the rbf activation\n",
        "  sigma2 =  ...\n",
        "  sigmas = ...\n",
        "\n",
        "  # plot learned centers\n",
        "  legend = pyplot.scatter(..., ..., color=\"k\", marker=\"o\", s=..., facecolors=\"none\")\n",
        "\n",
        "  # make the plot more beatuiful\n",
        "  ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9QCGZUFVe3h"
      },
      "outputs": [],
      "source": [
        "# call plot function and plot the result\n",
        "plot(...)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hueu1bsE1Xev"
      },
      "source": [
        "### (Optional) Task 7a: Deep Feature Visualization & Task 8: Basis Function Visualization for another network\n",
        "\n",
        "Follow the exact same setup as Task 7. This time, plot results for `network_minimal`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A25jJokkVmYN"
      },
      "outputs": [],
      "source": [
        "# instantiate plot function and plot the result\n",
        "plot(...)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "a29cabff5744fce69e08a959ab87b9e77a9f67b498d08783caa8c3bb16f23a00"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('DL')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
