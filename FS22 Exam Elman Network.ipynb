{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3Sy3dTGcaCKy"
      },
      "source": [
        "# 2. Activity Prediction\n",
        "\n",
        "First, we download and extract the data.\n",
        "We read them into two lists, one containing the input data $\\mathcal X$ and one the target classes $\\mathbf T$ as integral values.\n",
        "The input data $\\mathcal X \\in \\mathbb R^{M\\times N\\times D}$ is organized such that we have $M=15$ subjects with various numbers $N$ of samples, each with a data dimension of $D=3$.\n",
        "The target data $\\mathbf T \\in \\mathbb \\{0,...,6\\}^{M\\times N}$ provides the target class for each sample.\n",
        "\n",
        "Please do not modify this code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mFuJD6ozaCK5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of subjects: 15\n",
            "number of samples for the first subject: 162500\n",
            "length of one input sample: 3\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# download data\n",
        "if not os.path.exists(\"UserActivity.zip\"):\n",
        "  import urllib.request\n",
        "  urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00287/Activity%20Recognition%20from%20Single%20Chest-Mounted%20Accelerometer.zip\", \"UserActivity.zip\")\n",
        "  print (\"Downloaded datafile\", \"UserActivity.zip\")\n",
        "\n",
        "# extract data\n",
        "if not os.path.exists(\"Activity Recognition from Single Chest-Mounted Accelerometer\"):\n",
        "  import zipfile\n",
        "  zipfile.ZipFile(\"UserActivity.zip\").extractall()\n",
        "  print (\"Extracted datafile\", \"UserActivity.zip\")\n",
        "\n",
        "# read the data from the files\n",
        "X = []\n",
        "T = []\n",
        "for i in range(1,16):\n",
        "  # collect samples for each of the 15 subjects\n",
        "  with open(f\"Activity Recognition from Single Chest-Mounted Accelerometer/{i}.csv\") as f:\n",
        "    # inputs and targets for this subject\n",
        "    x, t = [], []\n",
        "    for i, line in enumerate(f):\n",
        "      # split the data\n",
        "      splits = line.rstrip().split(\",\")\n",
        "      # get the target value\n",
        "      tt = int(splits[-1])\n",
        "      # there are some invalid target values, we skip these\n",
        "      if tt: \n",
        "        # load the three accelorometer data\n",
        "        x.append([float(v) for v in splits[1:-1]])\n",
        "        # add the label (convert from one-based into zero-based indexing)\n",
        "        t.append(tt-1)\n",
        "    # append samples and targets of the current subject\n",
        "    X.append(x)\n",
        "    T.append(t)\n",
        "\n",
        "# print some statistics of the dataset\n",
        "print (f\"number of subjects: {len(X)}\\nnumber of samples for the first subject: {len(X[0])}\\nlength of one input sample: {len(X[0][0])}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UjkZsIvsaCK-"
      },
      "source": [
        "### 2. (d) Data Reduction\n",
        "\n",
        "Implement a strategy to reduce the amount of data for each subject. Assure that you apply the identical selection strategy for the inputs and the targets. Make sure that you do not change the arrangement of the data matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AQoNfk0MaCLA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of subjects: 15\n",
            "number of samples for the first subject: 500\n",
            "length of one input sample: 3\n",
            "number of subjects: 15\n",
            "number of targets for the first subject: 500\n"
          ]
        }
      ],
      "source": [
        "#I want to reduce the data by reducing dimension N: \n",
        "# select subset of data\n",
        "X_selected = [x[:500] for x in X] \n",
        "T_selected = [t[:500] for t in T] \n",
        "\n",
        "print (f\"number of subjects: {len(X_selected)}\\nnumber of samples for the first subject: {len(X_selected[0])}\\nlength of one input sample: {len(X_selected[0][0])}\")\n",
        "print (f\"number of subjects: {len(T_selected)}\\nnumber of targets for the first subject: {len(T_selected[0])}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z_Y-GoL-aCLC"
      },
      "source": [
        "Here, we concatenate the data of subjects 1-10 into the training matrices, and the data of subjects 11-15 into the validation set. \n",
        "There is no need to change this code (unless you changed the arrangement of the data above, in which case you also need to adapt this code)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7EWL_UUgaCLD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape of the training input is: torch.Size([5000, 3])\n",
            "the shape of the training targets is torch.Size([5000])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "X_train = torch.tensor(sum(X_selected[:10], []))\n",
        "X_val = torch.tensor(sum(X_selected[10:], []))\n",
        "\n",
        "T_train = torch.tensor(sum(T_selected[:10], []))\n",
        "T_val = torch.tensor(sum(T_selected[10:], []))\n",
        "\n",
        "print(f\"The shape of the training input is: {X_train.shape}\\nthe shape of the training targets is {T_train.shape}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "orqmeFsLaCLF"
      },
      "source": [
        "### 2. (e) Dataset Implementation\n",
        "\n",
        "Implement the dataset that takes the given data `X` and `T`, as well as a sequence length `S`. Return a sequence of `S` samples and the label for the last element in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Yu30QQN0aCLH"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  \n",
        "  def __init__(self, X, T, S):\n",
        "    # implement the constructor\n",
        "    self.X = X\n",
        "    self.T = T\n",
        "    self.S = S\n",
        "\n",
        "  def __len__(self):\n",
        "    # return the number of samples in this dataset\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # return the pair of input and target values for the given index\n",
        "    if index+self.S >= len(self.X):\n",
        "      return self.X[index:], self.T[index:]\n",
        "    return self.X[index:index+self.S], self.T[index+self.S]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8v8rKM4faCLI"
      },
      "source": [
        "### 2. (f) Data Loaders\n",
        "\n",
        "We need to instantiate training and validation set data loaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GwBmYRxoaCLK"
      },
      "outputs": [],
      "source": [
        "# instantiate training set data loader\n",
        "train_set = Dataset(X_train, T_train, 10)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "\n",
        "# and the validation set data loader\n",
        "val_set = Dataset(X_val, T_val, 10)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=32, shuffle=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wPZDKDnnaCLL"
      },
      "source": [
        "### 2. (g) Network Implementation\n",
        "\n",
        "Implement and instantiate one of the networks discussed in (a). Since there are various different ways to implement this network, no guidelines will be provided here. Note that the network should output the prediction only for the last sequence element."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5N9J8a8yaCLM"
      },
      "outputs": [],
      "source": [
        "# implement and instantiate the network\n",
        "\n",
        "class ElmanNetwork(torch.nn.Module):\n",
        "  def __init__(self, D, K):\n",
        "    super(ElmanNetwork,self).__init__()\n",
        "    self.W1 = torch.nn.Linear(D, K)\n",
        "    self.Wr = torch.nn.Linear(K, K)\n",
        "    self.W2 = torch.nn.Linear(K, D)\n",
        "    self.activation = torch.nn.PReLU()\n",
        "    self.K = K\n",
        "\n",
        "  def forward(self, x):\n",
        "    # get the shape of the data\n",
        "    B, S, D = x.shape\n",
        "    # initialize the hidden vector in the desired size with 0\n",
        "    # remember to put it on the device\n",
        "    h_s = torch.zeros((B,self.K)).to(device)\n",
        "    # store all logits (we will need them in the loss function)\n",
        "    Z = torch.empty(x.shape, device=device)\n",
        "    # iterate over the sequence\n",
        "    for s in range(S):\n",
        "      # use current sequence item\n",
        "      x_s = x[:, s]\n",
        "      # compute recurrent activation\n",
        "      a_s = self.W1(x_s) + self.Wr(h_s)\n",
        "      # apply activation function\n",
        "      h_s = self.activation(a_s)\n",
        "      # compute logit values\n",
        "      z = self.W2(h_s)\n",
        "      # store logit value\n",
        "      Z[:,s] = z\n",
        "\n",
        "    # return logits for all sequence elements\n",
        "    return Z\n",
        "\n",
        "network = ElmanNetwork(10, 10).to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AzNxHb7baCLN"
      },
      "source": [
        "### 2. (h) Network Training\n",
        "\n",
        "Instantiate the loss function and the optimizer. Train the network for 10 epochs and compute validation set accuracy. Note that one epoch of training might take several minutes. There is no need to wait for the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jlZNJ9nzaCLO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 10, 3])\n",
            "X:  tensor([[[2092., 2274., 1995.],\n",
            "         [1973., 2185., 2131.],\n",
            "         [2102., 2350., 2281.],\n",
            "         [2157., 2449., 2345.],\n",
            "         [2095., 2345., 2216.],\n",
            "         [2242., 2535., 2169.],\n",
            "         [2096., 2445., 2190.],\n",
            "         [2689., 2654., 2130.],\n",
            "         [1132., 2805., 2343.],\n",
            "         [2378., 2092., 2584.]],\n",
            "\n",
            "        [[2038., 2042., 1668.],\n",
            "         [2043., 2040., 1663.],\n",
            "         [2038., 2041., 1665.],\n",
            "         [2039., 2041., 1658.],\n",
            "         [2042., 2043., 1664.],\n",
            "         [2044., 2039., 1659.],\n",
            "         [2043., 2042., 1662.],\n",
            "         [2041., 2041., 1661.],\n",
            "         [2048., 2037., 1668.],\n",
            "         [2043., 2039., 1659.]],\n",
            "\n",
            "        [[1931., 2395., 1899.],\n",
            "         [1931., 2354., 1940.],\n",
            "         [1913., 2344., 1963.],\n",
            "         [1865., 2333., 1971.],\n",
            "         [1895., 2341., 1958.],\n",
            "         [1906., 2357., 1948.],\n",
            "         [1919., 2383., 1932.],\n",
            "         [1901., 2413., 1918.],\n",
            "         [1890., 2323., 1967.],\n",
            "         [1901., 2336., 1943.]],\n",
            "\n",
            "        [[1954., 1978., 1633.],\n",
            "         [1951., 1984., 1638.],\n",
            "         [1951., 1981., 1637.],\n",
            "         [1958., 1980., 1642.],\n",
            "         [1950., 1982., 1636.],\n",
            "         [1950., 1980., 1636.],\n",
            "         [1953., 1980., 1638.],\n",
            "         [1953., 1980., 1636.],\n",
            "         [1955., 1981., 1634.],\n",
            "         [1959., 1983., 1630.]],\n",
            "\n",
            "        [[2085., 2166., 1702.],\n",
            "         [2082., 2157., 1703.],\n",
            "         [2077., 2159., 1707.],\n",
            "         [2081., 2159., 1707.],\n",
            "         [2081., 2161., 1698.],\n",
            "         [2082., 2163., 1704.],\n",
            "         [2078., 2159., 1701.],\n",
            "         [2085., 2160., 1701.],\n",
            "         [2081., 2157., 1705.],\n",
            "         [2085., 2162., 1703.]],\n",
            "\n",
            "        [[2021., 2343., 2287.],\n",
            "         [1974., 2378., 2194.],\n",
            "         [1999., 2441., 2067.],\n",
            "         [1912., 2447., 1983.],\n",
            "         [1802., 2404., 2038.],\n",
            "         [1980., 2308., 2216.],\n",
            "         [2067., 2254., 2386.],\n",
            "         [1966., 2289., 2343.],\n",
            "         [1939., 2369., 2160.],\n",
            "         [1992., 2381., 2143.]],\n",
            "\n",
            "        [[2049., 2381., 1916.],\n",
            "         [2046., 2325., 2059.],\n",
            "         [2079., 2408., 2083.],\n",
            "         [2092., 2421., 2048.],\n",
            "         [2123., 2460., 1995.],\n",
            "         [2026., 2422., 2049.],\n",
            "         [1917., 2401., 2152.],\n",
            "         [1879., 2378., 2234.],\n",
            "         [1917., 2346., 2293.],\n",
            "         [1988., 2279., 1457.]],\n",
            "\n",
            "        [[2180., 2642., 1951.],\n",
            "         [2183., 2442., 1891.],\n",
            "         [2070., 2726., 1987.],\n",
            "         [2148., 2516., 2007.],\n",
            "         [2239., 2457., 1957.],\n",
            "         [2251., 2611., 2052.],\n",
            "         [2115., 2595., 1904.],\n",
            "         [2191., 2362., 1328.],\n",
            "         [2114., 2491., 2003.],\n",
            "         [2098., 2510., 2093.]],\n",
            "\n",
            "        [[1982., 2001., 1652.],\n",
            "         [1986., 2003., 1649.],\n",
            "         [1987., 2003., 1650.],\n",
            "         [1978., 2003., 1651.],\n",
            "         [1983., 1999., 1654.],\n",
            "         [1984., 2003., 1650.],\n",
            "         [1991., 2001., 1650.],\n",
            "         [1984., 2001., 1646.],\n",
            "         [1989., 2005., 1649.],\n",
            "         [1983., 2000., 1648.]],\n",
            "\n",
            "        [[2044., 2042., 1661.],\n",
            "         [2039., 2037., 1664.],\n",
            "         [2035., 2042., 1664.],\n",
            "         [2039., 2041., 1666.],\n",
            "         [2037., 2042., 1663.],\n",
            "         [2037., 2037., 1661.],\n",
            "         [2038., 2039., 1665.],\n",
            "         [2041., 2039., 1663.],\n",
            "         [2044., 2040., 1658.],\n",
            "         [2044., 2032., 1661.]],\n",
            "\n",
            "        [[1875., 2375., 1942.],\n",
            "         [1883., 2377., 1932.],\n",
            "         [1895., 2377., 1924.],\n",
            "         [1935., 2394., 1922.],\n",
            "         [1927., 2438., 1906.],\n",
            "         [1910., 2417., 1893.],\n",
            "         [1895., 2396., 1899.],\n",
            "         [1871., 2387., 1915.],\n",
            "         [1869., 2362., 1957.],\n",
            "         [1866., 2387., 1989.]],\n",
            "\n",
            "        [[2035., 2328., 1834.],\n",
            "         [2041., 2331., 1829.],\n",
            "         [2034., 2325., 1839.],\n",
            "         [2038., 2332., 1843.],\n",
            "         [2035., 2338., 1839.],\n",
            "         [2039., 2323., 1851.],\n",
            "         [2033., 2320., 1841.],\n",
            "         [2035., 2334., 1839.],\n",
            "         [2030., 2334., 1831.],\n",
            "         [2029., 2319., 1825.]],\n",
            "\n",
            "        [[2041., 2037., 1663.],\n",
            "         [2044., 2038., 1658.],\n",
            "         [2045., 2040., 1661.],\n",
            "         [2044., 2036., 1663.],\n",
            "         [2042., 2038., 1667.],\n",
            "         [2046., 2041., 1666.],\n",
            "         [2039., 2043., 1669.],\n",
            "         [2045., 2039., 1664.],\n",
            "         [2046., 2039., 1666.],\n",
            "         [2043., 2042., 1663.]],\n",
            "\n",
            "        [[2095., 2387., 2105.],\n",
            "         [2094., 2353., 2054.],\n",
            "         [2067., 2349., 2005.],\n",
            "         [2070., 2367., 1980.],\n",
            "         [2086., 2418., 1991.],\n",
            "         [2107., 2483., 2051.],\n",
            "         [2133., 2557., 2129.],\n",
            "         [2133., 2611., 2154.],\n",
            "         [2107., 2589., 2158.],\n",
            "         [2057., 2532., 2109.]],\n",
            "\n",
            "        [[2034., 2325., 1839.],\n",
            "         [2038., 2332., 1843.],\n",
            "         [2035., 2338., 1839.],\n",
            "         [2039., 2323., 1851.],\n",
            "         [2033., 2320., 1841.],\n",
            "         [2035., 2334., 1839.],\n",
            "         [2030., 2334., 1831.],\n",
            "         [2029., 2319., 1825.],\n",
            "         [2030., 2308., 1831.],\n",
            "         [2028., 2301., 1834.]],\n",
            "\n",
            "        [[1909., 2364., 1999.],\n",
            "         [1909., 2358., 2009.],\n",
            "         [1907., 2366., 2018.],\n",
            "         [1908., 2367., 2018.],\n",
            "         [1913., 2368., 2017.],\n",
            "         [1924., 2365., 2024.],\n",
            "         [1928., 2365., 2023.],\n",
            "         [1929., 2363., 2022.],\n",
            "         [1926., 2360., 2024.],\n",
            "         [1921., 2367., 2023.]],\n",
            "\n",
            "        [[2042., 2038., 1662.],\n",
            "         [2050., 2046., 1667.],\n",
            "         [2043., 2038., 1666.],\n",
            "         [2045., 2041., 1666.],\n",
            "         [2047., 2035., 1665.],\n",
            "         [2042., 2039., 1668.],\n",
            "         [2041., 2040., 1662.],\n",
            "         [2043., 2041., 1665.],\n",
            "         [2044., 2041., 1661.],\n",
            "         [2039., 2042., 1664.]],\n",
            "\n",
            "        [[1927., 2359., 1974.],\n",
            "         [1912., 2345., 1958.],\n",
            "         [1939., 2348., 1969.],\n",
            "         [1892., 2264., 1978.],\n",
            "         [1923., 2437., 1967.],\n",
            "         [1882., 2401., 1958.],\n",
            "         [1950., 2527., 1975.],\n",
            "         [1955., 2431., 1975.],\n",
            "         [1955., 2443., 2000.],\n",
            "         [1957., 2416., 1996.]],\n",
            "\n",
            "        [[2081., 2160., 1702.],\n",
            "         [2081., 2162., 1702.],\n",
            "         [2085., 2161., 1699.],\n",
            "         [2079., 2162., 1701.],\n",
            "         [2081., 2163., 1703.],\n",
            "         [2082., 2164., 1703.],\n",
            "         [2080., 2161., 1705.],\n",
            "         [2082., 2161., 1704.],\n",
            "         [2082., 2165., 1701.],\n",
            "         [2080., 2159., 1706.]],\n",
            "\n",
            "        [[1957., 1979., 1633.],\n",
            "         [1956., 1979., 1641.],\n",
            "         [1955., 1981., 1636.],\n",
            "         [1948., 1979., 1642.],\n",
            "         [1955., 1978., 1638.],\n",
            "         [1954., 1981., 1634.],\n",
            "         [1953., 1982., 1638.],\n",
            "         [1950., 1984., 1630.],\n",
            "         [1951., 1977., 1634.],\n",
            "         [1953., 1975., 1638.]],\n",
            "\n",
            "        [[1957., 2434., 2026.],\n",
            "         [1996., 2456., 1991.],\n",
            "         [1971., 2441., 2002.],\n",
            "         [1970., 2348., 1999.],\n",
            "         [1907., 2347., 1930.],\n",
            "         [1884., 2363., 2054.],\n",
            "         [1910., 2377., 2028.],\n",
            "         [1949., 2368., 2022.],\n",
            "         [1971., 2326., 2015.],\n",
            "         [1978., 2373., 1999.]],\n",
            "\n",
            "        [[1865., 2371., 2011.],\n",
            "         [1865., 2382., 1994.],\n",
            "         [1872., 2391., 1975.],\n",
            "         [1884., 2385., 1965.],\n",
            "         [1887., 2392., 1962.],\n",
            "         [1876., 2389., 1959.],\n",
            "         [1874., 2385., 1959.],\n",
            "         [1877., 2387., 1957.],\n",
            "         [1896., 2380., 1957.],\n",
            "         [1918., 2369., 1964.]],\n",
            "\n",
            "        [[2040., 2037., 1670.],\n",
            "         [2046., 2040., 1664.],\n",
            "         [2040., 2039., 1662.],\n",
            "         [2039., 2039., 1663.],\n",
            "         [2036., 2035., 1661.],\n",
            "         [2039., 2037., 1662.],\n",
            "         [2044., 2042., 1663.],\n",
            "         [2042., 2038., 1662.],\n",
            "         [2040., 2042., 1664.],\n",
            "         [2045., 2035., 1666.]],\n",
            "\n",
            "        [[2020., 2559., 1981.],\n",
            "         [2022., 2545., 2001.],\n",
            "         [2024., 2538., 1995.],\n",
            "         [2029., 2538., 2008.],\n",
            "         [2031., 2535., 1998.],\n",
            "         [2030., 2527., 1989.],\n",
            "         [2026., 2533., 1999.],\n",
            "         [2027., 2538., 2000.],\n",
            "         [2034., 2535., 1991.],\n",
            "         [2030., 2535., 1976.]],\n",
            "\n",
            "        [[2048., 2037., 1661.],\n",
            "         [2045., 2041., 1655.],\n",
            "         [2044., 2043., 1662.],\n",
            "         [2039., 2046., 1662.],\n",
            "         [2042., 2038., 1662.],\n",
            "         [2043., 2038., 1661.],\n",
            "         [2051., 2043., 1658.],\n",
            "         [2040., 2040., 1661.],\n",
            "         [2042., 2039., 1661.],\n",
            "         [2042., 2036., 1664.]],\n",
            "\n",
            "        [[1681., 2243., 1949.],\n",
            "         [1651., 2188., 1951.],\n",
            "         [1632., 2226., 1956.],\n",
            "         [1632., 2227., 1990.],\n",
            "         [1648., 2226., 1953.],\n",
            "         [1643., 2242., 1955.],\n",
            "         [1638., 2284., 1966.],\n",
            "         [1586., 2279., 1882.],\n",
            "         [1578., 2309., 1748.],\n",
            "         [1559., 2263., 1978.]],\n",
            "\n",
            "        [[1865., 2364., 1967.],\n",
            "         [1870., 2361., 1952.],\n",
            "         [1865., 2373., 1936.],\n",
            "         [1857., 2374., 1921.],\n",
            "         [1850., 2359., 1919.],\n",
            "         [1848., 2354., 1938.],\n",
            "         [1845., 2351., 1954.],\n",
            "         [1859., 2345., 1964.],\n",
            "         [1865., 2345., 1964.],\n",
            "         [1880., 2347., 1962.]],\n",
            "\n",
            "        [[2224., 2324., 1999.],\n",
            "         [2227., 2313., 2012.],\n",
            "         [2227., 2303., 2020.],\n",
            "         [2195., 2282., 2108.],\n",
            "         [2209., 2315., 2050.],\n",
            "         [2092., 2147., 2033.],\n",
            "         [2185., 2186., 2155.],\n",
            "         [2275., 2355., 2239.],\n",
            "         [2289., 2385., 2032.],\n",
            "         [2356., 2325., 1991.]],\n",
            "\n",
            "        [[2156., 2457., 2014.],\n",
            "         [2147., 2564., 1922.],\n",
            "         [2094., 2453., 1898.],\n",
            "         [2055., 2519., 1948.],\n",
            "         [2071., 2520., 1937.],\n",
            "         [2176., 2539., 1966.],\n",
            "         [2131., 2493., 1999.],\n",
            "         [2149., 2513., 1974.],\n",
            "         [2220., 2551., 1970.],\n",
            "         [2158., 2510., 1969.]],\n",
            "\n",
            "        [[1936., 2343., 1971.],\n",
            "         [1921., 2344., 1784.],\n",
            "         [1947., 2386., 1778.],\n",
            "         [1912., 2359., 1958.],\n",
            "         [1865., 2406., 2102.],\n",
            "         [1859., 2415., 1967.],\n",
            "         [1903., 2365., 1944.],\n",
            "         [1913., 2358., 1962.],\n",
            "         [1928., 2354., 1972.],\n",
            "         [1925., 2367., 1943.]],\n",
            "\n",
            "        [[2036., 2365., 1930.],\n",
            "         [2032., 2362., 1911.],\n",
            "         [2030., 2351., 1904.],\n",
            "         [2030., 2337., 1898.],\n",
            "         [2034., 2340., 1913.],\n",
            "         [2037., 2348., 1926.],\n",
            "         [2038., 2345., 1955.],\n",
            "         [2047., 2342., 1983.],\n",
            "         [2048., 2313., 2000.],\n",
            "         [2044., 2277., 2005.]],\n",
            "\n",
            "        [[1950., 1987., 1640.],\n",
            "         [1957., 1979., 1635.],\n",
            "         [1958., 1977., 1639.],\n",
            "         [1954., 1981., 1641.],\n",
            "         [1951., 1974., 1634.],\n",
            "         [1954., 1975., 1631.],\n",
            "         [1954., 1983., 1632.],\n",
            "         [1954., 1986., 1637.],\n",
            "         [1950., 1976., 1634.],\n",
            "         [1956., 1985., 1627.]]])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (32x3 and 10x10)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m x, t \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device), t\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m \u001b[39m# compute network output\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m z \u001b[39m=\u001b[39m network(x)\n\u001b[1;32m     17\u001b[0m \u001b[39m# compute loss, arrange order of logits and targets\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m#change order of dimensions for x, because the loss is computed over D,\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m#which is the third dimension, but needs to be the second one\u001b[39;00m\n\u001b[1;32m     20\u001b[0m J \u001b[39m=\u001b[39m loss(z\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m),t\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m))\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn[10], line 25\u001b[0m, in \u001b[0;36mElmanNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m x_s \u001b[39m=\u001b[39m x[:, s]\n\u001b[1;32m     24\u001b[0m \u001b[39m# compute recurrent activation\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m a_s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW1(x_s) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWr(h_s)\n\u001b[1;32m     26\u001b[0m \u001b[39m# apply activation function\u001b[39;00m\n\u001b[1;32m     27\u001b[0m h_s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(a_s)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x3 and 10x10)"
          ]
        }
      ],
      "source": [
        "# instantiate optimizer and loss function\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(network.parameters(), lr=0.001)\n",
        "\n",
        "val_acc = []\n",
        "\n",
        "# train the network for 10 epochs\n",
        "for epoch in range(10):\n",
        "  # use all training samples in batches\n",
        "   for x, t in train_loader:\n",
        "    print(x.shape)\n",
        "    print(\"X: \", x)\n",
        "    # put data on device\n",
        "    x, t = x.to(device), t.to(device)\n",
        "    # compute network output\n",
        "    z = network(x)\n",
        "    # compute loss, arrange order of logits and targets\n",
        "    #change order of dimensions for x, because the loss is computed over D,\n",
        "    #which is the third dimension, but needs to be the second one\n",
        "    J = loss(z.permute(0,2,1),t.permute(0,2,1))\n",
        "    # reset gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # compute gradient for this batch\n",
        "    J.backward()\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    # select a new sequence length S in [5,20]\n",
        "    S = torch.randint(5, 21, (1,)).item()\n",
        "    \n",
        "  # compute validation set accuracy\n",
        "   with torch.no_grad():\n",
        "    # store all predictions\n",
        "    predictions = []\n",
        "    # iterate over all validation samples\n",
        "    for x, t in val_loader:\n",
        "      # put data on device\n",
        "      x, t = x.to(device), t.to(device)\n",
        "      # compute network output\n",
        "      z = network(x)\n",
        "      # compute predictions\n",
        "      pred = torch.argmax(z, dim=2)\n",
        "      # store predictions\n",
        "      predictions.append(pred)\n",
        "    # concatenate all predictions\n",
        "    predictions = torch.cat(predictions)\n",
        "    # compute accuracy\n",
        "    acc = (predictions == T_val).float().mean()\n",
        "    # store accuracy\n",
        "    val_acc.append(acc.item())\n",
        "\n",
        "    # report validation set accuracy\n",
        "    print(f\"epoch {epoch+1}, validation accuracy: {acc.item():.3f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DL-FS22-Exam-Task2.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "a29cabff5744fce69e08a959ab87b9e77a9f67b498d08783caa8c3bb16f23a00"
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('DL')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
