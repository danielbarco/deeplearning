{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Universal Function Approximator\n",
    "\n",
    "\n",
    "The goal of this exercise is to train a two-layer fully-connected network to perform one-dimensional non-linear regression via gradient descent. To show the flexibility of the approach, three different functions will be approximated. First, the network and its gradient need to be implemented. Second, target data for three different functions will be generated. Finally, the training procedure will be applied to the data, and the resulting approximated function will be plotted together with the data samples.\n",
    "\n",
    "## Network Implementation\n",
    "\n",
    "A two-layer network is defined by parameters $\\Theta=(\\mathbf W^{(1)},\\vec w^{(2)})$ that are split into $\\mathbf W^{(1)}\\in\\mathbb R^{K\\times {(D+1)}}$ for the first layer and $\\vec w^{(2)}\\in\\mathbb R^{K+1}$ for the second layer. In our case, since we have only a single input, we have $D=1$.\n",
    "For a given input $\\vec x = (1, x)^T$, the network is implemented in three steps:\n",
    "\n",
    "1. Compute the first layer output, aka, the activation: $\\vec a_- = \\mathbf W^{(1)} \\vec x$\n",
    "2. Apply the activation function for each element of $\\vec a_- : \\vec h_- = g(\\vec a_-)$ and prepend the bias neuron $h_0=1$ to arrive at $\\vec h$.\n",
    "3. Compute the output of the network: $y = \\vec w^{(2)}\\ ^T\\vec h$.\n",
    "\n",
    "### Task 1  \n",
    "Implement a function that returns the network output for a given input $\\vec x$ and parameters $\\Theta=(\\mathbf W^{(1)}, \\vec w^{(2)})$. Remember that the input of the function $\\vec x = (1, x)^T$. Also remember to prepend $h_0=1$ in your implementation.\n",
    "\n",
    "We use hyperbolic tangent $(\\tanh)$ as the activation function:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\tanh(a) = \\frac{e^{a}-e^{-a}}{e^{a}+e^{-a}}\n",
    "\\end{equation*}\n",
    "\n",
    "Note:\n",
    "\n",
    "1. Use the `numpy` implemention of the hyperbolic tangent function.\n",
    "2. Use `numpy.concatenate` or `numpy.insert` to prepend $h_0$.\n",
    "3. Make use of `numpy.dot` to compute matrix-vector and vector-vector products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def network(x, Theta):\n",
    "  W1, w2 = Theta\n",
    "  x = torch.tensor(x, dtype=torch.float32)\n",
    "  W1 = torch.tensor(W1, dtype=torch.float32)\n",
    "  w2 = torch.tensor(w2, dtype=torch.float32)\n",
    "  a_ = torch.matmul(W1, x)\n",
    "  h_ = torch.tanh(a_)\n",
    "  h = torch.cat((torch.tensor([1.0]), h_))\n",
    "  y = torch.dot(w2, h)\n",
    "  return y.item(), h.numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 1: Sanity Check\n",
    "----------------------------\n",
    "\n",
    "We select a specific number of hidden neurons and create the weights accordingly, using all zeros in the first layer and all ones in the second. The test case below assures that the function from Task 1 actually returns $1$ for those weights.\n",
    "\n",
    "Note: your function should pass the test below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_358485/4069139751.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "/tmp/ipykernel_358485/4069139751.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  W1 = torch.tensor(W1, dtype=torch.float32)\n",
      "/tmp/ipykernel_358485/4069139751.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w2 = torch.tensor(w2, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "K = 20\n",
    "D = 1\n",
    "W1 = torch.zeros((K, D+1))\n",
    "w2 = torch.ones(K+1)\n",
    "x = torch.rand(D+1)\n",
    "\n",
    "y, _ = network(x, (W1, w2))\n",
    "assert abs(1 - y) < 1e-6\n",
    "print(\"Test passed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Implementation\n",
    "\n",
    "In order to perform gradient descent, we need to define a loss function. As provided in the lecture, the $\\mathcal J^{L_2}$ loss function is defined over a dataset $X=\\{(\\vec x^{[n]}, t^{[n]})\\}$, that is defined as a list of tuples, as follows:\n",
    "\n",
    "$$\n",
    "   \\mathcal J^{L_2} = \\frac{1}{N}\\sum_{i=1}^N (y^{[n]}-t^{[n]})^2\n",
    "$$\n",
    "\n",
    "where $y^{[n]}$ is the output of the network from Task 1 when inputting $\\vec x^{[n]}$. Interestingly, however, we never explicitly need to compute the output of the loss function. It is only used to analytically compute the gradient as shown in the lecture.\n",
    "\n",
    "The gradient is composed of two items, one for each layer. Particularly, for a given dataset $X$, the gradient of loss $J^{L_2}$ is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "  \\frac{\\partial \\mathcal J}{\\partial w_{kd}^{(1)}} &= \\frac{2}{N} \\sum\\limits_{n=1}^N (y^{[n]}-t^{[n]}) w_{k}^{(2)} (1-h_{k}^{[n]}\\cdot h_{k}^{[n]}) x_{d}^{[n]}\\\\\n",
    "  \\frac{\\partial \\mathcal J}{\\partial w_{k}^{(2)}} &= \\frac{2}{N} \\sum\\limits_{n=1}^N (y^{[n]}-t^{[n]}) w_{k}^{(2)} h_{k}^{[n]}\n",
    "\\end{align}\n",
    "\n",
    "### Task 2\n",
    "Implement a function that returns the gradient as defined in $(1)$ and $(2)$ for a given dataset $X$, and given weights $\\Theta=(\\mathbf W^{(1)},\\vec w^{(2)})$. Make sure that both parts of the gradient are computed. \n",
    "\n",
    "Hint:\n",
    "\n",
    "1. Make use of the the function implemented in Task 1 where appropriate\n",
    "\n",
    "Note:\n",
    "\n",
    "  1. This is a slow implementation. We will see how to speed this up in the next lecture.\n",
    "  2. You can make use of `numpy.zeros` to initialize the gradient.\n",
    "  3. The outper product can be computed via `numpy.outer`.\n",
    "  4. Remember that we used the $\\tanh$ activation function in our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, Theta):\n",
    "  # split parameters for easier handling\n",
    "  W1, w2 = Theta\n",
    "  \n",
    "  # define gradient with respect to both parameters\n",
    "  dW1 = torch.zeros(W1.shape)\n",
    "  dw2 = torch.zeros(w2.shape)\n",
    "\n",
    "  # iterate over dataset\n",
    "  for x, t in X:\n",
    "    # compute the gradient\n",
    "    y, h =  network(x, (W1, w2))\n",
    "    dW1 += torch.outer((y-t) * w2 * (1-h * h),  x)\n",
    "    dw2 += (y-t) * h\n",
    "\n",
    "  # anything else?\n",
    "  dW1 *= 2/len(X)\n",
    "  dw2 *= 2/len(X)\n",
    "  \n",
    "  return dW1, dw2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "The procedure of gradient descent is the repeated application of two steps.\n",
    " \n",
    "1. The gradient of loss $\\nabla_{\\Theta}\\mathcal J^{L_2}$ is computed based on the current value of the parameters $\\Theta=(\\mathbf W^{(1)},\\vec w^{(2)})$.\n",
    "2. The weights are updated by moving a small step $\\eta$ into the direction of the negative gradient:\n",
    "\n",
    "$$\n",
    "    \\Theta = \\Theta - \\eta \\nabla_{\\Theta}\\mathcal J\n",
    "$$\n",
    "\n",
    "As stopping criterion, we select the number of training epochs to be 10000.\n",
    "\n",
    "### Task 3\n",
    "Implement a function that performs gradient descent for a given dataset $X$, given initial parameters $\\Theta$ and a given learning rate $\\eta$ and returns the optimized parameters $\\Theta^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Theta, eta):\n",
    "  epochs = 10000\n",
    "  # perform iterative gradient descent\n",
    "  for epoch in range(epochs):\n",
    "    # compute the gradient\n",
    "    dW1, dw2 = gradient(X, Theta)\n",
    "\n",
    "    # update the parameters\n",
    "    W1, w2 = Theta\n",
    "    W1 -= eta * dW1\n",
    "    w2 -= eta * dw2\n",
    "    \n",
    "    # print progress\n",
    "    if epoch % 1000 == 0:\n",
    "      print(\"Epoch: {}\".format(epoch))\n",
    "\n",
    "  # return optimized parameters\n",
    "  print('Done!\\n')\n",
    "  return W1, w2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datasets\n",
    "\n",
    "In total, we will test our gradient descent function with three different datasets. Particularly, we approximate:\n",
    "\n",
    "1. $X_1: t = \\sin(2x)$ for $x\\in[-2,2]$\n",
    "2. $X_2: t = e^{-2x^2}$ for $x\\in[-2,2]$\n",
    "3. $X_3: t = -x^5 - 3x^4 + 11x^3 + 27x^2 - 10x - 64$ for $x\\in[-4.5,3.5]$\n",
    "\n",
    "### Task 4\n",
    "\n",
    "Generate dataset $X_1$, for $N=50$ samples randomly drawn from range $x\\in[-2,2]$. \n",
    "Generate data $X_2$ for $N=30$ samples randomly drawn from range $x\\in[-2,2]$. \n",
    "Generate dataset $X_3$ for $N=200$ samples randomly drawn from range $x\\in[-4.5,3.5]$. \n",
    "Implement all three datasets as lists of tuples: $\\{(\\vec x^{[n]}, t^{[n]})\\mid 1\\leq n\\leq N\\}$.\n",
    "\n",
    "Note:\n",
    "\n",
    "  1. You can use `numpy.random.uniform` to create uniformly distributed samples for $x$.\n",
    "  2. Make sure that $\\vec x = (1, x)^T$ for each sample.\n",
    "  3. You can make use of `numpy.sin`, `numpy.exp` and `numpy.pow` to compute target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = [((1, x := np.random.uniform(-2, 2, 1)[0]), np.sin(2*x)) for i in range(50)]\n",
    "X2 = [((1, x := np.random.uniform(-2, 2, 1)[0]), np.exp(-2*(x)**2)) for i in range(30)]\n",
    "X3 = [((1, x := np.random.uniform(-4.5, 3.5, 1)[0]), -x**5 - 3*x**4 + 11*x**3 + 27*x**2 - 10*x - 64) for i in range(200)]\n",
    "\n",
    "X1 = [((x := torch.rand(1) * 4 - 2), torch.sin(2*x)) for i in range(50)]\n",
    "X2 = [((x := torch.rand(1) * 4 - 2), torch.exp(-2*(x)**2)) for i in range(30)]\n",
    "X3 = [((x := torch.rand(1) * 8 - 4.5), -x**5 - 3*x**4 + 11*x**3 + 27*x**2 - 10*x - 64) for i in range(200)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Sanity Check\n",
    "\n",
    "The test case below assures that the elements of each generated dataset are tuples with two elements, that the first element ($\\vec x$) is a vector with two numbers, and that the second element ($t$) is a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "isinstance() arg 2 must be a type, a tuple of types, or a union",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39;49m(\n\u001b[1;32m      2\u001b[0m     \u001b[39misinstance\u001b[39;49m(x, (\u001b[39mtuple\u001b[39;49m,\u001b[39mlist\u001b[39;49m)) \u001b[39mand\u001b[39;49;00m \n\u001b[1;32m      3\u001b[0m     \u001b[39mlen\u001b[39;49m(x) \u001b[39m==\u001b[39;49m \u001b[39m2\u001b[39;49m \u001b[39mand\u001b[39;49;00m \n\u001b[1;32m      4\u001b[0m     \u001b[39misinstance\u001b[39;49m(x[\u001b[39m0\u001b[39;49m], (\u001b[39mtuple\u001b[39;49m,\u001b[39mlist\u001b[39;49m,np\u001b[39m.\u001b[39;49mndarray, torch\u001b[39m.\u001b[39;49mtensor)) \u001b[39mand\u001b[39;49;00m \n\u001b[1;32m      5\u001b[0m     \u001b[39mlen\u001b[39;49m(x[\u001b[39m0\u001b[39;49m]) \u001b[39m==\u001b[39;49m \u001b[39m2\u001b[39;49m \u001b[39mand\u001b[39;49;00m \n\u001b[1;32m      6\u001b[0m     \u001b[39misinstance\u001b[39;49m(x[\u001b[39m1\u001b[39;49m], \u001b[39mfloat\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m     \u001b[39mfor\u001b[39;49;00m X \u001b[39min\u001b[39;49;00m (X1, X2, X3)\n\u001b[1;32m      8\u001b[0m     \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m X\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTest passed!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m      2\u001b[0m     \u001b[39misinstance\u001b[39m(x, (\u001b[39mtuple\u001b[39m,\u001b[39mlist\u001b[39m)) \u001b[39mand\u001b[39;00m \n\u001b[1;32m      3\u001b[0m     \u001b[39mlen\u001b[39m(x) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m \n\u001b[0;32m----> 4\u001b[0m     \u001b[39misinstance\u001b[39;49m(x[\u001b[39m0\u001b[39;49m], (\u001b[39mtuple\u001b[39;49m,\u001b[39mlist\u001b[39;49m,np\u001b[39m.\u001b[39;49mndarray, torch\u001b[39m.\u001b[39;49mtensor)) \u001b[39mand\u001b[39;00m \n\u001b[1;32m      5\u001b[0m     \u001b[39mlen\u001b[39m(x[\u001b[39m0\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m \n\u001b[1;32m      6\u001b[0m     \u001b[39misinstance\u001b[39m(x[\u001b[39m1\u001b[39m], \u001b[39mfloat\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m (X1, X2, X3)\n\u001b[1;32m      8\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTest passed!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: isinstance() arg 2 must be a type, a tuple of types, or a union"
     ]
    }
   ],
   "source": [
    "assert all(\n",
    "    isinstance(x, (tuple,list)) and \n",
    "    len(x) == 2 and \n",
    "    isinstance(x[0], (tuple,list,np.ndarray, torch.tensor)) and \n",
    "    len(x[0]) == 2 and \n",
    "    isinstance(x[1], float)\n",
    "    for X in (X1, X2, X3)\n",
    "    for x in X\n",
    ")\n",
    "\n",
    "print('Test passed!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Approximation\n",
    "\n",
    "Finally, we want to make use of our gradient descent implementation to approximate our functions. In order to see our success, we want to plot the functions together with the data.\n",
    "\n",
    "### Task 5 (theoretical question)\n",
    "\n",
    "When looking at the example plots in the exercise slides (exemplary solutions for the plotting Task 8), how many hidden neurons $K$ do we need in order to approximate the functions? Is there any difference\n",
    "1 between the three target functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K1 = 3\n",
    "K2 = 2\n",
    "K3 = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "For each of the datasets, randomly initialize the parameters $\\Theta_1,\\Theta_2,\\Theta_3\\in[-1,1]$ according to the number of hidden neurons estimated in Task 5.\n",
    "\n",
    "Note:\n",
    "\n",
    "  1. You can use `numpy.random.uniform` to initialize the weights.\n",
    "  2. Make sure that the weight matrices are instantiated in the correct dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta1 = (np.random.uniform(-1, 1, (K1, 2)), np.random.uniform(-1, 1, K1+1))\n",
    "Theta2 = (np.random.uniform(-1, 1, (K2, 2)), np.random.uniform(-1, 1, K2+1))\n",
    "Theta3 = (np.random.uniform(-1, 1, (K3, 2)), np.random.uniform(-1, 1, K3+1))\n",
    "\n",
    "Theta1 = (torch.rand(K1, 2) * 2 - 1, torch.rand(K1 + 1) * 2 - 1)\n",
    "Theta2 = (torch.rand(K2, 2) * 2 - 1, torch.rand(K2 + 1) * 2 - 1)\n",
    "Theta3 = (torch.rand(K3, 2) * 2 - 1, torch.rand(K3 + 1) * 2 - 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "Call gradient descent function from Task 3 using the datasets $X_1, X_2, X_3$, the according created parameters $\\Theta_1,\\Theta_2,\\Theta_3$ and a learning rate of $\\eta=0.1$. Store the resulting optimized weights $\\Theta_1^*, \\Theta_2^*, \\Theta_3^*$ and the loss values.\n",
    "\n",
    "Optimize the learning rate $\\eta$ for each of the three functions. Do you see any differences? What are the best learning rates that you can find?\n",
    "\n",
    "WARNING: Depending on the implementation, this might run for several minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_358485/4069139751.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "/tmp/ipykernel_358485/4069139751.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  W1 = torch.tensor(W1, dtype=torch.float32)\n",
      "/tmp/ipykernel_358485/4069139751.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w2 = torch.tensor(w2, dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, got 3, 3x2,1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Theta_1 \u001b[39m=\u001b[39m gradient_descent(X1, Theta1, \u001b[39m0.1\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m Theta_2 \u001b[39m=\u001b[39m gradient_descent(X2, Theta2, \u001b[39m0.1\u001b[39m)\n\u001b[1;32m      3\u001b[0m Theta_3 \u001b[39m=\u001b[39m gradient_descent(X3, Theta3, \u001b[39m0.1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(X, Theta, eta)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# perform iterative gradient descent\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      5\u001b[0m   \u001b[39m# compute the gradient\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m   dW1, dw2 \u001b[39m=\u001b[39m gradient(X, Theta)\n\u001b[1;32m      8\u001b[0m   \u001b[39m# update the parameters\u001b[39;00m\n\u001b[1;32m      9\u001b[0m   W1, w2 \u001b[39m=\u001b[39m Theta\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36mgradient\u001b[0;34m(X, Theta)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m# iterate over dataset\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m x, t \u001b[39min\u001b[39;00m X:\n\u001b[1;32m     11\u001b[0m   \u001b[39m# compute the gradient\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m   y, h \u001b[39m=\u001b[39m  network(x, (W1, w2))\n\u001b[1;32m     13\u001b[0m   dW1 \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mouter((y\u001b[39m-\u001b[39mt) \u001b[39m*\u001b[39m w2 \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mh \u001b[39m*\u001b[39m h),  x)\n\u001b[1;32m     14\u001b[0m   dw2 \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (y\u001b[39m-\u001b[39mt) \u001b[39m*\u001b[39m h\n",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m, in \u001b[0;36mnetwork\u001b[0;34m(x, Theta)\u001b[0m\n\u001b[1;32m      8\u001b[0m W1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(W1, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m      9\u001b[0m w2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(w2, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m---> 10\u001b[0m a_ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(W1, x)\n\u001b[1;32m     11\u001b[0m h_ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtanh(a_)\n\u001b[1;32m     12\u001b[0m h \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((torch\u001b[39m.\u001b[39mtensor([\u001b[39m1.0\u001b[39m]), h_))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, got 3, 3x2,1"
     ]
    }
   ],
   "source": [
    "Theta_1 = gradient_descent(X1, Theta1, 0.1)\n",
    "Theta_2 = gradient_descent(X2, Theta2, 0.1)\n",
    "Theta_3 = gradient_descent(X3, Theta3, 0.1) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Function Plotting\n",
    "\n",
    "### Task 8\n",
    "\n",
    "Implement a plotting function that takes a given dataset $X$, given parameters $\\Theta$ and a defined range $R$. Each data sample $(x^{[n]},t^{[n]})$ of the dataset is plotted as an $''x''$. In order to plot the function that is approximated by the network, generate sufficient equally-spaced input values $x\\in R$, compute the network output $y$ for these inputs, and plot them with a line.\n",
    "\n",
    "Note:\n",
    "\n",
    "  1. The dataset $X$ is defined as above, a list of tuples $(\\vec x, t)$.\n",
    "  2. Each input in the dataset is defined as $\\vec x = (1,x)^T$.\n",
    "  3. Equidistant points can be obtained via `numpy.arange`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "def plot(X, Theta, R):\n",
    "  # first, plot data samples\n",
    "  pyplot.plot(..., ..., \"rx\", label=\"Data\")\n",
    "\n",
    "  # define equidistant points from R[0] to R[1] to evaluate the network\n",
    "  x = ...\n",
    "  # compute the network outputs for these values\n",
    "  y = ...\n",
    "  # plot network approximation\n",
    "  pyplot.plot(x,y,\"k-\", label=\"network\")\n",
    "  pyplot.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9\n",
    "\n",
    "For each of the datasets and their according optimized parameters, call the plotting function from Task 8. Use range $R=[-3,3]$ for dataset $X_1$ and $X_2$, and range $R=[-5.5,4.5]$ for dataset $X_3$. Note that the first element of range $R$ should be the lowest $x$-location, and the second element of $R$ the highest value for $x$. Did the networks approximate the functions? What can we do if not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = pyplot.figure(figsize=(10,3))\n",
    "\n",
    "# plot first function\n",
    "pyplot.subplot(131)\n",
    "...\n",
    "\n",
    "# plot second function\n",
    "pyplot.subplot(132)\n",
    "...\n",
    "\n",
    "# plot third function\n",
    "pyplot.subplot(133)\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2dd53f8ad749bca69f7250ce75eb4f0def59db5cf79075a9716322ffc58e8a2e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
