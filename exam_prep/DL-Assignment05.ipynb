{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Classification in PyTorch \n",
    "\n",
    "\n",
    "For this exercise, we will switch to an implementation in PyTorch. \n",
    "The goal of this exercise is to get used to some concepts in PyTorch, such as relying on the `torch.tensor` data structure, implementing the network, the loss functions, the training loop and accuracy computation, which we will apply to binary and categorical classification.\n",
    "\n",
    "Please make sure that all your variables are compatible with `torch`.\n",
    "For example, you cannot mix `torch.tensor`s and `numpy.ndarray`s in any part of the code.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We will use two different datasets, the *spambase* dataset https://archive.ics.uci.edu/ml/datasets/spambase for binary classification and the *wine* dataset https://archive.ics.uci.edu/ml/datasets/wine for categorical classification. Both datasets are available on the UCI Machine Learning repository. \n",
    "The binary classification dataset contains features extracted from emails, which are classified as either spam or not. \n",
    "The categorical classification dataset contains some manually selected features for three different types of wines. \n",
    "For the former, the class is provided in the last column of the data file, whereas for the latter, the first index provides class information.\n",
    "\n",
    "Please run the code block below to download the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# download the two dataset files\n",
    "dataset_files = {\n",
    "  \"spambase.data\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/\",\n",
    "  \"wine.data\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/\"\n",
    "}\n",
    "for name, url in dataset_files.items():\n",
    "  if not os.path.exists(name):\n",
    "    import urllib.request\n",
    "    urllib.request.urlretrieve(url+name, name)\n",
    "    print (\"Downloaded datafile\", name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Dataset Loading\n",
    "\n",
    "The first task deals with the loading of the datasets. \n",
    "When training networks in PyTorch, all data needs to be stored as datatype ``torch.tensor``. \n",
    "The data should be split between input sets $\\mathbf X = [\\vec x^{[1]}, \\ldots, \\vec x^{[N]}]^T \\in \\mathbb R^{N\\times D}$ and targets.\n",
    "There is **no need to add a bias neuron to the input**, and the transposition of the data matrix is different from what we have seen before.\n",
    "\n",
    "For the targets, we have to be more careful as there are differences w.r.t. the applied loss function.\n",
    "For binary classification, we need $\\mathbf T = [[t^{[1]}, \\ldots, t^{[N]}]]$ to be in dimension $\\mathbb R^{N\\times1}$ and of type ``torch.float``.\n",
    "For categorical classification, we only need the class indexes $\\vec t = [t^{[1]}, \\ldots, t^{[N]}]$ to be in dimension $\\mathbb N^N$ and of type ``torch.long``.\n",
    "\n",
    "Implement a function that returns both the input and the target data for a given dataset\n",
    "\n",
    "Note:\n",
    "\n",
    "1. You can use `csv.reader()` to read the dataset, or rely on other methods such as `pandas`.\n",
    "2. For the wine dataset, subtract the target by `-1` to get the target values in range $\\{0, 1, 2\\}$.\n",
    "3. Be aware both datasets are sorted w.r.t. their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(dataset_file=\"wine.data\"):\n",
    "  # read dataset\n",
    "  data = pd.read_csv(dataset_file, header=None).values.tolist()\n",
    "\n",
    "  print (f\"Loaded dataset with {len(data)} samples\")\n",
    "  \n",
    "  # convert to torch.tensor\n",
    "  data = torch.tensor(data)\n",
    "\n",
    "  if dataset_file == \"wine.data\":\n",
    "    # target is in the first column and needs to be converted to long\n",
    "    X = data[:, 1:]\n",
    "    T = data[:, :1].flatten() - 1\n",
    "    T = T.long()\n",
    "  else:\n",
    "    # target is in the last column and needs to be of type float\n",
    "    X = data[:, :-1]\n",
    "    T = data[:, -1:].float()\n",
    "  return X, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 178 samples\n"
     ]
    }
   ],
   "source": [
    "X, T = dataset(\"wine.data\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Dataset Check\n",
    "\n",
    "Test 1 assures the correctness of the data and target dimensions.\n",
    "\n",
    "1. For the wine dataset, we make sure that the dataset is in the correct dimensions, i.e., $\\mathbf X\\in \\mathbb R^{N\\times D}$ and $\\mathbf T \\in \\mathbb N^N$. And all class labels are in the correct range $[0, O-1]$ where $O$ is the number of classes.\n",
    "\n",
    "2. For the spambase data, we assure that all dimensions are correct and that class labels are in range $\\{0, 1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 178 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 4601 samples\n"
     ]
    }
   ],
   "source": [
    "X, T = dataset(\"wine.data\")\n",
    "\n",
    "assert X.shape[1] == 13, X.shape[1]\n",
    "assert torch.all(T >= 0) and torch.all(T <= 2)\n",
    "assert T.dtype == torch.long\n",
    "\n",
    "X, T = dataset(\"spambase.data\")\n",
    "assert X.shape[1] == 57, X.shape[1]\n",
    "assert T.shape[1] == 1, T.shape[1]\n",
    "assert torch.all(T >= 0) and torch.all(T <= 1)\n",
    "assert T.dtype == torch.float"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Split Training and Validation Data\n",
    "\n",
    "The data should be split into 80% for training and 20% for validation. Implement a function that takes the full dataset $(X,T)$ and returns $(X_t, T_t, X_v, T_v)$ accordingly.\n",
    "\n",
    "Write a function that splits off training and validation samples from a given dataset. What do we need to assure before splitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_training_data(X,T,train_percentage=0.8):\n",
    "  # shuffle data\n",
    "  ## ??? why do we need to shuffle the data here and not after every epoch ???\n",
    "  ## ??? why do I get really weired results without this???\n",
    "  ## data is sorted by classes\n",
    "  indices = torch.randperm(len(X))\n",
    "  X = X[indices]\n",
    "  T = T[indices]\n",
    "  \n",
    "  # split into training/validation dataset\n",
    "  N = X.shape[0]\n",
    "  # split pytorch tensor into training and validation\n",
    "  X_train = X[:int(N * train_percentage)]\n",
    "  T_train = T[:int(N * train_percentage)]\n",
    "  X_val = X[int(N * train_percentage):]\n",
    "  T_val = T[int(N * train_percentage):]\n",
    "\n",
    "  assert X_train.shape[0] + X_val.shape[0] == X.shape[0]\n",
    "  assert T_train.shape[0] + T_val.shape[0] == T.shape[0]\n",
    "  return X_train, T_train, X_val, T_val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Input Data Standardization\n",
    "\n",
    "As we have seen last week, the standardization of the data provides many advantages. \n",
    "Hence, in this task you should write a function that takes $(X_t,X_v)$ as input and standardizes them by subtracting the mean and dividing by the \n",
    "standard deviation of $X_t$, and returning the standardized versions of both. Assure that each input dimension is standardized individually.\n",
    "\n",
    "Implement a function that standardizes all input data for the training and validation set.\n",
    "Return the standardized data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "1. Use `torch.mean()` and `torch.std()` with the correct dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X_train, X_val):\n",
    "  # compute statistics\n",
    "  mean = torch.mean(X_train)\n",
    "  std = torch.std(X_train)\n",
    "\n",
    "  # standardize both X_train and X_val\n",
    "  X_train = X_train - mean / std\n",
    "  X_val = X_train - mean / std\n",
    "  return X_train, X_val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Implementation\n",
    "\n",
    "We will use a two-layer fully-connected network with $D$ input neurons, $K$ hidden neurons and $O$ output neurons. \n",
    "Depending on the task, $D$ and $O$ need to be selected appropriately, while $K$ is a parameter to play around with. \n",
    "In PyTorch, the easiest way to implement a network is by providing the requested sequence of layers to `torch.nn.Sequential`, which will build a network containing the given layers. \n",
    "We will use two `torch.nn.Linear` layers and one `torch.nn.Tanh` activation function in between. \n",
    "The network will return the logits $\\vec z$ for a given input $\\vec x$.\n",
    "\n",
    "\n",
    "### Task 4: Implement Network\n",
    "\n",
    "Implement a two-layer fully-connected network in PyTorch. \n",
    "The given network uses $\\tanh$ as activation function, and provide the possibility to change the number of inputs $D$, the number of hidden neurons $K$ and the number of outputs $O$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def Network(D, K, O):\n",
    "  return torch.nn.Sequential(\n",
    "    torch.nn.Linear(D, K),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.Linear(K, O)\n",
    "  )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Accuracy Computation\n",
    "\n",
    "To monitor the training process, we want to compute the accuracy. \n",
    "The function will obtain the logits $\\vec z$ extracted from the network and the according target $t$. \n",
    "Assure that this function works both for binary and categorical classification. \n",
    "How can we identify, which of the two variants is currently required?\n",
    "\n",
    "Note: you can make use of the following pytorch functions:\n",
    "\n",
    "1. `torch.mean()` which computes the mean or average of the input tensor.\n",
    "2. `torch.argmax()` which returns the indices of the maximum values of all elements of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(Z, T):\n",
    "  # check if we have binary or categorical classification\n",
    "  # for binary classification, we will have a two-dimensional target tensor\n",
    "  if len(T.shape) == 2:\n",
    "    # binary classification\n",
    "    # If z is equal or larger than the threshold 0.5, then we predict 1, otherwise 0 \n",
    "    # we use the .float() function to convert the boolean to a float\n",
    "    # then we compare the prediction with the target and compute the mean\n",
    "    \n",
    "    # ??? our data is binary between 0 and 1, so why do you use 0 as threshold ???\n",
    "    # So only if we use sigmoid activation function for binary or softmax for multi-class\n",
    "    # after the last layer, we need to use 0.5 as threshold\n",
    "    return torch.mean(((Z>=0).float() == T).float())\n",
    "\n",
    "  else:\n",
    "    # categorical classification\n",
    "    # the argmax function returns the index of the maximum value\n",
    "    # we use the .float() function to convert the boolean to a float\n",
    "    # then we compare the prediction with the target and compute the mean\n",
    "    # return torch.mean((torch.argmax(Z, dim=1).float() == T).float())\n",
    "    \n",
    "    # Y is the index of the maximum value in Z\n",
    "    Y = torch.argmax(Z, dim=1)\n",
    "    return torch.mean((Y == T).float())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Test Accuracy Function\n",
    "\n",
    "Test 2 assures the correctness of your accuracy function in both binary and categorical cases. We make sure that the accuracy will compute the correct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, test binary classification\n",
    "ZZ = torch.ones((20,1)) * -5.\n",
    "ZZ[15:20] = 5\n",
    "assert(abs(accuracy(ZZ,torch.zeros((20,1))) - 0.75) < 1e-8)\n",
    "assert(abs(accuracy(ZZ,torch.ones((20,1))) - 0.25) < 1e-8)\n",
    "\n",
    "# now, test categorical classification with 4 classes\n",
    "ZZ = torch.ones((20,4)) * -5\n",
    "ZZ[0:1,0] = 5\n",
    "ZZ[1:4,1] = 5\n",
    "ZZ[4:10,2] = 5\n",
    "ZZ[10:20,3] = 5\n",
    "\n",
    "assert(abs(accuracy(ZZ,torch.zeros(20)) - 0.05) < 1e-8)\n",
    "assert(abs(accuracy(ZZ,torch.ones(20)) - 0.15) < 1e-8)\n",
    "assert(abs(accuracy(ZZ,torch.ones(20)*2) - 0.3) < 1e-8)\n",
    "assert(abs(accuracy(ZZ,torch.ones(20)*3) - 0.5) < 1e-8)\n",
    "assert(abs(accuracy(ZZ,torch.tensor((0,1,1,1,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3))) - 1.) < 1e-8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Training Loop\n",
    "\n",
    "Implement a function that takes all necessary parameters to run a training on a given dataset.\n",
    "In this week, we will run gradient descent, i.e., we will train on the whole dataset in each training step, so there is no need to define anything related to batches. \n",
    "Select the optimizer to be `torch.optim.SGD`. \n",
    "\n",
    "Implement a training loop over 10'000 epochs with a learning rate of $\\eta=0.1$. \n",
    "Make sure that you train on the training data only, and **not** on the validation data.\n",
    "In each loop, compute and store the training loss, training accuracy, validation loss and validation accuracy. \n",
    "At the end, return the lists of these values.\n",
    "\n",
    "Note:\n",
    "\n",
    "1. When storing accuracy or loss values in a list, make sure to convert the to float via `v.item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, T_train, X_val, T_val, network, loss_function, epochs=1000, learning_rate=0.1):\n",
    "  optimizer = torch.optim.SGD(\n",
    "    network.parameters(), \n",
    "    lr=learning_rate,\n",
    "  )\n",
    "\n",
    "  # collect loss and accuracy values\n",
    "  train_loss, train_acc, val_loss, val_acc = [], [], [], []\n",
    "\n",
    "  for epoch in range(0, epochs):\n",
    "    # train on training set\n",
    "    # ... compute network output on training data\n",
    "    x_hat = network(X)\n",
    "    # ... compute loss from network output and target data\n",
    "    loss = loss_function(x_hat, T)\n",
    "    # ... perform parameter update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # ... remember loss\n",
    "    train_loss.append(loss)\n",
    "    # ... compute training set accuracy\n",
    "    acc = accuracy(x_hat, T)\n",
    "    train_acc.append(acc)\n",
    "\n",
    "    # test on validation data\n",
    "    with torch.no_grad():\n",
    "      # ... compute network output on training data\n",
    "      x_hat = Network(X)\n",
    "      # ... compute loss from network output and target data\n",
    "      loss = loss_function(x_hat, T)\n",
    "      # ... remember loss\n",
    "      val_loss.append(loss)\n",
    "      # ... compute validation set accuracy\n",
    "      acc = accuracy(x_hat, T)\n",
    "      val_acc.append(acc)\n",
    "\n",
    "  # return the four lists of losses and accuracies\n",
    "  return train_loss, train_acc, val_loss, val_acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "Finally, we want to train our network on our data and plot the accuracy and loss values that were obtained through the epochs. \n",
    "Exemplary plots can be found in the exercise slides.\n",
    "\n",
    "\n",
    "### Task 7: Plotting Function\n",
    "\n",
    "Implement a function that takes four lists containing the training loss, the training accuracy, the validation loss and the validation accuracy and plot them into two plots. \n",
    "The first plot should contain the loss values for both training and validation. The second plot should contain the according accuracy values.\n",
    "\n",
    "Note:\n",
    "\n",
    "1. You might need to convert remaining `torch.tensor` values to `float`, lists, or `numpy.nadrray` for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "def plot(train_loss, train_acc, val_loss, val_acc):\n",
    "  pyplot.figure(figsize=(10,3))\n",
    "  ax = pyplot.subplot(121)\n",
    "  ax.plot(..., \"g-\", label=\"Training set loss\")\n",
    "  ax.plot(..., \"b-\", label=\"Validation set loss\")\n",
    "  ax.legend()\n",
    "\n",
    "  ax = pyplot.subplot(122)\n",
    "  ax.plot(..., \"g-\", label=\"Training set accuracy\")\n",
    "  ax.plot(..., \"b-\", label=\"Validation set accuracy\")\n",
    "  ax.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8: Binary Classification\n",
    "\n",
    "\n",
    "1. Load the data for binary classification, using the ``\"spambase.data\"`` file.\n",
    "2. Split the data into training and validation sets.\n",
    "3. Standardize both training and validation input data using the function from Task 3.\n",
    "4. Instantiate a network with the correct number of input neurons, a reasonable number of $K$ hidden neurons and one output neuron.\n",
    "\n",
    "Which loss function do we need for this task?\n",
    "\n",
    "Train the network with our data for 10'000 epochs and plot the training and validation accuracies and losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 178 samples\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([178])) must be the same as input size (torch.Size([178, 1]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m network \u001b[39m=\u001b[39m Network(X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], K \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m, O \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[39m# train network on our data\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m results \u001b[39m=\u001b[39m train(X_train, T_train, X_val, T_val, network, loss_function)\n\u001b[1;32m     15\u001b[0m \u001b[39m# plot the results\u001b[39;00m\n\u001b[1;32m     16\u001b[0m plot(\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(X_train, T_train, X_val, T_val, network, loss_function, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     13\u001b[0m x_hat \u001b[39m=\u001b[39m network(X)\n\u001b[1;32m     14\u001b[0m \u001b[39m# ... compute loss from network output and target data\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m loss \u001b[39m=\u001b[39m loss_function(x_hat, T)\n\u001b[1;32m     16\u001b[0m \u001b[39m# ... perform parameter update\u001b[39;00m\n\u001b[1;32m     17\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/loss.py:720\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 720\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target,\n\u001b[1;32m    721\u001b[0m                                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    722\u001b[0m                                               pos_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_weight,\n\u001b[1;32m    723\u001b[0m                                               reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py:3163\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3160\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[0;32m-> 3163\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()))\n\u001b[1;32m   3165\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([178])) must be the same as input size (torch.Size([178, 1]))"
     ]
    }
   ],
   "source": [
    "# define loss function\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "# load dataset\n",
    "X, T = dataset()\n",
    "# split dataset\n",
    "X_train, T_train, X_val, T_val = split_training_data(X, T)\n",
    "# standardize input data\n",
    "X_train, X_val = standardize(X_train, X_val)\n",
    "# instantiate network\n",
    "network = Network(X.shape[1], K = 10, O = 1)\n",
    "\n",
    "# train network on our data\n",
    "results = train(X_train, T_train, X_val, T_val, network, loss_function)\n",
    "\n",
    "# plot the results\n",
    "plot(...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Categorical Classification\n",
    "\n",
    "Perform the same tasks with the ``\"wine.data\"`` dataset for categorical classification. \n",
    "How many input and output neurons do we need?\n",
    "Change the number of input, hidden, and output neurons accordingly.\n",
    "\n",
    "Select the appropriate loss function for categorical classification.\n",
    "Which loss function will we need this time?\n",
    "\n",
    "How many hidden neurons will we need to get 100% training set accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "loss = ...\n",
    "# load dataset\n",
    "X, T = ...\n",
    "# split dataset\n",
    "X_train, T_train, X_val, T_val = ...\n",
    "# standardize input data\n",
    "X_train, X_val = ...\n",
    "# instantiate network\n",
    "network = ...\n",
    "\n",
    "# train network on our data\n",
    "results = ...\n",
    "\n",
    "# plot the results\n",
    "plot(...)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2dd53f8ad749bca69f7250ce75eb4f0def59db5cf79075a9716322ffc58e8a2e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('DL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
