{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture 3 (p.11) why do we need an activation for non-linearity? Why do we need a transform to multiply the weights(2) with the activation and the W(1)?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture 3 (p.41) why do we have a dimension mismatch and how do we solve it (Vectorizing Computation)?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture 5 (p.19) why does BCE loss turn L2 loss into binary classification by adding sigma(z) <br>\n",
    "ðŸ’¡ sigma stands for sigmoid function which outputs a number between 0 and 1, which is great for probabilities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are logits only for classification problems? <br>\n",
    "CHAT-GPT So, while the term \"logits\" is commonly associated with classification problems, it is not exclusively limited to them. It refers to the unnormalized values produced by a model before applying a final activation function or transformation, and it can be utilized in various types of machine learning tasks beyond classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need two Jacobian matrices for each operation $\\overrightarrow{z} = W^{(2)} * \\overrightarrow{h}$ ? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture 6 quiz last question C. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture 7 p.5 what does q, k, m mean?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture 7 p. 35 Adam Optimizer, basically momentum learning per gradient instead of averaged over everything?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz 8 question 1 B & F learnable parameters?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz 8 question 4 F which activation provides well-discriminative features ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture 8 as from p. 30, open-set classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture 9 p. 16  fractionally-strided convolution, what is it, why do we need it?\n",
    "**Answer** increases output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz 9 exercise 3D : A two-layer auto-encoder can learn a non-linear embedding of the features. Why\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz 9 4C ?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
