{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3Sy3dTGcaCKy"
      },
      "source": [
        "# 2. Activity Prediction\n",
        "\n",
        "First, we download and extract the data.\n",
        "We read them into two lists, one containing the input data $\\mathcal X$ and one the target classes $\\mathbf T$ as integral values.\n",
        "The input data $\\mathcal X \\in \\mathbb R^{M\\times N\\times D}$ is organized such that we have $M=15$ subjects with various numbers $N$ of samples, each with a data dimension of $D=3$.\n",
        "The target data $\\mathbf T \\in \\mathbb \\{0,...,6\\}^{M\\times N}$ provides the target class for each sample.\n",
        "\n",
        "Please do not modify this code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "mFuJD6ozaCK5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of subjects: 15\n",
            "number of samples for the first subject: 162500\n",
            "length of one input sample: 3\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# download data\n",
        "if not os.path.exists(\"UserActivity.zip\"):\n",
        "  import urllib.request\n",
        "  urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00287/Activity%20Recognition%20from%20Single%20Chest-Mounted%20Accelerometer.zip\", \"UserActivity.zip\")\n",
        "  print (\"Downloaded datafile\", \"UserActivity.zip\")\n",
        "\n",
        "# extract data\n",
        "if not os.path.exists(\"Activity Recognition from Single Chest-Mounted Accelerometer\"):\n",
        "  import zipfile\n",
        "  zipfile.ZipFile(\"UserActivity.zip\").extractall()\n",
        "  print (\"Extracted datafile\", \"UserActivity.zip\")\n",
        "\n",
        "# read the data from the files\n",
        "X = []\n",
        "T = []\n",
        "for i in range(1,16):\n",
        "  # collect samples for each of the 15 subjects\n",
        "  with open(f\"Activity Recognition from Single Chest-Mounted Accelerometer/{i}.csv\") as f:\n",
        "    # inputs and targets for this subject\n",
        "    x, t = [], []\n",
        "    for i, line in enumerate(f):\n",
        "      # split the data\n",
        "      splits = line.rstrip().split(\",\")\n",
        "      # get the target value\n",
        "      tt = int(splits[-1])\n",
        "      # there are some invalid target values, we skip these\n",
        "      if tt: \n",
        "        # load the three accelorometer data\n",
        "        x.append([float(v) for v in splits[1:-1]])\n",
        "        # add the label (convert from one-based into zero-based indexing)\n",
        "        t.append(tt-1)\n",
        "    # append samples and targets of the current subject\n",
        "    X.append(x)\n",
        "    T.append(t)\n",
        "\n",
        "# print some statistics of the dataset\n",
        "print (f\"number of subjects: {len(X)}\\nnumber of samples for the first subject: {len(X[0])}\\nlength of one input sample: {len(X[0][0])}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UjkZsIvsaCK-"
      },
      "source": [
        "### 2. (d) Data Reduction\n",
        "\n",
        "Implement a strategy to reduce the amount of data for each subject. Assure that you apply the identical selection strategy for the inputs and the targets. Make sure that you do not change the arrangement of the data matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "AQoNfk0MaCLA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of subjects: 15\n",
            "number of samples for the first subject: 81250\n",
            "length of one input sample: 3\n",
            "number of subjects: 15\n",
            "number of targets for the first subject: 81250\n"
          ]
        }
      ],
      "source": [
        "# reduce the number of samples on first dimension by a factor of 5 (every 5th sample)\n",
        "\n",
        "X_selected = []\n",
        "T_selected = []\n",
        "for subject in X:\n",
        "    subject_data = []\n",
        "    for i,x in enumerate(subject):\n",
        "        if i % 2 != 0:\n",
        "            subject_data.append(x)\n",
        "    X_selected.append(subject_data)         \n",
        "\n",
        "for subject in T:\n",
        "    subject_data = []\n",
        "    for i,x in enumerate(subject):\n",
        "        if i % 2 != 0:\n",
        "            subject_data.append(x)\n",
        "    T_selected.append(subject_data)       \n",
        "                \n",
        "\n",
        "print (f\"number of subjects: {len(X_selected)}\\nnumber of samples for the first subject: {len(X_selected[0])}\\nlength of one input sample: {len(X_selected[0][0])}\")\n",
        "print (f\"number of subjects: {len(T_selected)}\\nnumber of targets for the first subject: {len(T_selected[0])}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z_Y-GoL-aCLC"
      },
      "source": [
        "Here, we concatenate the data of subjects 1-10 into the training matrices, and the data of subjects 11-15 into the validation set. \n",
        "There is no need to change this code (unless you changed the arrangement of the data above, in which case you also need to adapt this code)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "7EWL_UUgaCLD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape of the training input is: torch.Size([708387, 3])\n",
            "the shape of the training targets is torch.Size([708387])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "X_train = torch.tensor(sum(X_selected[:10], []))\n",
        "X_val = torch.tensor(sum(X_selected[10:], []))\n",
        "\n",
        "T_train = torch.tensor(sum(T_selected[:10], []))\n",
        "T_val = torch.tensor(sum(T_selected[10:], []))\n",
        "\n",
        "print(f\"The shape of the training input is: {X_train.shape}\\nthe shape of the training targets is {T_train.shape}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "orqmeFsLaCLF"
      },
      "source": [
        "### 2. (e) Dataset Implementation\n",
        "\n",
        "Implement the dataset that takes the given data `X` and `T`, as well as a sequence length `S`. Return a sequence of `S` samples and the label for the last element in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Yu30QQN0aCLH"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  \n",
        "  def __init__(self, X, T, S):\n",
        "    # implement the constructor\n",
        "    self.S = S\n",
        "    self.T = T\n",
        "    self.X = X\n",
        "\n",
        "  def __len__(self):\n",
        "    # return the number of samples in this dataset\n",
        "    return len(x) #because we want the dimension N, which is the second, i.e. index 1\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # return the pair of input and target values for the given index\n",
        "    return self.X[index], self.T[index]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8v8rKM4faCLI"
      },
      "source": [
        "### 2. (f) Data Loaders\n",
        "\n",
        "We need to instantiate training and validation set data loaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "GwBmYRxoaCLK"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "object of type 'int' has no len()",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[51], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# instantiate training set data loader\u001b[39;00m\n\u001b[1;32m      2\u001b[0m train_set \u001b[39m=\u001b[39m Dataset(X_train, T_train)\n\u001b[0;32m----> 3\u001b[0m train_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mDataLoader(train_set, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      5\u001b[0m \u001b[39m# and the validation set data loader\u001b[39;00m\n\u001b[1;32m      6\u001b[0m val_set \u001b[39m=\u001b[39m Dataset(X_val, T_val)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:351\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# map-style\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[39mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 351\u001b[0m         sampler \u001b[39m=\u001b[39m RandomSampler(dataset, generator\u001b[39m=\u001b[39;49mgenerator)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m         sampler \u001b[39m=\u001b[39m SequentialSampler(dataset)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/sampler.py:106\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplacement, \u001b[39mbool\u001b[39m):\n\u001b[1;32m    103\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mreplacement should be a boolean value, but got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mreplacement=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplacement))\n\u001b[0;32m--> 106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_samples, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    107\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnum_samples should be a positive integer \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mvalue, but got num_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples))\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/sampler.py:114\u001b[0m, in \u001b[0;36mRandomSampler.num_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnum_samples\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m    112\u001b[0m     \u001b[39m# dataset size might change at runtime\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_samples \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_source)\n\u001b[1;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_samples\n",
            "Cell \u001b[0;32mIn[50], line 11\u001b[0m, in \u001b[0;36mDataset.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     10\u001b[0m   \u001b[39m# return the number of samples in this dataset\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39;49m(x)\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
          ]
        }
      ],
      "source": [
        "# instantiate training set data loader\n",
        "train_set = Dataset(X_train, T_train, len(X_selected[0]))\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "\n",
        "# and the validation set data loader\n",
        "val_set = Dataset(X_val, T_val, len(X_selected[0]))\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=64, shuffle=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wPZDKDnnaCLL"
      },
      "source": [
        "### 2. (g) Network Implementation\n",
        "\n",
        "Implement and instantiate one of the networks discussed in (a). Since there are various different ways to implement this network, no guidelines will be provided here. Note that the network should output the prediction only for the last sequence element."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5N9J8a8yaCLM"
      },
      "outputs": [],
      "source": [
        "# implement and instantiate the network\n",
        "\n",
        "class ElmanNetwork(torch.nn.Module):\n",
        "  def __init__(self, D, K):\n",
        "    super(ElmanNetwork,self).__init__()\n",
        "    self.W1 = torch.nn.Linear(D, K)\n",
        "    self.Wr = torch.nn.Linear(K, K)\n",
        "    self.W2 = torch.nn.Linear(K, D)\n",
        "    self.activation = torch.nn.Softmax()\n",
        "    self.K = K\n",
        "\n",
        "  def forward(self, x):\n",
        "    # get the shape of the data\n",
        "    B, S, D = x.shape\n",
        "    # initialize the hidden vector in the desired size with 0\n",
        "    # remember to put it on the device\n",
        "    h_s = torch.zeros((B,self.K)).to(device)\n",
        "    # store all logits (we will need them in the loss function)\n",
        "    Z = torch.empty(x.shape, device=device)\n",
        "    # iterate over the sequence\n",
        "    for s in range(S):\n",
        "      # use current sequence item and all batches simultaneously (:) (shape = B,S,D) -> the D sample are taken in individually (x)\n",
        "      x_s = x[:, s]\n",
        "      # compute recurrent activation: h_s is still stored from the last iteration of S and h[0]=0\n",
        "      a_s = self.W1(x_s) + self.Wr(h_s)\n",
        "      # apply activation function\n",
        "      h_s = self.activation(a_s)\n",
        "      # compute logit values\n",
        "      z = self.W2(h_s)\n",
        "      # store logit values for sequence element s and all batches\n",
        "      Z[:,s] = z\n",
        "\n",
        "    # return logits for all sequence elements\n",
        "    return Z\n",
        "\n",
        "network = ..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AzNxHb7baCLN"
      },
      "source": [
        "### 2. (h) Network Training\n",
        "\n",
        "Instantiate the loss function and the optimizer. Train the network for 10 epochs and compute validation set accuracy. Note that one epoch of training might take several minutes. There is no need to wait for the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlZNJ9nzaCLO"
      },
      "outputs": [],
      "source": [
        "# instantiate optimizer and loss function\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=network.parameters(),lr=0.01, momentum=0.9) #could also use Adam optimiser\n",
        "\n",
        "val_acc = []\n",
        "\n",
        "# train the network for 10 epochs\n",
        "for epoch in range(10):\n",
        "  # use all training samples in batches\n",
        "   for x, t in train_loader:\n",
        "    x = x.to(device)\n",
        "    t = t.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    # compute network output\n",
        "    z = network(x)\n",
        "    # compute loss, arrange order of logits and targets\n",
        "    #change order of dimensions for x, because the loss is computed over D,\n",
        "    #which is the third dimension, but needs to be the second one\n",
        "    J = loss(z.permute(0,2,1),t.permute(0,2,1))\n",
        "    # compute gradient for this batch\n",
        "    J.backward()\n",
        "    optimizer.step()\n",
        "    # select a new sequence length S in [5,20]\n",
        "    epoch_length = np.random.randint(5,20)    \n",
        "\n",
        "  # compute validation set accuracy\n",
        "   with torch.no_grad():\n",
        "      cur_acc = 0. #for each epoch, we compute the validation acc\n",
        "      for x,t in val_loader:\n",
        "            z = network(x.to(device))\n",
        "            j = loss(z, t.to(device))\n",
        "            cur_acc += torch.sum(torch.argmax(z, dim=1) == t.to(device).item()) #accuracy identity function (last time the accuracy function was implemented seperately)\n",
        "      val_acc.append(cur_acc/(len(testset)))\n",
        "\n",
        "  # report validation set accuracy\n",
        "  print(f\"epoch {epoch+1}, validation accuracy: {...}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DL-FS22-Exam-Task2.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "a29cabff5744fce69e08a959ab87b9e77a9f67b498d08783caa8c3bb16f23a00"
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('DL')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
